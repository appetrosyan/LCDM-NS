% Compile with xelatex -shell-escape

\documentclass[usenatbib]{mnras}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{varioref}
\usepackage{bm}
\usepackage{pgfplots}
\usepackage[nameinlink, capitalize, noabbrev]{cleveref}

\usepgfplotslibrary{groupplots,dateplot}
\usetikzlibrary{patterns,shapes.arrows,external}
\tikzexternalize
\pgfplotsset{compat=newest}
\usepackage{dsfont}
\usepackage{xcolor}
\usepackage{listings}

\DeclareMathOperator{\TopHat}{TH}
\DeclareMathOperator{\CDF}{CDF}

\author[TODO exam number]{
  EXAM NUMBER
}
\date{\today}
\title[Accelerated Nested Sampling]{Accelerated nested sampling in context of cosmological parameter estimation}
\hypersetup{
 pdfauthor={},
 pdftitle={Accelerated nested sampling in context of cosmological parameter estimation},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.3 (Org mode 9.1.9)}, 
 pdflang={English}}
\crefname{Option}{Option}{Options}
\crefname{Property}{Property}{Properties}

\begin{document}

\maketitle
\begin{abstract}
  Bayesian inference is a robust framework for testing of scientific
  hypotheses and is used in multiple areas of physics. In cosmology
  said hypotheses are remarkably complex, necessitating the use of
  sophisticated Monte-Carlo tecnhiques such as nested sampling. Even
  so the computations are expensive: typically requiring resources
  outside the realm of one's personal electronics, and even on
  supercomputer clusters take weeks to complete. We have found a
  methodology that allows one to incorporate intuitive guesses into
  nested sampling to improve performance. This we call posterior
  repartitioning. We have also discovered a methodology of mixing
  several intuitive guesses into a single consistent model. Our
  isometric model mixing scheme allows one to extract the useful
  information out of multiple intuitive guesses without fear of
  obtaining a biased result. We demonstrate a significant uplift in
  precision, accuracy and performance of a full cosmological inference
  done both with Cobaya and CosmoChord. As an illustration of the
  latter, we have conducted a full cosmological inference using
  CosmoChord on a personal computer, completing on a time-scale
  comparable to the supercomputer.
\end{abstract}

\begin{keywords}
Bayesian inference -- automated posterior repartitioning -- nested sampling -- cosmology: miscellaneous -- methods: statistical -- methods: data analysis
\end{keywords}

\section{Introduction}\label{sec:org14413d7}

The standard model of the universe and its evolution in modern
cosmology is the \(\Lambda\)CDM model \citep{Condon2018}, so named
after the main components of the universe: the \emph{cosmological
  constant} \(\Lambda\) and \emph{cold dark matter}. It has six major
independent parameters: physical baryon density \(\Omega_{b}h^{2}\);
physical dark matter density \(\Omega_{c}h^{2}\); the Hubble parameter
at present\(H_{0}\)\footnote{Equivalently the age of the universe
  \(t_0\)}; curvature fluctuation amplitude \(\Delta_{R}^{2}\); and
re-ionization optical depth \(\tau_\text{Reio.}\).

It is the task of the present study, to \emph{develop better tools}
for evaluating the agreement of our observations from the Planck
mission with \(\Lambda\)CDM, estimating the parameters in the process.
This is done conveniently in the formalism of Bayesian statistics, due
to the explicit representation of our knowledge, and data. In that
language our goal is a type of Bayesian inference. Many algorithms
were developed to accelerate said process: Metropolis-Hastings
\citep{Metropolis} in conjunction with the Gibbs sampler
\citep{Metropolis-Hastings-Gibbs}; Hybrid (Hamiltonian) Monte Carlo
\citep{1701.02434,Duane_1987}, and nested Sampling
\citep{Skilling2006} which is our focus for the project. 

Nested sampling~\cite{Skilling2006} is a family of algorithms, each
with their own unique algorithmic implementation details:
\begin{enumerate}
\item \texttt{MultiNest} \citep{Feroz2009MultiNestAE},
\item \texttt{PolyChord} \citep{polychord}.
\item \texttt{nestle} \citep{nestle}
\item \texttt{dyNesty} \citep{Speagle_2020},
\end{enumerate}
to name a few.  

In the following sections we shall provide a brief primer on Bayesian
inference and nested sampling. Followed by an exploration of work by
\cite{chen-ferroz-hobson} recontextualised in a rigorous mathematical
framework of our making.From within that framework we shall introduce
isometric model mixing, its implementation using superposition via
stochastic choice, followed by demonstrations of its efficacy. Lastly,
we shall discuss potential extensions of the idea to what we call
Bayesian meta-analysis.

\section{Theoretical background}\label{sec:orge6061a4}
In this section we primarily focus on previous work. We introduce only
the bare minimum conceptual background needed to formulate the
framework.

\subsection{Bayesian inference}\label{sec:org6b7d2fe}

This topic has been discussed at length in literature
\citep{jeffreys2010scientific}, so we shall omit unnecessary details.

Let \({\cal M}\) be a model of some process, parameterised by
\(\bm{\theta} = (\theta_{1}, \theta_{2}, \ldots , \theta_{n})\), Let's
encapsulate our empyrical observations of said process in
\(\mathfrak{D}\).  These objects in Bayesian statistics are
represented by conditional probabilities given
in~\cref{table-defs}. Their interactions are governed by
\citeauthor{1763}'s theorem:
\begin{equation}\label{eq:bayes} 
 {\cal L}(\bm{\theta})  \pi (\bm{\theta}) = {\cal Z}  {\cal P} (\bm{\theta}).
\end{equation}
An immediate corollary of~\cref{eq:bayes} is the definition of \emph{evidence}:
\begin{equation}\label{eq:def-z}
 {\cal Z} \triangleq \int_{\Psi} {\cal L}(\bm{\theta}) \pi(\bm{\theta}) d\bm{\theta}, 
\end{equation}
where \(\Psi\) is the \emph{prior space} --- the domain of
\(\pi(\bm{\theta})\) and \(A \triangleq B\) is standard notation for
``A is defined by A = B''. The evidence is, to quote
\cite{importanceOfZ} ``Often the single most important number in the
problem''. In Bayesian inference it is a measure of agreement between
our observations and predictions. It constitutes a result of Bayesian
inference, alongside the \emph{posterior} \({\cal
  P}(\bm{\theta})\). The latter is the probability of the model
parameters after accounting for \(\mathfrak{D}\). 

\begin{table}
  \caption{Definitions of main quantities in Bayesian inference. By
    convention, for continuous variables, the probabilities are
    specified via probability density functions (PDF), ave evidence
    which is always a scalar.  \label{table-defs}}
\centering
\begin{tabular}{llr}
\textbf{\textbf{Term}} & \textbf{\textbf{Symbol}} & \textbf{\textbf{Definition}}\\
\hline
Prior (PDF) & \(\pi(\bm{\theta})\) & \(P ( \bm{\theta}  \vert \bm{ \mathfrak{D}})\) \\
Likelihood (PDF) & \({\cal L}(\bm{\theta})\) & \(P ( \bm{\mathfrak{D}} \vert \bm{\theta} \cap M)\) \\
Posterior (PDF) & \({\cal P}(\bm{\theta})\) & \(P ( \bm{\theta} \vert \bm{\mathfrak{D}} \cap M)\) \\
Evidence & \({\cal Z}\) & \(P ( \bm{\mathfrak{D}} \vert {\cal M})\) \\
\end{tabular}
\end{table}

The two independent functions, \(\pi(\bm{\theta})\) and
\({\cal L}(\bm{\theta})\) are inputs to Bayesian inference. The former
represents our \emph{prior} knowledge of the model parameters, while
the latter, represents the \emph{likelihood} of the model parameters
predicting a particular datum\footnote{note that by writing ${\cal L}$
  as just a function of $\theta$, we've implicitly assumed the
  existence of a surjective map
  ${\cal M}: \Psi \rightarrow \mathfrak{D}$. While not generally true,
  models where such a map doesn't exist are non-deterministic, hence
  rarely considered in Bayesian inference. } labelled by
$\bm{\theta}$.

The convenient depiction of inputs depends on the particular sampling
algorithm, however, for most nested samplers (e.g. \texttt{PolyChord},
\texttt{MultiNest}) we delineate them indirectly with log-likelihood
\(\ln \cal L (\bm{\theta})\), and \emph{prior quantile}
\(C\{\pi\}(\bm{\theta})\). The latter, can be thought of as a
coordinate transformation $C: \bm{u} \mapsto \bm{\theta}$ that maps a
uniform distribution of $\bm{u}$ in a unit hypercube to
$\pi(\bm{\theta})$ in $\Psi$.

\subsection{Nested Sampling}\label{sec:org36366f8}

\citeauthor{1763}'s theorem reduces the problem of parameter
estimation to integration~\citep{bayes-integration}. Thus the na√Øve
approach: uniformly rasterise \(\Psi\) and \({\cal Z}\) via Riemann
sums, is valid. However, for hypotheses with \(O(30)\) parameters,
said rasterisation is intractable \citep{Caflisch_1998} and
integration is done using Monte Carlo techniques. Nested sampling is
one such technique that has considerable performance advantages in
many-parameter models.

The gist of the algorithm is that we pick according to some principle
(\cref{sec:ns}) \emph{live points} and iteratively move them into
regions of high likelihood. Based on a statistical argument this
allows us to approximate ${\cal Z}$, ergo ${\cal P}(\bm{\theta})$
using \cref{eq:bayes}.

The time complexity $T$ of nested sampling satisfies
\begin{subequations}\label{eq:complexity}
  \begin{align}
    T \propto & n_\text{live} \langle {\cal T}\{{\cal L}(\bm{\theta})\} \rangle {\cal N}\{{\cal T}\{{\cal L}(\bm{\theta}) \},
  \end{align}
\end{subequations}
where ${\cal T}\{f(\bm{\theta})\}$ represents time complexity of
evaluating $f(\bm{\theta})$ and ${\cal N}\{f(\bm{\theta})\}$, the
number of such evaluations. Reducing $n_\text{live}$ reduces the
resolution of nested sampling, while
${\cal T}\{ {\cal T}\{{\cal L}(\bm{\theta})\}$ is model-dependent. We
can, however, reduce the number of likelihood evaluations.





\subsection{Biases, guesses and unrepresentative
  priors\label{discussion-bias}}
The choice of prior is relatively arbitrary, yet we have
demonstrated that one can choose them differently accelerating
inference.

So why not just adjust our prior based on intuition?  To illustrate,
consider that one has gathered data from free fall experiments. On
earth, one knows the posterior for \(g\) to be a normal distribution
peaked at \[\langle g \rangle=9.81,\] with standard
deviation \[\sigma_{g} = 0.01\] due to regional variations, which we
shall compactly refer to as
\begin{equation*}
  \pi(\bm{\theta}) = G(\bm{\theta};\bm{\mu}_{g}=9.81, \bm{\sigma}_{g}=0.01, \ldots ).
\end{equation*}
We use this to obtain a posterior distribution.

So if one chooses a prior without changing the likelihood, the
posterior would be different. If the prior changed due to new
information, this is desired. If the prior was chosen ad-hoc, this
limits the utility of the posterior and evidence.

But we can re-scale ${\cal L} \rightarrow \tilde{\cal L}$ such that
\begin{equation}
  \label{eq:idea-repart}
{\cal L}(\theta)\pi(\theta)= \tilde{\cal L}(\theta) \tilde{\pi}(\theta)
\end{equation}
which will preserve the outputs.  This is known as \emph{intuitive guess posterior repartitioning} (IGPR).

This is the fastest known method of accelerating convergence of nested sampling discussed in detail in \cref{sec:accelerating}. 

However, if our informative $\tilde{\pi}(\theta)$ is peaked where the
posterior is flat, the probability of the correct region being sampled
is inadequate, and the algorithm will be incapable of obtaining the
correct result. It might obtain it far slower than a less-informative
$\pi(\theta)$, which amounts to being useless. This is known as the
problem of \emph{unrepresentative priors}.


\subsection{Power posterior repartitioning}\label{sec:org68fff63}

\citeauthor{chen-ferroz-hobson} have proposed introducing an
extra parameter \(\beta\) that re-scales the prior:
\begin{equation*}
  \tilde{\pi}(\bm{\bm{\theta}};\beta) = \cfrac{\pi(\bm{\theta})^{\beta}}{Z(\beta)\{\pi\}},
\end{equation*}
where \(Z(\beta)\{\pi\}\) is a normalisation factor, i.e. 
\begin{equation*}
  Z(\beta)\{\pi\} = \int_{\bm{\theta} \in \Psi} \pi(\bm{\bm{\theta}})^{\beta}d\bm{\bm{\theta}}.
\end{equation*}
In their prescription, the likelihood changes to
\begin{equation*}
  \tilde{\cal L}(\bm{\theta}) = {\cal L}(\bm{\theta}) Z(\beta)\{\pi\} \cdot \pi^{1-\beta}(\bm{\theta}).
\end{equation*}
The domains of all functions need to be the same. Let
\(D(f)\) denote the domain of the function \(f\), i.e.~where the
function is both defined and \textbf{non-zero}. Hence
\begin{equation*}
  D(\pi) = D({\cal L}) = \Psi = D({\cal P}),
\end{equation*} 
meaning the posterior is within the domain of the prior and
likelihood, which will be important later.\label{domain-discussion}

There is no general prescription for determining the prior of
\(\beta\). The tightest constraints on \(\beta\) produce the best
convergence speed, however broad constraints may be valuable.  If
\(\pi(\bm{\theta})\) is Gaussian, a uniform 
\[\beta\in[0,1]\] prior is a convenient starting point.
If the original prior \(\pi\) was representative, one could introduce
a non-linear map that favours \(\beta\approx 1\) making
\(\tilde{\pi}\) more representative. If the original prior may have
been too broad (overestimated errors) we could extend it
to \[\beta>1.\] Rarely, \[\beta<0,\] may also be useful.

This, for the cases that \citeauthor{chen-ferroz-hobson} have
originally considered, resolves the issue of non-representative
priors, because the evidence associated with the biased prior reduces
as \(\beta\rightarrow0\). This is meant to trade performance for
mitigating systematic errors, though faster than a uniform
prior.\footnote{in practice, the overhead associated with PPR is
  negligible, and even in the case of uni-variate examples, where the
  relative impact of adding an extra parameter is maximal, the
  overhead is insignificant~\cite[see][section ``numerical
  examples'']{chen-ferroz-hobson}.}

Notice that the \citeauthor{chen-ferroz-hobson}'s construction is
only useful if the prior we started with --- \(\pi\), was
peaked. Indeed, raising a uniform prior \(\pi\) to power \(\beta \in
   \mathbb{R}\) would not change it in any way.

\section{Theoretical discoveries}
\subsection{Repartitioning for  accelerated convergence\label{sec:accelerating}}

As we have seen at the end of \cref{discussion-bias}, we can use
intuitive guesses to accelerate the convergence of nested
sampling. More formally: Given a model \({\cal M}\), for which we have
no prior knowledge, (i.e. \(\pi(\theta) = \text{Const.}\)
).\footnote{the standard invariant objective prior in the general case
  was proven by~\cite{JeffreysPrior} to be the determinant of the
  fisher Matrix. A straightforward calculation thus yields that for a
  Gaussian distribution with a fixed standard deviation the Xprior is
  unity and unbounded, hence not normaliseable. Normally, however,
  it's assumed to be normalised and bounded.} Central limit theorem
suggests that the posterior may be a Gaussian:
\begin{equation}
 \pi (\bm{\theta}) \propto \exp \left[-\left(\cfrac{\bm{\theta} - \bm{\mu}}{2\bm{\sigma}}\right)^{2} \right],
 \label{eq:bias}
\end{equation}
where \(\bm{\mu}\) and \(\bm{\sigma}\) are unknown to us\footnote{we
  have slightly abused the notation: the quotient of two vector
  quantities is undefined. Instead, in such fractions there is an
  implicit tensor index. Whenever a quantity with an implicit index is
  equated to a probability density, summation over the index is
  implied.}, but we have a guess that $\mu=\mu_{g}$ and
$\sigma=\sigma_{g}$. We shall refer to the function in \cref{eq:bias}
as the\emph{intuition}, or the \emph{biased prior}. We now describe
how one can extract the useful information from the biased prior,
sidestepping the issues of it being unrepresentative: In PPR, a point
with random coordinates is drawn from an \(n+1\) dimensional space
with vectors:
\begin{equation}
  \tilde{\bm{\theta}} = (\theta_{1}, \theta_{2}, \ldots, \theta_{n}, \beta). 
\end{equation}
Live points with $\tilde{\bm{\theta}}$ corresponding to higher
${\cal L}$, are favoured, so are the values of $\beta$ for which
$\tilde{\pi}(\bm{\theta})$ is more similar to ${\cal P}(\bm{\theta})$.

In our case, this feedback ensures that if the true posterior lies
within the typical set: the region of radius \(\sigma / \beta\) of the
guessed value of \(\bm{\mu}_{g}\), then the new points are chosen
preferentially from the posterior region, including values of
\(\beta\) consistent with this. Specifically, if our hypothesis were
completely wrong, then values of \(\beta \rightarrow 0\) would be
favoured, and $\beta\rightarrow 1$ otherwise. The effective prior
would then tend to a uniform distribution. This is ensured by the
re-normalisation of \({\cal L}\).

\begin{figure}
 \input{./illustrations/ppr.tex}
 \caption{\label{org44950de} Demonstration of
   \(\tilde{\pi}(\bm{\theta}; \beta)\) for different values of
   \(\beta\). Note that we've assumed that the distribution is a
   truncated Gaussian, i.e.~zero outside the region \((-1, 1)\), which
   manifests as changes in curvature at the boundaries. Note that
   Observe that the area under curves for different $\beta$ is always
   normalised to unity. }
\end{figure}

With a representative prior, whenever live points are chosen, they
have a higher chance of enclosing more evidence. Hence fewer
iterations are needed, and hence PPR is faster, which is what we
observe in practice: \cref{fig:benchmark}.

\subsection{General automatic posterior repartitioning}

Let us recap the key components of posterior repartitioning. We
have a baseline uniform prior, with its likelihood \((\pi(\bm{\theta}),
   \cal L (\bm{\theta}))\), and a parameterised pair of biased prior and
likelihood \((\pi'(\bm{\theta}; \beta), \cal L' (\bm{\theta}; \beta))\), which
satisfy the following requirements.

\begin{enumerate}
\item For some \(\beta_{0}\), 
\begin{subequations}
\begin{align}
\tilde{\pi}(\bm{\theta}; \beta_{0}) &\equiv \pi(\bm{\theta}) \\
\tilde{\cal L}(\bm{\theta}, \beta_{0}) &\equiv {\cal L}(\bm{\theta})
\end{align}
\end{subequations}
known as the \textbf{specialisation property}.\label[Property]{spec-prop}
\item The product of the parameterised pair is constant for all values
of \(\beta\) and by~\cref{spec-prop}, 
\begin{equation*} 
\pi'(\bm{\theta}; \beta) \cal L'(\bm{\theta}; \beta) = \pi(\bm{\theta}) \cal L (\bm{\theta}),
\end{equation*}
which is the \textbf{normalisation property}.\label[Property]{norm-prop}
\item There's a guiding dynamical principle that favours the
representative prior \(\pi_{R}\), i.e. \(\beta\rightarrow\beta_{R}\)
that satisfies
\begin{equation*}
  \lim_{\beta\rightarrow\beta_{R}}\cfrac{\int \pi(\bm{\theta}, \beta) - \pi_{R}(\bm{\theta}) d\bm{\theta}}{\beta - \beta_{R}} = \min
\end{equation*}
which we call the \textbf{convergence property}.\label[Property]{vconv-prop}
\end{enumerate}

PPR satisfies all three properties as follows:~\vref{spec-prop} is
fulfilled with \[\pi'(\bm{\theta}; 0) =\pi(\bm{\theta}),\]~\cref{norm-prop} is
fulfilled by construction and~\cref{vconv-prop}, by noting that
\[\lim_{\beta \rightarrow 0} \pi'(\bm{\theta}; \beta) =
  \pi(\bm{\theta}).\]

Any pair of functions \(\pi'(\bm{\theta}; \beta)\),
\({\cal L}'(\bm{\theta}; \beta)\) that satisfy these requirements
constitute a repartitioning scheme. They are all guaranteed to yield
the same evidence and posterior, so our remaining task is to identify
schemes that produce better performance. In the following subsections
we shall consider several such possibilities.

\subsubsection{Re-sizeable-bounds uniform prior}\label{sec:orga67f872}

The three requirements outlined at the beginning of this section are
not necessary and sufficient. As we have noted in
section~\vref{domain-discussion}, the domains of all functions need to
be consistent, otherwise~\vref{eq:bayes} no longer holds, and our
analysis wholly is invalid. The mathematical implications of
neglecting function domains have in the context of Quantum mechanics
has been discussed by~\cite{Gieres_2000}.

To illustrate, consider a uniform prior with the following
parametrisation.
\begin{equation*}
  \tilde{\pi}(\bm{\theta}; \beta) = \TopHat(\bm{\theta}; \beta \bm{a}, \beta \bm{b})
\end{equation*}
Although there are no issues when \(\beta>1\) (we set
\({\cal\tilde{L}}(\bm{\theta}; \beta>1)=0\)), one can immediately
spot the issues with \(\beta \in (0,1)\); and \(\beta=0\) is
altogether nonsensical.

This issue indicates that the prescription of keeping \[\pi {\cal
	L} = \text{Const.}\] is not complete. Nevertheless, such a scheme
may be salvaged, with counter-intuitive extensions, e.g. for a
point \(\bm{\theta}_{0} \notin \Psi\), we don't expect
\[{\cal L}(\bm{\theta}_{0}) \rightarrow \infty,\] but as we shall see in
the next section, \[{\cal L}(\bm{\theta}_{0}) \rightarrow 0.\]

The first crucial step is to recognise that the algorithm draws
from a unit hypercube with uniform probability, and that the prior
is an artifact of a coordinate transformation which we referred to
as the prior quantile.

Let \(u\) be a point in unit hypercube \(\Psi_{C}\). The quantile
defines a mapping functionally dependent on the PDF of the prior
\[C(\beta)\lbrace \tilde{\pi}\rbrace:u \mapsto \bm{\theta},\] such that
the uniform distribution of \(\bm{u}\) leads through
\(C_{\beta}\{\tilde{\pi}\}(\bm{u})\) to a \(\tilde{\pi}(\bm{\theta};\beta)\)
distribution of \(\bm{\theta} \in\Psi(\beta)\).Note that we replaced the
parametrisation of the function \(\tilde{\pi}\) with an explicit
parametrisation of the coordinate transformation, specifically
\begin{equation*}
  \pi(C(\beta)\{\tilde{\pi}\}(u)) \equiv \tilde{\pi}(\bm{\theta}; \beta),
\end{equation*}
where 
\begin{equation*}
  \tilde{\pi} =  \pi \circ C(\beta) \{ \pi \} 
\end{equation*}
is a parameterised distribution resulting from a parameterised
coordinate transformation of an un-parameterised prior PDF. We shall
have~\vref{eq:bayes} hold only in the hypercube
\begin{equation*}
{\cal \hat{P}}(u) = {\cal P}(C(\beta_{0}){\tilde{\pi}}^{-1}(\bm{\theta})) = \cfrac{\hat{\pi} (u) {\cal \hat{L}}(u)}{\int_{\Psi}{\cal \hat{L}}(u) \hat{\pi}(u) du},
\end{equation*}
which is always true, regardless of the repartitioning
scheme. Trivially, the functional form of \(P(\bm{\theta})\) is not the same
as \(P(u)\); it's related via a co-ordinate transform, which in our
case contributes a Jacobian factor \(J(\beta)\{\tilde{\pi}\}\) to the
evidence. But since we're interested in the posterior in the
coordinates \(\bm{\theta}\), given by the transformation \(C(\beta_{0})\{\tilde{\pi}\}\),
while the prior and the likelihood are in the from corresponding
to \(\beta\).

Finally, 
\begin{equation*}
 {\cal P}(\bm{\theta}) = \cfrac{J(\beta_{0})}{J(\beta)} \cfrac{\pi(\bm{\theta}; \beta) {\cal L}(\bm{\theta}; \beta)}{\int \pi(\bm{\theta}; \beta) {\cal L}(\bm{\theta}; \beta) d \bm{\theta}}.
\end{equation*}
So we expect that for the simple case of scaling the uniform box
prior with \(\beta\), that we need to re-scale the likelihood by
\(\beta^{2n}\). The second Jacobian factor enters the likelihood because
we have normalised \(\pi(\bm{\theta})\), but not \(\pi(\bm{\theta}; \beta)\). This is hinted at in
the notation, (no tilde), and when accounted for, gives  the correct
posterior and evidence as seen in the experiments.

\subsubsection{Argument scaling repartitioning}\label{sec:orgfe92f25}

Power posterior repartitioning in the case of a Gaussian
distribution (also a Cauchy distribution), can be thought of as
scaling the distribution using \(\beta\).

We shall discuss multiple forms, of such repartitioning schemes,
and extend the idea to discontinuous distributions, such as a
re-sizeable uniform prior.  

So far, the main practical considerations for choosing such a
distribution is that for some attainable value of \(\beta\), the
distribution resolves to a reference. For that reason, for example
the Cauchy distribution is also more convenient to treat using a
power, because the manifest reduction to a uniform distribution is
obvious when raising the entire distribution to the power of
\(\beta\), and not when it pre-multiplies the breadth parameter
\(\gamma\).

A drawback of using power repartitioning is that it's not always
possible to find an analytical result for \(Z(\beta)\{\pi\}\), indeed
in the case of trigonometric distributions, such as \(Z(\beta)\{\pi\}\),
was proven to only be analytical if \(\beta\), is an integer, and
proven not to be analytical otherwise \citep{Liouville1837}. Mixture
repartitioning on the other hand can easily cope with such
functions, as it only requires for them to be normalised once
(e.g.~for \(\beta=0\) and \(\beta=1\)), and re-use the normalisation
factor.

\subsection{Isometric mixtures of repartitioning schemes}
An \emph{isometric model mixture}, is a model that incorporates
information from multiple repartitioning schemes of the same model,
preserving the posterior and evidence.

Each of the following shall be proven to be a repartitioning
scheme. They are useful also when mixing models with few overlapping
parameters, however these cases are beyond the scope of this project.
\subsubsection{Additive isometric mixtures}\label{sec:org418133f}
Consider a weighted sum of a uniform distribution with
a Gaussian, e.g.~in one dimension
\begin{equation}\label{eq:additive-mix}
  \pi(\bm{\theta}) = \frac{ \left\lbrace \cfrac{1- \beta} {\bm{b} - \bm{a}} + \beta \exp \left[ -\left(\cfrac{\bm{\theta} - \bm{\mu}}{\bm{\sigma}} \right)^{2}\right]\right\rbrace \cdot \TopHat(\bm{\theta}; \bm{a}, \bm{b})}{Z}.
\end{equation}
where \[\TopHat(\bm{\theta};\bm{a},\bm{b}) = \prod_{i}
	\TopHat(\theta_{i}; a_{i}, b_{i})\] is the top-hat function. Integrate
to obtain the normalisation factor \(Z(\beta)\{\pi\}\), utilised
to re-scale \({\cal L}\). Recall, however, that we represent the
prior via the inverse of the cumulative distribution. The iCDF of
each component is usually known, however the iCDF of their sum, is
not guaranteed to be representable in closed form.

\begin{figure}
  \input{illustrations/additive_mixtures.tex}
\caption{\label{orgda3e5e9}
An illustration of the additive mixture repartitioning. PPR for the same value of \(\beta=0.3\), added for comparison.}
\end{figure}

This inconvenience, can be mitigated, since the probability
density functions (PDF) \[\pi_{i}(\bm{\theta}; \beta) >0,\] the
cumulative distribution functions (CDF)
\[\CDF\{\pi_{i}\}(\bm{\theta};\beta) = \int_{\Psi}
	\pi_{i}(\bm{\theta}; \beta)d\bm{\theta}\] are monotonic;
so is their sum. Hence the iCDF exists, and can be computed
numerically. While we did not have to resort to numerical methods
in the PPR case for a Gaussian, for general distributions
computing the iCDF for \(\pi^{\beta}\) will prove more
computationally intensive than inverting the sum.

One significant improvement over PPR is in likelihoods. For two
priors \(\pi_{1}\) and \(\pi_{2}\), normalising the likelihoods is
trivial:
\begin{equation*}
{\cal L}(\bm{\theta}; \beta) = \cfrac{{\cal L}_{1}(\bm{\theta}) \pi_{1}(\bm{\theta})}{\tilde{\pi}(\bm{\theta}; \beta)}.
\end{equation*}
where we've assumed that \[{\cal L}_{1}(\bm{\theta})\pi_{1}(\bm{\theta})
	={\cal L}_{2}(\bm{\theta}) \pi_{2}(\bm{\theta}).\] This generalises
straightforwardly to \(\pi_{i}\) for all \(i\). The likelihood is a
well-behaved function in the prior space, (because we've required
the priors be non-zero in their domain), which is not guaranteed
for every value of \(\beta\) and every \(\pi(\bm{\theta})\) in PPR.

Another advantage is that by construction the normalisation factor
\[Z \{ \pi\}(\beta) = 1\] for arbitrary \(\beta\). This saves
considerable effort: one does not care if the Gaussian is
correlated,\footnote{one could argue that correlated-ness is irrelevant,
as one can always diagonalise the covariance matrix. The problem,
however, is thus transferred onto the boundary, where for a narrow
prior the orientation of the rectangle's edges in the covariance
eigen-basis can cause issues.} or if the boundaries of the
uniform prior are at an angle.

A flaw, (which additive mixtures share with PPR), is that the
probability of having no bias is negligible. There's always a
preferred direction: if our original prior were uniform, the
probability of having no bias: the probability of drawing the
value \(\beta=0\) at random is negligible. It is not nil; not in our
case, where \(\beta\) can only be a machine-representable 64-bit
floating point number; however this is sufficient to bias the
sampler for almost all values of \(\beta\) (see
\cref{fig:convergence}).

In terms of numerical computations, additive mixtures don't
significantly outperform PPR, though they may be preferable if
inverting the sum is cheap. However with Gaussian priors, additive
mixtures impractical due to unstable (loss of precision in floating
point operations) and expensive numerical inversion. That said,
additive mixtures may be useful. We have not identified a case, where
an additive mixture would be better than a stochastic one, but our
testing is not exhaustive, and such pathological cases may exist.


\subsubsection{Stochastic superpositional isometric mixtures}

Consider repartitioning schemes
${\cal M}_{i} = (\tilde{\pi}_{i}, \tilde{\cal L}_{i})$ for
$i=1, \ldots, m$. By adding \(m-1\) extra parameters $\bm{\beta}$,
construct the parameterised prior:
\begin{equation*}
  \tilde{\pi}(\bm{\theta}; \beta)  \triangleq \begin{cases}
	\tilde{\pi}_{1}(\bm{\theta}) & \text{with probability } \beta_{1},\\
	& \vdots,\\
	\tilde{\pi}_{n}(\bm{\theta}) & \text{with probability } (1- \sum_{i}^{m}\beta_{i}),
	\end{cases}
\end{equation*}
and likelihood:
\begin{equation*}
  \tilde{\cal L}(\bm{\theta}; \bm{\beta})  \triangleq
  \begin{cases}
	\tilde{\cal L}_{1}(\bm{\theta}) &  \text{with probability } \beta_{1},\\
		    &\vdots,\\
	\tilde{\cal L}_{m}(\bm{\theta}) & \text{with probability} (1- \sum_{i}^{m}\beta_{i}).
\end{cases}
\end{equation*}
This is a general superpositional mixture of $\{{\cal M}_{i}\}$ if and only if
\begin{equation}
  \label{eq:sspr}
  \tilde{\pi}(\bm{\theta}; \bm{\beta}) = \tilde{\pi}(\bm{\theta})_{i} \Leftrightarrow \tilde{\cal L}(\bm{\theta}; \bm{\beta}) = \tilde{\cal L}_{m}(\bm{\theta}; \bm{\beta}), 
\end{equation}
that is, the branch of each function is chosen consistently.

The~\cref{spec-prop,norm-prop} are satisfied by
construction. The~\cref{vconv-prop} is satisfied using the same
feedback mechanism as PPR: the likelihood is determined by
\(\bm{\theta}\), and \(\bm{\beta}\) s that lead to higher likelihoods are
favoured. The corresponding limit being minimum is satisfied as
each Riemann sum in the integral has a higher probability of being
minimised as \(\bm{\beta}\rightarrow\bm{\beta}_{R}\). In other words, the
convergence property is satisfied probabilistically. Thus, this is
a valid posterior repartitioning scheme.

\emph{Stochastic superpositional isometric mixture} repartitioning
(SSIM) ensures branch consistency by requiring
\begin{equation*}
\tilde{\pi}(\bm{\theta}; \bm{\beta}) = \tilde{\pi}_{F(\bm{\theta};
  \bm{\beta})}(\bm{\theta};\bm{\beta}),
\end{equation*}
where $F(\bm{\theta}; \bm{\beta})$ is a function. In our implementation
\begin{equation*}
  F(\bm{\theta};\bm{\beta})= \text{{\cal N}}_{m}\left(\text{pseudo-random}(\bm{\theta}); \bm{\beta}\right)
\end{equation*}
where \({\cal N}_{m}\) is the smallest index \(n \leq m\) for
which \[x > \sum_{i}\beta_{i}.\]

Our implementation of SSIM is illustrated in \cref{fig:mixture}.

Domains of individual models are not a concern, provided we require
that if $\theta_{e} \not\in D(\pi_{i})$ then
${\cal L}_{i}(\theta_{e})=0$ for $i=1,\ldots,m$, contrary to
\cref{norm-prop}. Thus the effective domain of SSIM is the set union
of the domains of its constituents.

\begin{figure}
 \input{./illustrations/mixture.tex}
\caption{An example of a mixture repartitioning. Notice that the mixture is not normalised to emphasise the coincidence of values with both the uniform distribution and a Gaussian.\label{fig:mixture}}
\end{figure}



\section{Practical measurements}
\subsection{Performance}
We shall adopt the weighted accounting approach, which common in
computer science, to measure performance using the number of
\({\cal L}\) evaluations as the unit. Additionally, the
Kullback-Leibler divergence \({\cal D}\)~\cite[see]{Kullback_1951} is
used to estimate both the speed and the agreement of posteriors. It
defines an ordering on distributions: it's less for similar
distributions, ${\cal D}=0$ if identical everywhere in their domain.

\begin{figure}
  \input{./illustrations/kullback-leibler.tex}
\caption{Kullback-Leibler divergence \({\cal D}\) for different offsets: Gaussian peaks displaced from \(\bm{\mu}\) by \(\text{Offset}\times \bm{\mu}\). Notice that the faster repartitioning methods produce a lower value of \({\cal D}\). The divergence \({\cal D}\) scales linearly with the offset.\label{fig:kl-d}}
\end{figure}

\begin{figure}
  \input{./illustrations/scaling-kld.tex}
\caption{Scaling of number of likelihood calls as a function of Kullback-Leibler divergence \({\cal D}\). The best fit line indicates that \({\cal D}\) is a reliable performance indicator for \texttt{PolyChord}.\label{fig:kl-scaling}}
\end{figure}


\begin{figure}
\input{./illustrations/histograms.tex}
\caption{An illustration of the evidence distributions of different
  types of repartitioning schemes. The reference is
  \(\log {\cal Z} = -62 = - \log V(\Psi)\), from \cref{eq:evidence},
  where \((a,b)=(-6, 6)\cdot 10^{8}\) and \(G=\mathds{1}_{3}\). Note
  that both SSIM and PPR have made fewer misestimates of ${\cal
    Z}$. If the repartitioning is done incorrectly, as with $R$, which
  is re-sizeable uniform repartitioning with the ${\cal L}$ correction
  deliberately containing only one Jacobian factor, the evidence
  estimates are systematically shifted. Mixture repartitioning is able
  to mitigate the shift of $R$ and compute $\ln {\cal Z}$
  correctly.\label{fig:hist}}
\end{figure}

Unfortunately, while a full analysis of the posterior distributions
would be much more in the spirit of Bayesian analysis, the data-sets
being are huge, so one cannot practically include all of the
\emph{marginalised posterior plots} to prove the correctness of a
run. We shall provide one example, and drop the discussion: one should
assume that the posteriors coincide unless otherwise specified. In the
latter case, the misfit is quantified by Kullback Leibler divergence
from the true posterior distribution.


\begin{figure}
  \includegraphics[width=0.5\textwidth]{./illustrations/triangle-fit.pdf}
  \caption{An example of a posterior obtained with PPR, based on
    Planck parameter covariance matrix, compared with the Planck
    posterior chains. The differences in the distributions indicate
    variance across different inference
    runs. \label{fig:overlay-posteriors}}
\end{figure}


\subsection{Simulations}
\subsubsection{Numerical models}

We shall begin our analysis with help of a simplified model that is
general-enough to share features with the Cosmological scale problem,
but also practical to investigate in depth, with multiple variations.

Our original model is a Gaussian peak. By choosing the uniform prior
as a baseline, and setting the log-likelihood as:
\begin{equation*}
  \ln {\cal L}(\bm{\theta}) = - \dfrac{1}{2} \left\{{(\bm{\theta} - \bm{\mu})}^{T}G^{-1}(\bm{\theta}-\bm{\mu})  + \ln \det \left| 2\mathrm{\pi} \bm{\Sigma}\right| \right\},
\end{equation*}
where the covariance matrix \(\bm{\Sigma}\), specifies the extent of
the peak, and the vector \(\bm{\mu}\), its location. We thus expect
the posterior to be a truncated and re-scaled Gaussian. However its
typical set is still approximately at a distance of the square root of
the diagonal elements of the covariance matrix form the peak,
i.e.~\emph{one standard deviation}.

The covariance matrix is positive semi-definite and symmetric, hence
it can be diagonalised \citep{taboga2017lectures}. If the covariance
matrix is diagonal, the Gaussian distribution is called
uncorrelated. If all diagonal elements are equal, then the Gaussian is
spherical with characteristic diameter given by
\(2\bm{\sigma} = 2\sqrt{\bm{\Sigma}}\), where \(\bm{\Sigma} = \Sigma \mathds{1}\).


Under such circumstances it's a matter of integrating \cref{eq:def-z}
to obtain the evidence. Most generally
\begin{equation}\label{eq:evidence}
   {\cal Z} = \cfrac{{\left( \sqrt{ \det \left| 2\mathrm{\pi} \bm{\Sigma} \right|} \right)}^{n}}{\bm{b}-\bm{a}}, 
\end{equation}
where \(n\) is the number of parameters in the model.

To simulate imperfections we shall consider translational offsets
between the intuition prior and the likelihood\footnote{for PR the
  breadth of the distributions is irrelevant.}.


\section{Results and Discussion.}\label{sec:org50493c6}
The first test case is an uncorrelated spherical Gaussian posterior
in three dimensions \[\mathcal{P}(\bm{\theta}) = G(\bm{\theta}; \bm{\mu} =
  (1,2,3),\bm{\sigma} = \mathds{1}).\] The corresponding evidence
(\cref{eq:evidence}) is \(\mathcal{Z}\approx-62.3\). First we shall
assume that the mean and standard deviation of all the
repartitioning schemes is exactly the same as that of the
posterior.

All but one repartitioning scheme yielded the correct evidence. The
resize-able uniform prior model was constructed to systematically
overestimating the evidence (\cref{fig:hist}) which is due to
underestimating the normalisation factor for
\(\mathcal{L}\).


We shall now show that repartitioning is able to drastically reduce
the run-time compared to using a uniform prior. More specifically,
guessing a posterior distribution and using repartitioning, one may
reduce the initial compression stage to virtually none.

Having proven the correctness of the runs, let's turn to performance
and benchmarks. The central metric is the number of \({\cal L}\)
evaluations. \cref{fig:benchmark} shows that mixture
repartitioning, produces a significant speed-up compared to even
power-posterior repartitioning. Moreover, the slope of the curve of
the number of \({\cal L}\) evaluations is much steeper for the
slower repartitioning schemes, indicating that for large numbers of
live points, mixture repartitioning yields an even greater
speed-up.



\begin{figure}
  \input{illustrations/benchmark.tex}
  \caption{number of ${\cal L}$ evaluations as a function of the
    number of live points. From the slope of best-fit lines, the
    number of evaluations scales as $\kappa n_{live}^{1.1 \pm 0.2}$,
    where $\kappa$ reduces across repartitioning
    schemes. \label{fig:benchmark}}
\end{figure}




The next trial involved a variable offset, where convergence to the
correct posterior and evidence is not guaranteed even with the correct
normalisation. For this case, we have taken the same Gaussian
truncated to a cube \(1000\times1000\times1000\). Two types of
sampling runs were considered: one where the posterior and prior
distributions coincided, and one with the mean of the posterior
shifted relative to the prior by an amount proportional to the mean
$\mu = (1,2,3)$.

The exemplary results are given in \cref{fig:convergence}.

The main notable feature is the inaccuracy of the posterior for
PPR. If the offset is small --- \(O(2\sigma)\), the posterior is
shifted. With a larger offset, e.g. \(O(4\sigma)\), two peaks can be
resolved, sadly, with less density near the correct Gaussian
peak. Both errors are compounded by incorrect evidence (see
\cref{fig:drift}) PPR: \(\ln {\cal Z}\approx -25.4 \pm 0.2\), vs
uniform reference \(\ln {\cal Z} = -22.7 \pm 0.4\).

In practice one has the following options:
\begin{enumerate}
\item accept the posterior as is~\label[Option]{opt:accept}
\item accept the posterior, but as a less credible result\label[Option]{opt:accept-with-err}
\item reject the PPR result entirely, and perform a run with only a
uniform prior\label[Option]{opt:uniform}
\item readjust the PPR mean and variance using the posterior, and
re-run~\label[Option]{opt:shift}
\item combine PPR with SSIM in mixture with a uniform prior
\end{enumerate}
\vref{opt:accept} is adequate for low accuracy standards provided the
error is properly estimated using a tool such as \texttt{nestcheck}.
From \cref{fig:benchmark}, we see that the performance uplift allows
for \cref{opt:shift} to be more efficient than~\ref{opt:uniform},
albeit marginally so. \Cref{opt:accept-with-err} is a last resort.

This is where our technique is most useful: one obtains, as we've
shown in~\cref{fig:convergence}, a more accurate
\({\cal P}(\bm{\theta})\), by using PPR from within SSIM. The
performance impact has considerable run-to-run variance, however never
exceeds \(20\%\) extra \({\cal L}\) evaluations, which is an order of
magnitude less than~\vref{opt:uniform,opt:shift} would afford.

\begin{figure}
\includegraphics[width=0.5\textwidth]{./illustrations/convergence.pdf}
\caption{An illustration of offsets affecting ${\cal P}$ under various
  repartitioning schemes. The offset models must produce an offset
  posterior (offset uniform series), given the same $\pi$ as model
  runs. The mixture is SSIM of PPR with the reference
  uniform.\label{fig:convergence}}
\end{figure}

\begin{figure}
  \input{./illustrations/evidence-drift.tex}
  \caption{An illustration of offsets affecting ${\cal Z}$. The true
    value is constant, mirrored by the mixture: SSIM of PPR and
    reference uniform. PPR alone produces incorrect
    evidence, consistent with \cref{fig:convergence}. \label{fig:drift}}
\end{figure}


Lastly, \emph{posterior mass} is a measure of converge
speed~\cite{higson2018nestcheck}, often used in diagnosing nested
sampling. Typical examples of posterior mass for a run with
$\pi=\text{Const.}$ and runs accelerated by posterior repartitioning
are given in \cref{fig:higson}. Notice that the repartitioned series
has a longer extinction phase, as a result of introducing extra
nuisance parameters. Also, the confidence intervals on each parameter
between the uniform and the repartitioned run are identical,
signifying that we have not lost precision.

\begin{figure}
\includegraphics[width=.5\textwidth]{./illustrations/higson.png}
\caption{plot of the evolution of nested sampling. The \color{red} red
  \color{black} series corresponds to SSIM of IGPR, while the
  \color{blue} blue \color{black} series --- to a reference
  uniform. The horizontal axis of plots in the second column is
  \(\ln X\), where \(X(\mathcal{L}) \in [0,1]\) is the fraction of the
  prior with likelihood greater than \(\mathcal{L}\). The top plot is
  the relative posterior mass. In row $i$ the ${\cal P}(\theta_{i})$
  is plotted. Confidence intervals represented with color
  intensity. \label{fig:higson}}
\end{figure}




\subsection{Cosmological Simulations.}\label{sec:orgb81c159}
After an initial run of \texttt{Cobaya}, we have obtained the marginalised
posteriors of all the key parameters of the \(\Lambda\)CDM model,
as well as the nuisance parameters.

Ignoring any off-diagonal elements of their co-variance, we have
constructed a mixture re-partitioned prior, containing a Gaussian
with our best estimates, a uniform containing the original
boundaries. A second run was thus performed.

Benchmarking on a cluster using time is impractical. Instead we
measured the number of likelihood calls for each invocation of
\texttt{PolyChord.run\_polychord()}.

The result is a \textbf{substantial} reduction in run-time.




\section{Conclusions}\label{sec:orgdf2cbd9}

\subsection{Results}\label{sec:orgc48c55d}
Our project's purpose had been to investigate the performance
increase attainable by algorithmic optimisations of the inputs to
nested samplers.

We have identified a general prescription, named superpositional
mixture repartitioning that netted the same if not greater
performance improvement as power posterior repartitioning (PPR).

We have also established that the aforementioned prescription had a
number of advantages:
\begin{enumerate}
\item it allows multiple priors to be mixed, while PPR only allows
one.
\item it permits a broader class of functions, than are practical for
PPR, e.g.~ones where \(Z_{\pi}(\beta)\) cannot be represented in
closed form.
\item it copes with functions having different domains. PPR cannot.
\item it is abstract, i.e.~the prior iCDF is a superposition iCDFs of
the constituents priors. For PPR the iCDF needs to be computed.
\item it supports an unbiased reference (uniform) prior exactly. PPR
tends to an unbiased reference as \(\beta\rightarrow\beta_{0}\).
\item it is able to mitigate improper re-scaling of the likelihood. If
one of the priors is improperly normalised, the offset from the
true evidence is reduced as \(n_{live}\rightarrow\infty\). PPR
does not.
\item it is resilient to human error.
\item it is easier to implement and requires little to no mathematical
input from the user, beyond ensuring the three properties.
\end{enumerate}


\subsection{Further refinements}\label{sec:org8314ddf}

A purpose-built nested sampler may be more efficient at compressing
the extinction phase, negatively affected by repartitioning. Slice
sampling if made aware of the branching may be able to mitigate the
number of rejections. 

We must also explore the implementation of Superpositional mixtures
using quantum computers. A natural mapping is to use qubits to
represent the models. We might also benefit from true quantum
randomness.



\subsection{Applications}\label{sec:orgc67317e}
Nested sampling is a universal algorithm that can be applied to any
problem involving either direct parameter estimation (e.g.~analysis of
Planck data), or indirectly such as neural-network based machine
learning.To clarify the latter, the process of training a neural
network involves a process of estimating the connection strengths
between layers of states. Normally it is done via negative feedback:
the connections correlated with the right answer are reinforced, and
vice versa.

Identifying the connection strengths as the parameters, the prior is
uniform. Our intuitive guess, represented by a Gaussian based on a
neural network performing a similar task, can be used with
repartitioning to achieve faster training.

We may also extend Bayesian analysis to \emph{consistent Bayesian
  meta-analysis}. Consider data obtained from multiple physical
processes, that are described in one theory with an overlapping set of
parameters $\theta$. As of now, we may only perform separate analyses
of each individual experiment. However, SSIM allows us to combine
these models, and require consistency in the posteriors of the shared
parameters naturally.

As an example, all of the estimates of the age of the universe may be
obtained in one fell swoop from all the available models and data, not
separately\footnote{Additional, more elaborate explanations shall be
  published in a paper submitted to the \emph{Monthly Notices of the
    Royal Astronomical Society}.}.

In conclusions, the new methodology of combining information from many
priors shows great promise in the field of Bayesian inference. It has
demonstrably reduced the run-time of some of the most complex
problems: that of Cosmological Parameter Estimation. A rich field of
research awaits those courageous to follow. It is ours but to point
the way.

\bibliography{bibliography}
\bibliographystyle{mnras}

\appendix
\section{Nested sampling in detail}\label{sec:ns}
Consider without loss of generality, a prior space \(\Psi\) that is a
unit hypercube, where \(\pi(\bm{\theta}) = \text{Const.}\) Draw
\(n_\text{live}\) random \emph{live points} from the unit
hypercube. If \({\cal L}\) is a well-behaved function, the probability
that two points have the same likelihood is vanishing, so each of them
lies on a \textbf{distinct} iso-likelihood
hyper-surface\footnote{analogy: height on a contour map. }. Each
hyper-surface encloses the fraction
\begin{equation}
\cfrac{1}{n_\text{live}}
\end{equation}
of the total volume of the hypercube on average. More specifically,
each shell's enclosed volume shall have some random deviation \(\Delta\), from
\(\cfrac{1}{n_\text{live}}\), with an associated cumulative
distribution \(P(\Delta)\).

Subsequently, we pick another point at random, requiring that the
likelihood of the new point be higher than the lowest likelihood of
the initial \emph{live point} ensemble. In \citeauthor{Skilling2006}'s
notation, the point with the lowest likelihood becomes \emph{dead} and
the new point becomes is \emph{live}. This is a single iteration of
nested sampling.

Our argument of approximately equal volumes holds for the new
ensemble, so the volume encased in the outer-most shell iteratively
reduces by the same fraction, allows us to approximate said volumes:
\begin{equation}\label{eq:recurrence-relation}
  \begin{array}{rcl}
  X_{0} &=  &1, \\
  X_{1} &= &X_{0} \left(1- \cfrac{1}{n_\text{live}}\right),\\
  & \vdots &, \\
  X_{i} &= &X_{i-1}\left(1- \cfrac{1}{n_\text{live}}\right),\\
  & \vdots, &
\end{array}
\end{equation}
Thus we iteratively pick live points in regions $\{\bm{\theta}\}$ of
high \({\cal L}\), and also estimating the evidence, and stop when the
prior volume encased in the outer shell is lower than a predetermined
fraction e.g. \(0.01\) of the original hypercube volume.

The recurrence relation~\eqref{eq:recurrence-relation} is not exact,
however, \(P(\Delta)\) is a known distribution, dependent on the
\(\dim \Psi\) and \({\cal L}\). Thus, for each \(\epsilon>0\), we
exists
\[\delta(\epsilon) >0,\] such that \[P(\Delta > \delta)<\epsilon.\]
Hence, by choosing \(\epsilon\) based on \(n_\text{live}\), one
obtains an estimate of the error \(\delta\). Propagating these errors
allows us to evaluate the prior volume, ergo: ${\cal Z}$ up to an
estimable error.

This is generalised to non-hypercube $\Psi$ and non-uniform $\pi$ via
the prior quantile. 
\end{document}