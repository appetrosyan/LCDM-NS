\documentclass[usenatbib]{mnras}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{varioref}
\usepackage{bm}
\usepackage{pgfplots}
\usepackage[nameinlink, capitalize, noabbrev]{cleveref}

\usepgfplotslibrary{groupplots,dateplot}
\usetikzlibrary{patterns,shapes.arrows,external}
\tikzexternalize
\pgfplotsset{compat=newest}
\usepackage{dsfont}
\usepackage{xcolor}
\usepackage{listings}

\DeclareMathOperator{\TopHat}{TH}
\DeclareMathOperator{\CDF}{CDF}

\author[A. Petrosyan and W. J. Handley]{
  Aleksandr Petrosyan,$^{1}$
  William James Handley$^{2}$
  \\
  $^{1}$ University of Cambridge, Queens' College, Silver st., Cambridge, CB3 9ET, UK\\
  $^{2}$ University of Cambridge, Kavli Institute of Astronomy, Madingley Road, Cambridge, CB3 0HA, UK 
}
\date{\today}
\title[Stochastic posterior repartitioning]{Bayesian Machine learning in context of cosmological parameter estimation}
\hypersetup{
 pdfauthor={Aleksandr Petrosyan, William James Handley},
 pdftitle={Bayesian Machine learning in context of cosmological parameter estimation},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.3 (Org mode 9.1.9)}, 
 pdflang={English}}
\crefname{Option}{Option}{Options}
\crefname{Property}{Property}{Properties}

\begin{document}

\maketitle
\begin{abstract}
Bayesian inference is one of the most robust scientific hypothesis testing frameworks. It is used in multiple areas of physics that require detailed analysis of the underlying hypothesis. Cosmology, one such field is faced with challenges due to the complexity of the underlying theory, making inference computationally expensive even with the state of the art Bayesian inference software:  nested sampling packages like \texttt{PolyChord}. We here present a methodology that allows one to simultaneously improve the robustness and accelerate nested sampling based Bayesian inference. In this paper we introduce a formal mathematical framework that explicates previous work in the field, as well as allows further developments to be made. We subsequently introduce three other methodologies for conducting repartitioning: argument scaling, boundary re-sizing and superpositional mixing. We demonstrate that these techniques are able to produce more accurate and more precise results, while also \emph{significantly} improving run-time characteristics of the nested sampling package they're used with. Finally, as a demonstration of its most lucrative application, we demonstrate that it is able to reduce the execution time of a \texttt{Cobaya}/\texttt{CLASS} based cosmological parameter inference by a factor of TODO. 

\end{abstract}

\begin{keywords}
Bayesian inference -- automated posterior repartitioning -- nested sampling -- cosmology: miscellaneous -- methods: statistical -- methods: data analysis
\end{keywords}

\section{Introduction}\label{sec:org14413d7}

The standard model of the universe and its evolution in modern
cosmology is the \(\Lambda\)CDM model \citep{Condon2018}, so named
after the main components of the universe: the \emph{cosmological constant} \(\Lambda\)
and \emph{cold dark matter}. It has six major independent parameters:
physical baryon density \(\Omega_{b}h^{2}\); physical dark matter
density \(\Omega_{c}h^{2}\); the age of the universe \(t_{0}\); scalar
spectral index \(n_{s}\); curvature fluctuation amplitude
\(\Delta_{R}^{2}\); and re-ionization optical depth \(\tau\). It is the
task of the present study to \emph{develop better tools} for estimating
the agreement of our observations from the Planck mission with
\(\Lambda\)CDM, simultaneously estimating the parameters. In this
section we shall describe the main approaches, as well as the
refinements to these approaches that we have developed.

The above makes Bayesian inference a convenient methodology for
estimating cosmological parameters. Hence a large number of algorithms
was developed to accelerate the computation: Metropolis-Hastings
\citep{Metropolis} in conjunction with the Gibbs sampler
\citep{Metropolis-Hastings-Gibbs}; Hybrid (Hamiltonian) Monte Carlo
\citep{1701.02434,Duane_1987}, and --- nested Sampling
\citep{Skilling2006}, which we shall focus on.

Nested sampling~\cite{Skilling2006} is a family of algorithms, each
with their own unique implementation details:
\begin{enumerate}
\item \texttt{MultiNest} \citep{Feroz2009MultiNestAE},
\item \texttt{nestle} \citep{nestle}
\item \texttt{dyNesty} \citep{Speagle_2020},
\item \texttt{PolyChord} \citep{polychord}.
\end{enumerate}
to name a few major implementations.

We have \emph{discovered} a method accelerates \emph{all}  nested sampling
implementations, each to a different extent. The following paper, was
primarily tested with \texttt{PolyChord}, however other samplers
should (and do) see similar performance gains.

Our optimisation is based on the observation due
to~\cite{chen-ferroz-hobson}, that the nested sampling algorithm,
unlike other Markov-chain Monte Carlo Bayesian inference methods, is
sensitive to how the posterior is partitioned into the two conditional
probabilities: \emph{likelihood} and \emph{prior}, with respect to the
posterior distribution. They used this to develop a technique ---
\emph{automatic power posterior repartitioning} (PPR), that they used
to improve the stability of convergence for prior distributions that
may have been at variance with the true posterior.

We have found a \emph{generalisation} of this technique, and showed
that it can be used to \emph{significantly} accelerate the convergence.

We've developed the mathematical framework that encapsulates the idea
and explores the extents of its utility. In particular, we have found
a way to describe \emph{how} one may achieve better stability and
better performance. We also present a numerical application of this
framework in terms of the following techniques that we have developed:
mixture repartitioning (superpositional and additive) and re-sizeable
bounds uniform repartitioning, as well as demonstrated that PPR can
be used to heuristically accelerate nested sampling.

\section{Theoretical background}\label{sec:orge6061a4}

\subsection{Bayesian inference}\label{sec:org6b7d2fe}

This topic has been discussed at length in literature
\citep{jeffreys2010scientific}, so we shall restrict ourselves to the
minimum required to understand what follows.

Let \({\cal M}\) be a model of some process, parameterised with
\[\bm{\theta} = (\theta_{1}, \theta_{2}, \ldots ,
  \theta_{n}).\] Meaning that it defines a probability distribution of
physical observables ${\cal O}_\text{predicted}$, contingent on model
parameters \(\bm{\theta}\). Actual experimental values of
${\cal O}_\text{actual}$ encapsulated in \(\mathfrak{D}\). From these
we build conditional probabilities. In the language of
\cref{table-defs}, \citeauthor{1763}'s theorem is
\begin{equation}\label{eq:bayes} 
 {\cal L} \times \pi (\bm{\theta}) = {\cal Z}\times {\cal P} (\bm{\theta}).
\end{equation}
With \emph{evidence} \({\cal Z}\) 
\begin{equation}\label{eq:def-z}
 {\cal Z} \triangleq \int_{\Psi} {\cal L}(\theta) \pi(\theta) d\theta, 
\end{equation}
where \(\Psi\) is the \emph{prior space} --- the domain of 
\(\pi(\bm{\theta})\).\footnote{\Cite{jeffreys2010scientific} believes \({\cal Z}\) to be no more than a normalisation factor, but by definition (see Table~\vref{table-defs}), it quantifies the consistency of the hypothesis with  observation. Essentially,  higher \({\cal Z}\), implies higher accuracy of our model.}

\begin{table}
  \caption{Definitions of main quantities in Bayesian analysis. PDF means \textbf{p}robability \textbf{d}ensity \textbf{f}unction, PVE --- \textbf{p}arameter \textbf{v}olume \textbf{e}lement, i.e. \(d \bm{\theta}\). Units given for reference, but also to emphasise the use of density functions, vs.\ cumulative distributions.\label{table-defs}}
\centering
\begin{tabular}{llrr}
\textbf{\textbf{Term}} & \textbf{\textbf{Symbol}} & \textbf{\textbf{Definition}} & \textbf{\textbf{Units}}\\
\hline
Prior (PDF) & \(\pi(\theta)\) & \(P ( \theta  \vert {\cal D})\) & PVE\(^{-1}\)\\
Likelihood (PDF) & \({\cal L}(\theta)\) & \(P ( {\cal D} \vert \bm{\theta} \cup M)\) & PVE\(^{-1}\)\\
Posterior (PDF) & \({\cal P}(\theta)\) & \(P ( \theta \vert {\cal D} \cup M)\) & PVE\(^{-1}\)\\
Evidence & \({\cal Z}\) & \(P ( {\cal D} \vert {\cal M})\) & arbitrary units\\
\end{tabular}
\end{table}

The two independent functions, \({\cal L}(\bm{\theta})\) and
\(\pi(\bm{\theta})\) are inputs to Bayesian inference machine
(sampler). Their convenient depiction depends on the particular
sampling algorithm, however, for most nested samplers
(e.g. \texttt{PolyChord}, \texttt{MultiNest}) we delineate them indirectly with
log-likelihood:
\begin{equation*}
  L(\bm{\theta}) = \ln \cal L (\bm{\theta})
\end{equation*}
and \emph{prior quantile} \(C\{\pi\}(\bm{\theta})\), which is the
\emph{inverse cumulative distribution function} (iCDF) of random
variable \(\bm{\theta}\) with the probability density function
\(\pi(\bm{\theta})\). Specifically,
\begin{equation*}
 C\{\pi\} : \text{unit hyper-cube} \rightarrow \Psi;
\end{equation*}
a mapping from a unit hypercube (where the distribution of the images
of parameters \(\bm{\theta}\) is uniform), onto the prior space
\(\Psi\).

Note, that there is a redundancy in our specification of \({\cal
   L}(\bm{\theta})\) and \(\pi(\bm{\theta})\).  Consider a
different pair of functions \(\tilde{{\cal L}}(\bm{\theta})\) and
\(\tilde{\pi}(\bm{\theta})\), such that
\begin{equation}\label{eq:redundancy}
  \tilde{\cal L}(\bm{\theta}) \tilde{\pi}(\bm{\theta}) = \cal L (\bm{\theta}) \pi (\bm{\theta}), 
\end{equation}
for all \(\bm{\theta} \in \Psi\). In this representation, the
value of \({\cal Z}\) is unchanged, hence by \cref{eq:bayes},
\({\cal P}(\bm{\theta})\) is also. Thus, \emph{most} Bayesian samplers
are indifferent to concrete definitions of \(\cal L\) and \(\pi\),
as long as their product --- the posterior, corresponds to the same
model. However, nested sampling \emph{is} sensitive to the
\emph{partitioning} of the \({\cal P} (\bm{\theta})\) into \({\cal L}(\bm{\theta})\)
and \(\pi(\bm{\theta})\), which is the centerpiece of the techniques
discussed earlier. While as we mentioned it must obtain the same
evidence and the same posterior, remarkably it may obtain those
much more quickly for some choices of \(\tilde{\pi}\). In the next
section we shall elaborate on that point.

\subsection{Nested Sampling}\label{sec:org36366f8}

Bayes' theorem reduces the problem of parameter estimation to
integration. Thus the naïve approach: uniformly rasterise \(\Psi\) and
\({\cal Z}\) via Riemann sums, is valid. However, for hypotheses with
\(O(30)\) parameters, said rasterisation is intractable
\citep{Caflisch_1998} and integration is done using Monte Carlo
techniques. Nested sampling is one such technique that has
considerable performance advantages in high-dimensional many-parameter
models.

Consider without loss of generality, a prior space \(\Psi\) that is a
unit hypercube, where \[\pi(\bm{\theta}) = \text{Const.}\] Draw
\(n_\text{live}\) random \emph{live points} from the unit
hypercube. If \({\cal L}\) is a well-behaved function, the probability
that two points have the same likelihood is vanishing, so each of them
lies on a \textbf{distinct} iso-likelihood
hyper-surface\footnote{analogy: height on a contour map. }. Each
hyper-surface encloses the fraction
\begin{equation}
\cfrac{1}{n_\text{live}}
\end{equation}
of the total volume of the hypercube on average. More specifically,
each shell's enclosed volume shall have some random deviation \(\Delta\), from
\(\cfrac{1}{n_\text{live}}\), with an associated cumulative
distribution \(P(\Delta)\).

Subsequently, we pick another point at random, requiring that the
likelihood of the new point be higher than the lowest likelihood of
the initial \emph{live point} ensemble. In \citeauthor{Skilling2006}'s
notation, the point with the lowest likelihood becomes \emph{dead} and
the new point becomes is \emph{live}. This is a single iteration of
nested sampling.

Our argument of approximately equal volumes holds for the new
ensemble, so the volume encased in the outer-most shell iteratively
reduces by the same fraction, allows us to approximate said volumes:
\begin{equation}\label{eq:recurrence-relation}
  \begin{array}{rcl}
  X_{0} &=  &1, \\
  X_{1} &= &X_{0} \left(1- \cfrac{1}{n_\text{live}}\right),\\
  & \vdots &, \\
  X_{i} &= &X_{i-1}\left(1- \cfrac{1}{n_\text{live}}\right),\\
  & \vdots, &
\end{array}
\end{equation}
Thus we iteratively pick live points in regions $\{\bm{\theta}\}$ of
high \({\cal L}\), and also estimating the evidence, and stop when the
prior volume encased in the outer shell is lower than a predetermined
fraction e.g. \(0.01\) of the original hypercube volume.

The recurrence relation~\eqref{eq:recurrence-relation} is not exact,
however, \(P(\Delta)\) is a known distribution, dependent on the
\(\dim \Psi\) and \({\cal L}\). Thus, for each \(\epsilon>0\), we
exists
\[\delta(\epsilon) >0,\] such that \[P(\Delta > \delta)<\epsilon.\]
Hence, by choosing \(\epsilon\) based on \(n_\text{live}\), one
obtains an estimate of the error \(\delta\). Propagating these errors
allows us to evaluate the prior volume, ergo: ${\cal Z}$ up to an
estimable error.

This is generalised to non-hypercube $\Psi$ and non-uniform $\pi$ via
the prior quantile. 

The time complextity of nested sampling $T$ satisfies the following
relations.
\begin{subequations}\label{eq:complexity}
  \begin{align}
    T \propto & n_\text{live}\\
    T \propto & \langle {\cal T}\{{\cal L}(\bm{\theta})\} \rangle
  \end{align}
\end{subequations}
where ${\cal T}$ represents time complexity of the
function. Corollary: \emph{rejection sampling} --- drawing a point at random,
and re-drawing at random based on the criteria mentioned, is less
efficient than \emph{slice sampling} \citep{Neal_2003}.

Generally, if the prior is informative, one can design an
implementation of nested sampling which incorporates that extra
information, to accelerate nested sampling --- an ideal sampler. It
would converge optimally when the prior and the posterior coincide:
\begin{equation}
\begin{array}{rl} 
{\cal P}(\bm{\theta}) = \pi(\bm{\theta}), & \forall \bm{\theta}. 
\end{array}
\end{equation}
Many algorithms, however, may have minimum run-time for non-coinciding
informative priors.

\subsection{Unrepresentative priors\label{discussion-bias}}
The choice of prior is relatively arbitrary, yet we have
demonstrated that one can choose them differently accelerating
inference.

So why not just adjust our prior based on intuition?  To illustrate,
consider that one has gathered data from free fall experiments. On
earth, one knows the posterior for \(g\) to be a normal distribution
peaked at \[\langle g \rangle=9.81,\] with standard
deviation \[\sigma_{g} = 0.01\] due to regional variations, which we
shall compactly refer to as
\begin{equation*}
  \pi(\bm{\theta}) = G(\bm{\theta};\bm{\mu}_{g}=9.81, \bm{\sigma}_{g}=0.01, \ldots ).
\end{equation*}
We use this to obtain a posterior distribution.

So if one chooses a prior without changing the likelihood, the
posterior would be different. If the prior changed due to new
information, this is desired. If the prior was chosen ad-hoc, this
limits the utility of the posterior and evidence.


This is the problem of \emph{unrepresentative priors} and
\citeauthor*{chen-ferroz-hobson} have developed power-posterior
repartitioning specifically as a mitigation of this issue. 

\subsection{Power posterior repartitioning}\label{sec:org68fff63}

\citeauthor{chen-ferroz-hobson} have proposed introducing an
extra parameter \(\beta\) that re-scales the prior:
\begin{equation*}
  \tilde{\pi}(\bm{\bm{\theta}};\beta) = \cfrac{\pi(\bm{\theta})^{\beta}}{Z(\beta)\{\pi\}},
\end{equation*}
where \(Z(\beta)\{\pi\}\) is a normalisation factor, i.e. 
\begin{equation*}
  Z(\beta)\{\pi\} = \int_{\bm{\theta} \in \Psi} \pi(\bm{\bm{\theta}})^{\beta}d\bm{\bm{\theta}}.
\end{equation*}
In their prescription, the likelihood changes to
\begin{equation*}
  \tilde{\cal L}(\bm{\theta}) = {\cal L}(\bm{\theta}) Z(\beta)\{\pi\} \cdot \pi^{1-\beta}(\bm{\theta}).
\end{equation*}
The domains of all functions need to be the same. Let
\(D(f)\) denote the domain of the function \(f\), i.e.~where the
function is both defined and \textbf{non-zero}. Hence
\begin{equation*}
  D(\pi) = D({\cal L}) = \Psi = D({\cal P}),
\end{equation*} 
meaning the posterior is within the domain of the prior and
likelihood, which will be important later.\label{domain-discussion}

There is no general prescription for determining the prior of
\(\beta\). The tightest constraints on \(\beta\) produce the best
convergence speed, however broad constraints may be valuable.  If
\(\pi(\bm{\theta})\) is Gaussian, a uniform 
\[\beta\in[0,1]\] prior is a convenient starting point.
If the original prior \(\pi\) was representative, one could introduce
a non-linear map that favours \(\beta\approx 1\) making
\(\tilde{\pi}\) more representative. If the original prior may have
been too broad (overestimated errors) we could extend it
to \[\beta>1.\] Rarely, \[\beta<0,\] may also be useful.

This, for the cases that \citeauthor{chen-ferroz-hobson} have
originally considered, resolves the issue of non-representative
priors, because the evidence associated with the biased prior reduces
as \(\beta\rightarrow0\). This is meant to trade performance for
mitigating systematic errors, though faster than a uniform
prior.\footnote{in practice, the overhead associated with PPR is
  negligible, and even in the case of uni-variate examples, where the
  relative impact of adding an extra parameter is maximal, the
  overhead is insignificant~\cite[see][section ``numerical
  examples'']{chen-ferroz-hobson}.}

Notice that the \citeauthor{chen-ferroz-hobson}'s construction is
only useful if the prior we started with --- \(\pi\), was
peaked. Indeed, raising a uniform prior \(\pi\) to power \(\beta \in
   \mathbb{R}\) would not change it in any way.

\section{Theoretical discoveries}
\subsection{PPR for  accelerating convergence}

Our first discovery pertains to the inverted premise, where we guess a
prior to accelerate convergence.

We have a model \({\cal M}\), for which we have no prior knowledge,
hence the prior \(\pi\) is uniform.\footnote{the standard invariant
  objective prior in the general case was proven
  by~\cite{JeffreysPrior} to be the determinant of the fisher
  Matrix. A straightforward calculation thus yields that for a
  Gaussian distribution with a fixed standard deviation the Xprior is
  unity and unbounded, hence not normaliseable. Normally, however,
  it's assumed to be normalised and bounded.} Central limit theorem
suggests that the posterior may be a Gaussian:
\begin{equation}
 \pi (\bm{\theta}) \propto \exp \left[-\left(\cfrac{\bm{\theta} - \bm{\mu}}{2\bm{\sigma}}\right)^{2} \right],
 \label{eq:bias}
\end{equation}
where \(\bm{\mu}\) and \(\bm{\sigma}\) are unknown to us.\footnote{we
  have slightly abused the notation: the quotient of two vector
  quantities is undefined. Instead, in such fractions there is an
  implicit tensor index. Whenever a quantity with an implicit index is
  equated to a probability density, summation over the index is
  implied.} Based on our experience we may guess what these values may
be, without guarantee that either the shape or the location of the
posterior is given by \cref{eq:bias}.

We shall refer to this function as the \emph{intuition}, or the \emph{biased
prior}. This intuition is subjective, and therefore using it
directly, will negatively affect our posterior (\vref{discussion-bias}). Can one incorporate the useful
information if the guess is correct, without that biasing the
result? Using the guess as the initial prior \(\pi\) in PPR, seems to
produce what we need.

In detail: a point with random coordinates is drawn from an \(n+1\)
dimensional space with vectors:
\begin{equation}
  \tilde{\bm{\theta}} = (\theta_{1}, \theta_{2}, \ldots, \theta_{n}, \beta)
\end{equation}
where $\beta$ is treated by (nested sampling) the same as any other
component of \(\bm{\theta}\). This leads to live points with
$\tilde{\bm{\theta}}$ corresponding to higher ${\cal L}$, being
favoured. Consequently, the value of $\beta$ for which the prior is
similar to the posterior, is biased for.

This feedback ensures that if the true posterior is within the
region of radius \(\sigma / \beta\) of the guessed value of
\(\bm{\mu}\), then the new points are chosen preferentially from the
posterior region, including values of \(\beta\) that keep the
posterior region close to the prior peak. Specifically, if our
hypothesis were completely wrong, then the values of \(\beta
   \rightarrow 0\) would be favoured. The effective prior would then
tend to a uniform distribution. This is ensured by the
re-normalisation of \({\cal L}\).

\begin{figure}
 \input{./illustrations/ppr.tex}
\caption{\label{org44950de}
A demonstration of the function \(\tilde{\pi}(\bm{\theta}; \beta)\) for different values of \(\beta\). Note that we've started under the assumption that the distribution is a truncated Gaussian, i.e.~that it is zero outside the range \((-1, 1)\). This manifests as sharp changes in curvature at the boundaries. Note that \(\forall \beta\), \(\int_{-1}^{1}\tilde{\pi}(\bm{\theta}; \beta) = 1\).}
\end{figure}

Having demonstrated correctness, let's focus on performance. The
majority of the run-time of nested sampling with a uniform prior is
spent transplanting the live points onto the posterior
contour. With \(\beta>0\), the probability that points will be chosen
from high-likelihood regions is enhanced, so on-average the
execution time should decrease.

This is what we observe in practice: \cref{fig:benchmark}. 

\subsection{General automatic posterior repartitioning}

Let us recap the key components of posterior repartitioning. We
have a baseline uniform prior, with its likelihood \((\pi(\bm{\theta}),
   \cal L (\bm{\theta}))\), and a parameterised pair of biased prior and
likelihood \((\pi'(\bm{\theta}; \beta), \cal L' (\bm{\theta}; \beta))\), which
satisfy the following requirements.

\begin{enumerate}
\item For some \(\beta_{0}\), 
\begin{subequations}
\begin{align}
\tilde{\pi}(\bm{\theta}; \beta_{0}) &\equiv \pi(\bm{\theta}) \\
\tilde{\cal L}(\bm{\theta}, \beta_{0}) &\equiv {\cal L}(\bm{\theta})
\end{align}
\end{subequations}
known as the \textbf{specialisation property}.\label[Property]{spec-prop}
\item The product of the parameterised pair is constant for all values
of \(\beta\) and by~\cref{spec-prop}, 
\begin{equation*} 
\pi'(\bm{\theta}; \beta) \cal L'(\bm{\theta}; \beta) = \pi(\bm{\theta}) \cal L (\bm{\theta}),
\end{equation*}
which is the \textbf{normalisation property}.\label[Property]{norm-prop}
\item There's a guiding dynamical principle that favours the
representative prior \(\pi_{R}\), i.e. \(\beta\rightarrow\beta_{R}\)
that satisfies
\begin{equation*}
  \lim_{\beta\rightarrow\beta_{R}}\cfrac{\int \pi(\bm{\theta}, \beta) - \pi_{R}(\bm{\theta}) d\bm{\theta}}{\beta - \beta_{R}} = \min
\end{equation*}
which we call the \textbf{convergence property}.\label[Property]{vconv-prop}
\end{enumerate}

PPR satisfies all three properties as follows:~\vref{spec-prop} is
fulfilled with \[\pi'(\bm{\theta}; 0) =\pi(\bm{\theta}),\]~\cref{norm-prop} is
fulfilled by construction and~\cref{vconv-prop}, by noting that
\[\lim_{\beta \rightarrow 0} \pi'(\bm{\theta}; \beta) =
  \pi(\bm{\theta}).\]

Any pair of functions \(\pi'(\bm{\theta}; \beta)\),
\({\cal L}'(\bm{\theta}; \beta)\) that satisfy these requirements
constitute a repartitioning scheme. They are all guaranteed to yield
the same evidence and posterior, so our remaining task is to identify
schemes that produce better performance. In the following subsections
we shall consider several such possibilities.

\subsubsection{Additive mixtures.}\label{sec:org418133f}
Consider a weighted sum of a uniform distribution with
a Gaussian, e.g.~in one dimension
\begin{equation}\label{eq:additive-mix}
  \pi(\bm{\theta}) = \frac{ \left\lbrace \cfrac{1- \beta} {\bm{b} - \bm{a}} + \beta \exp \left[ -\left(\cfrac{\bm{\theta} - \bm{\mu}}{\bm{\sigma}} \right)^{2}\right]\right\rbrace \cdot \TopHat(\bm{\theta}; \bm{a}, \bm{b})}{Z}.
\end{equation}
where \[\TopHat(\bm{\theta};\bm{a},\bm{b}) = \prod_{i}
	\TopHat(\theta_{i}; a_{i}, b_{i})\] is the top-hat function. Integrate
to obtain the normalisation factor \(Z(\beta)\{\pi\}\), utilised
to re-scale \({\cal L}\). Recall, however, that we represent the
prior via the inverse of the cumulative distribution. The iCDF of
each component is usually known, however the iCDF of their sum, is
not guaranteed to be representable in closed form.

\begin{figure}
  \input{illustrations/additive_mixtures.tex}
\caption{\label{orgda3e5e9}
An illustration of the additive mixture repartitioning. PPR for the same value of \(\beta=0.3\), added for comparison.}
\end{figure}

This inconvenience, can be mitigated, since the probability
density functions (PDF) \[\pi_{i}(\bm{\theta}; \beta) >0,\] the
cumulative distribution functions (CDF)
\[\CDF\{\pi_{i}\}(\bm{\theta};\beta) = \int_{\Psi}
	\pi_{i}(\bm{\theta}; \beta)d\bm{\theta}\] are monotonic;
so is their sum. Hence the iCDF exists, and can be computed
numerically. While we did not have to resort to numerical methods
in the PPR case for a Gaussian, for general distributions
computing the iCDF for \(\pi^{\beta}\) will prove more
computationally intensive than inverting the sum.

One significant improvement over PPR is in likelihoods. For two
priors \(\pi_{1}\) and \(\pi_{2}\), normalising the likelihoods is
trivial:
\begin{equation*}
{\cal L}(\bm{\theta}; \beta) = \cfrac{{\cal L}_{1}(\bm{\theta}) \pi_{1}(\bm{\theta})}{\tilde{\pi}(\bm{\theta}; \beta)}.
\end{equation*}
where we've assumed that \[{\cal L}_{1}(\bm{\theta})\pi_{1}(\bm{\theta})
	={\cal L}_{2}(\bm{\theta}) \pi_{2}(\bm{\theta}).\] This generalises
straightforwardly to \(\pi_{i}\) for all \(i\). The likelihood is a
well-behaved function in the prior space, (because we've required
the priors be non-zero in their domain), which is not guaranteed
for every value of \(\beta\) and every \(\pi(\bm{\theta})\) in PPR.

Another advantage is that by construction the normalisation factor
\[Z \{ \pi\}(\beta) = 1\] for arbitrary \(\beta\). This saves
considerable effort: one does not care if the Gaussian is
correlated,\footnote{one could argue that correlated-ness is irrelevant,
as one can always diagonalise the covariance matrix. The problem,
however, is thus transferred onto the boundary, where for a narrow
prior the orientation of the rectangle's edges in the covariance
eigen-basis can cause issues.} or if the boundaries of the
uniform prior are at an angle.

A flaw, (which additive mixtures share with PPR), is that the
probability of having no bias is negligible. There's always a
preferred direction: if our original prior were uniform, the
probability of having no bias: the probability of drawing the
value \(\beta=0\) at random is negligible. It is not nil; not in our
case, where \(\beta\) can only be a machine-representable 64-bit
floating point number; however this is sufficient to bias the
sampler for almost all values of \(\beta\) (see
\cref{fig:convergence}).

In terms of numerical computations, additive mixtures don't
significantly outperform PPR.\@It may be preferable if inverting
the sum is cheap. However with Gaussian priors, additive mixtures
are held back by unstable (loss of precision in floating point
operations) expensive numerical inversion, while Gaussian PPR can
be inverted analytically. Thus we have omitted additive mixture
repartitioning from our experiments, in lieu of superpositional
mixture repartitioning. The reasoning is, that in most cases where
additive mixtures outperform PPR, superpositional mixtures
outperform both by a significant margin.

That said, additive mixtures may be useful. We have not identified
a case, where an additive mixture would be better than a
stochastic one, but our testing is not exhaustive, and such
pathological cases may exist.

\subsubsection{Re-sizeable-bounds uniform prior.}\label{sec:orga67f872}

The three requirements outlined at the beginning of this section are
not necessary and sufficient. As we have noted in
section~\vref{domain-discussion}, the domains of all functions need to
be consistent, otherwise~\vref{eq:bayes} no longer holds, and our
analysis wholly is invalid. The mathematical implications of
neglecting function domains have in the context of Quantum mechanics
has been discussed by~\cite{Gieres_2000}.

To illustrate, consider a uniform prior with the following
parametrisation.
\begin{equation*}
  \tilde{\pi}(\bm{\theta}; \beta) = \TopHat(\bm{\theta}; \beta \bm{a}, \beta \bm{b})
\end{equation*}
Although there are no issues when \(\beta>1\) (we set
\({\cal\tilde{L}}(\bm{\theta}; \beta>1)=0\)), one can immediately
spot the issues with \(\beta \in (0,1)\); and \(\beta=0\) is
altogether nonsensical.

This issue indicates that the prescription of keeping \[\pi {\cal
	L} = \text{Const.}\] is not complete. Nevertheless, such a scheme
may be salvaged, with counter-intuitive extensions, e.g. for a
point \(\bm{\theta}_{0} \notin \Psi\), we don't expect
\[{\cal L}(\bm{\theta}_{0}) \rightarrow \infty,\] but as we shall see in
the next section, \[{\cal L}(\bm{\theta}_{0}) \rightarrow 0.\]

The first crucial step is to recognise that the algorithm draws
from a unit hypercube with uniform probability, and that the prior
is an artifact of a coordinate transformation which we referred to
as the prior quantile.

Let \(u\) be a point in unit hypercube \(\Psi_{C}\). The quantile
defines a mapping functionally dependent on the PDF of the prior
\[C(\beta)\lbrace \tilde{\pi}\rbrace:u \mapsto \bm{\theta},\] such that
the uniform distribution of \(\bm{u}\) leads through
\(C_{\beta}\{\tilde{\pi}\}(\bm{u})\) to a \(\tilde{\pi}(\bm{\theta};\beta)\)
distribution of \(\bm{\theta} \in\Psi(\beta)\).Note that we replaced the
parametrisation of the function \(\tilde{\pi}\) with an explicit
parametrisation of the coordinate transformation, specifically
\begin{equation*}
  \pi(C(\beta)\{\tilde{\pi}\}(u)) \equiv \tilde{\pi}(\bm{\theta}; \beta),
\end{equation*}
where 
\begin{equation*}
  \tilde{\pi} =  \pi \circ C(\beta) \{ \pi \} 
\end{equation*}
is a parameterised distribution resulting from a parameterised
coordinate transformation of an un-parameterised prior PDF. We shall
have~\vref{eq:bayes} hold only in the hypercube
\begin{equation*}
{\cal \hat{P}}(u) = {\cal P}(C(\beta_{0}){\tilde{\pi}}^{-1}(\bm{\theta})) = \cfrac{\hat{\pi} (u) {\cal \hat{L}}(u)}{\int_{\Psi}{\cal \hat{L}}(u) \hat{\pi}(u) du},
\end{equation*}
which is always true, regardless of the repartitioning
scheme. Trivially, the functional form of \(P(\bm{\theta})\) is not the same
as \(P(u)\); it's related via a co-ordinate transform, which in our
case contributes a Jacobian factor \(J(\beta)\{\tilde{\pi}\}\) to the
evidence. But since we're interested in the posterior in the
coordinates \(\bm{\theta}\), given by the transformation \(C(\beta_{0})\{\tilde{\pi}\}\),
while the prior and the likelihood are in the from corresponding
to \(\beta\).

Finally, 
\begin{equation*}
 {\cal P}(\bm{\theta}) = \cfrac{J(\beta_{0})}{J(\beta)} \cfrac{\pi(\bm{\theta}; \beta) {\cal L}(\bm{\theta}; \beta)}{\int \pi(\bm{\theta}; \beta) {\cal L}(\bm{\theta}; \beta) d \bm{\theta}}.
\end{equation*}
So we expect that for the simple case of scaling the uniform box
prior with \(\beta\), that we need to re-scale the likelihood by
\(\beta^{2n}\). The second Jacobian factor enters the likelihood because
we have normalised \(\pi(\bm{\theta})\), but not \(\pi(\bm{\theta}; \beta)\). This is hinted at in
the notation, (no tilde), and when accounted for, gives  the correct
posterior and evidence as seen in the experiments. 


\subsubsection{Argument scaling repartitioning}\label{sec:orgfe92f25}

Power posterior repartitioning in the case of a Gaussian
distribution (also a Cauchy distribution), can be thought of as
scaling the distribution using \(\beta\).

We shall discuss multiple forms, of such repartitioning schemes,
and extend the idea to discontinuous distributions, such as a
re-sizeable uniform prior.  

So far, the main practical considerations for choosing such a
distribution is that for some attainable value of \(\beta\), the
distribution resolves to a reference. For that reason, for example
the Cauchy distribution is also more convenient to treat using a
power, because the manifest reduction to a uniform distribution is
obvious when raising the entire distribution to the power of
\(\beta\), and not when it pre-multiplies the breadth parameter
\(\gamma\).

A drawback of using power repartitioning is that it's not always
possible to find an analytical result for \(Z(\beta)\{\pi\}\), indeed
in the case of trigonometric distributions, such as \(Z(\beta)\{\pi\}\),
was proven to only be analytical if \(\beta\), is an integer, and
proven not to be analytical otherwise \citep{Liouville1837}. Mixture
repartitioning on the other hand can easily cope with such
functions, as it only requires for them to be normalised once
(e.g.~for \(\beta=0\) and \(\beta=1\)), and re-use the normalisation
factor.


\subsubsection{Stochastic superpositional repartitioning}

Consider repartitioning schemes
${\cal M}_{i} = (\tilde{\pi}_{i}, \tilde{\cal L}_{i})$ for
$i=1, \ldots, m$. By adding \(m-1\) extra parameters $\bm{\beta}$,
construct the parameterised prior:
\begin{equation*}
  \tilde{\pi}(\bm{\theta}; \beta)  \triangleq \begin{cases}
	\tilde{\pi}_{1}(\bm{\theta}) & \text{with probability } \beta_{1},\\
	& \vdots,\\
	\tilde{\pi}_{n}(\bm{\theta}) & \text{with probability } (1- \sum_{i}^{m}\beta_{i}),
	\end{cases}
\end{equation*}
and likelihood:
\begin{equation*}
  \tilde{\cal L}(\bm{\theta}; \bm{\beta})  \triangleq
  \begin{cases}
	\tilde{\cal L}_{1}(\bm{\theta}) &  \text{with probability } \beta_{1},\\
		    &\vdots,\\
	\tilde{\cal L}_{m}(\bm{\theta}) & \text{with probability} (1- \sum_{i}^{m}\beta_{i}).
\end{cases}
\end{equation*}
This is a general superpositional mixture of $\{{\cal M}_{i}\}$ if and only if
\begin{equation}
  \label{eq:sspr}
  \tilde{\pi}(\bm{\theta}; \bm{\beta}) = \tilde{\pi}(\bm{\theta})_{i} \Leftrightarrow \tilde{\cal L}(\bm{\theta}; \bm{\beta}) = \tilde{\cal L}_{m}(\bm{\theta}; \bm{\beta}), 
\end{equation}
that is the branch of each function is chosen consistently.

The~\cref{spec-prop,norm-prop} are satisfied by
construction. The~\cref{vconv-prop} is satisfied using the same
feedback mechanism as PPR: the likelihood is determined by
\(\bm{\theta}\), and \(\bm{\beta}\) s that lead to higher likelihoods are
favoured. The corresponding limit being minimum is satisfied as
each Riemann sum in the integral has a higher probability of being
minimised as \(\bm{\beta}\rightarrow\bm{\beta}_{R}\). In other words, the
convergence property is satisfied probabilistically. Thus, this is
a valid posterior repartitioning scheme.

\emph{Stochastic superpositional mixture repartitioning}
(SSPR) ensures branch consistency by requiring
\begin{equation*}
\tilde{\pi}(\bm{\theta}; \bm{\beta}) = \tilde{\pi}_{F(\bm{\theta};
  \bm{\beta})}(\bm{\theta};\bm{\beta}),
\end{equation*}
where $F(\bm{\theta}; \bm{\beta})$ is a function. In our implementation
\begin{equation*}
  F(\bm{\theta};\bm{\beta})= \text{{\cal N}}_{m}\left(\text{pseudo-random}(\bm{\theta}); \bm{\beta}\right)
\end{equation*}
where \({\cal N}_{m}\) is the smallest index \(n \leq m\) for
which \[x > \sum_{i}\beta_{i}.\]

An illustration of priors in our implementation of the scheme is seen
in \cref{fig:mixture}.

The greatest advantage that mixture repartitioning nets is that it is
model-agnostic: one could, for example, use PPR in the mixture of
priors. A mixture of mixtures is also valid, however a flat mixture
would have less redundancy in its description.  One, should judge
which mixing method suits their needs, SSPR is generally faster, and
more robust, however there may be cases where PPR-like smooth priors
are necessary.


Domains of individual models are not a concern, if we require that if
$\theta_{e} \not\in D(\pi_{i})$ then ${\cal L}_{i}(\theta_{e})=0$ for
$i=1,\ldots,m$, contrary to what \cref{norm-prop} may suggest. Thus
the effective domain of SSPR is the set union of the domains of its
constituents.

\begin{figure}
 \input{./illustrations/mixture.tex}
\caption{An example of a mixture repartitioning. Notice that the mixture is not normalised to emphasise the coincidence of values with both the uniform distribution and a Gaussian.\label{fig:mixture}}
\end{figure}



\section{Practical measurements}
\subsection{Performance}
We shall adopt the weighted accounting approach, which common in
computer science, to measure performance using the number of
\({\cal L}\) evaluations as the unit. Additionally, the
Kullback-Leibler divergence \({\cal D}\)~\cite[see]{Kullback_1951} is
used to estimate both the speed and the agreement of posteriors. It
defines an ordering on distributions: it's less for similar
distributions, ${\cal D}=0$ if identical everywhere in their domain.

\begin{figure}
  \input{./illustrations/kullback-leibler.tex}
\caption{Kullback-Leibler divergence \({\cal D}\) for different offsets: Gaussian peaks displaced from \(\bm{\mu}\) by \(\text{Offset}\times \bm{\mu}\). Notice that the faster repartitioning methods produce a lower value of \({\cal D}\). The divergence \({\cal D}\) scales linearly with the offset.\label{fig:kl-d}}
\end{figure}

\begin{figure}
  \input{./illustrations/scaling-kld.tex}
\caption{Scaling of number of likelihood calls as a function of Kullback-Leibler divergence \({\cal D}\). The best fit line indicates that \({\cal D}\) is a reliable performance indicator for \texttt{PolyChord}.\label{fig:kl-scaling}}
\end{figure}


\begin{figure}
\input{./illustrations/histograms.tex}
\caption{An illustration of the evidence distributions of different types of repartitioning schemes. The Uniform reference obtained a distribution centered around \(\log {\cal Z} = -62 = - \log V(\Psi)\) (see \cref{eq:evidence}), where \((a,b)=(-6, 6)\cdot 10^{8}\) and \(G=\mathds{1}_{3}\). Note that both mixture modelling and PPR have found the same value, and the distributions are more sharply peaked. Also notice that if the repartitioning is done incorrectly, the evidence will also be estimated incorrectly. However, mixture repartitioning is able to correctly mitigate the offset of one of the models in its mixture: it computed the correct evidence despite one of the models in the mixture being the manifestly wrong repartitioning scheme.\label{fig:hist}}
\end{figure}

Unfortunately, while a full analysis of the posterior distributions
would be much more in the spirit of Bayesian analysis, the data-sets
being are huge, so one cannot practically include all of the
\emph{marginalised posterior plots} to prove the correctness of a
run. We shall provide one example, and drop the discussion: one should
assume that the posteriors coincide unless otherwise specified. In the
latter case, the misfit is quantified by Kullback Leibler divergence
from the true posterior distribution.


\begin{figure}
  \includegraphics[width=0.5\textwidth]{./illustrations/triangle-fit.pdf}
\caption{An example of a posterior distribution generated with power posterior repartitioning, based on data from Planck. The posteriors are near identical, and a slight misfit can be explained with arithmetic rounding errors, and run-to-run variance of the position of the live points (see top right figure).\label{fig:overlay-posteriors}}
\end{figure}


\subsection{Simulations}
\subsubsection{Numerical models}

We shall begin our analysis with help of a simplified model that is
general-enough to share features with the Cosmological scale problem,
but also practical to investigate in depth, with multiple variations.

Our original model is a Gaussian peak. By choosing the uniform prior
as a baseline, and setting the log-likelihood as:
\begin{equation*}
  \ln {\cal L}(\bm{\theta}) = - \dfrac{1}{2} \left\{{(\bm{\theta} - \bm{\mu})}^{T}G^{-1}(\bm{\theta}-\bm{\mu})  + \ln \det \left| 2\mathrm{\pi} \bm{\Sigma}\right| \right\},
\end{equation*}
where the covariance matrix \(\bm{\Sigma}\), specifies the extent of
the peak, and the vector \(\bm{\mu}\), its location. We thus expect
the posterior to be a truncated and re-scaled Gaussian. However its
typical set is still approximately at a distance of the square root of
the diagonal elements of the covariance matrix form the peak,
i.e.~\emph{one standard deviation}.

The covariance matrix is positive semi-definite and symmetric, hence
it can be diagonalised \citep{taboga2017lectures}. If the covariance
matrix is diagonal, the Gaussian distribution is called
uncorrelated. If all diagonal elements are equal, then the Gaussian is
spherical with characteristic diameter given by
\(2\bm{\sigma} = 2\sqrt{\bm{\Sigma}}\), where \(\bm{\Sigma} = \Sigma \mathds{1}\).


Under such circumstances it's a matter of integrating \cref{eq:def-z}
to obtain the evidence. Most generally
\begin{equation}\label{eq:evidence}
   {\cal Z} = \cfrac{{\left( \sqrt{ \det \left| 2\mathrm{\pi} \bm{\Sigma} \right|} \right)}^{n}}{\bm{b}-\bm{a}}, 
\end{equation}
where \(n\) is the number of parameters in the model.

To simulate imperfections we shall consider translational offsets
between the intuition prior and the likelihood\footnote{for PR the
  breadth of the distributions is irrelevant.}.


\section{Results and Discussion.}\label{sec:org50493c6}
The first test case is an uncorrelated spherical Gaussian posterior
in three dimensions \[\mathcal{P}(\bm{\theta}) = G(\bm{\theta}; \bm{\mu} =
  (1,2,3),\bm{\sigma} = \mathds{1}).\] The corresponding evidence
(\cref{eq:evidence}) is \(\mathcal{Z}\approx-62.3\). First we shall
assume that the mean and standard deviation of all the
repartitioning schemes is exactly the same as that of the
posterior.

All but one repartitioning scheme yielded the correct evidence. The
resize-able uniform prior model was constructed to systematically
overestimating the evidence (\cref{fig:hist}) which is due to
underestimating the normalisation factor for
\(\mathcal{L}\).


We shall now show that repartitioning is able to drastically reduce
the run-time compared to using a uniform prior. More specifically,
guessing a posterior distribution and using repartitioning, one may
reduce the initial compression stage to virtually none.

Having proven the correctness of the runs, let's turn to performance
and benchmarks. The central metric is the number of \({\cal L}\)
evaluations. \cref{fig:benchmark} shows that mixture
repartitioning, produces a significant speed-up compared to even
power-posterior repartitioning. Moreover, the slope of the curve of
the number of \({\cal L}\) evaluations is much steeper for the
slower repartitioning schemes, indicating that for large numbers of
live points, mixture repartitioning yields an even greater
speed-up.



\begin{figure}
  \input{illustrations/benchmark.tex}
\caption{comparison of likelihood calls necessary for obtaining the correct evidence for the case of a spherical uncorrelated Gaussian posterior. Note that almost all series scale linearly with the number of live points. \label{fig:benchmark}}
\end{figure}




The next trial involved a variable offset, where convergence to the
correct posterior and evidence is not guaranteed even with the correct
normalisation. For this case, we have taken the same Gaussian
truncated to a cube \(1000\times1000\times1000\). Two types of
sampling runs were considered: one where the posterior and prior
distributions coincided, and one with the mean of the posterior
shifted relative to the prior by an amount proportional to the mean
$\mu = (1,2,3)$.

The exemplary results are given in \cref{fig:convergence}.

The main notable feature is the inaccuracy of the posterior for
PPR. If the offset is small --- \(O(2\sigma)\), the posterior is
shifted. With a larger offset, e.g. \(O(4\sigma)\), two peaks can be
resolved, sadly, with less density near the correct Gaussian
peak. Both errors are compounded by incorrect evidence (see
\cref{fig:drift}) PPR: \(\ln {\cal Z}\approx -25.4 \pm 0.2\), vs
uniform reference \(\ln {\cal Z} = -22.7 \pm 0.4\).

In practice one has the following options:
\begin{enumerate}
\item accept the posterior as is~\label[Option]{opt:accept}
\item accept the posterior, but as a less credible result\label[Option]{opt:accept-with-err}
\item reject the PPR result entirely, and perform a run with only a
uniform prior\label[Option]{opt:uniform}
\item readjust the PPR mean and variance using the posterior, and
re-run~\label[Option]{opt:shift}
\item combine PPR with SSPR in mixture with a uniform prior
\end{enumerate}
\vref{opt:accept} is adequate for low accuracy standards provided the
error is properly estimated using a tool such as \texttt{nestcheck}.
From \cref{fig:benchmark}, we see that the performance uplift allows
for \cref{opt:shift} to be more efficient than~\ref{opt:uniform},
albeit marginally so. \Cref{opt:accept-with-err} is a last resort.

This is where our technique is most useful: one obtains, as we've
shown in~\cref{fig:convergence}, a more accurate
\({\cal P}(\bm{\theta})\), by using PPR from within SSPR. The
performance impact has considerable run-to-run variance, however never
exceeds \(20\%\) extra \({\cal L}\) evaluations, which is an order of
magnitude less than~\vref{opt:uniform,opt:shift} would afford.

\begin{figure}
\includegraphics[width=0.5\textwidth]{./illustrations/convergence.pdf}
\caption{An illustration of how offsets affect the convergence of nested sampling under different kinds of repartitioning. The offset models should produce an offset posterior, whilst sharing the prior with the model runs. The mixture is of the present uniform model and PPR.\label{fig:convergence}}
\end{figure}

\begin{figure}
  \input{./illustrations/evidence-drift.tex}
\caption{Comparison of evidence estimates produced by different repartitioning schemes. The true value is constant, and should not depend on the offset. Mixture repartitioning is able to correctly cope with the offset, producing the correct evidence and posterior, while PPR is gradually drifting.\label{fig:drift}}
\end{figure}


Lastly, \emph{posterior mass} is a measure of converge
speed~\cite{higson2018nestcheck}, which is used in diagnosing
pathological issues specific to nested sampling. Typical examples for
uniform and re-partitioned priors are given in
plot~\Cref{fig:higson}. Firstly, note faster compression with
repartitioning, consistent with run-time reduction. Secondly, notice
that the re-partitioned series has a longer extinction phase, as a
result of introducing extra nuisance parameters. Finally, notice that
the confidence intervals for the parameters' distributions are near
identical, signifying that the obtained posteriors are more precise,
agreement with the constructed posterior:
\(G(\bm{\theta} = (0, 4, 8), \bm{\sigma} = 1)\), --- accurate.

\begin{figure}
\includegraphics[width=.5\textwidth]{./illustrations/higson.png}
\caption{An evolutionary insight into the behaviour of nested sampling. The \color{red} red \color{black} series corresponds to mixture repartitioning, while the \color{blue} blue \color{black} series --- to a reference uniform. All plots are given in \(\ln X\), where \(X(\mathcal{L}) \in [0,1]\) is the fraction of the prior with likelihood greater than \(\mathcal{L}\). The top plot is the relative posterior mass, which is the total weight assigned to samples from the region. In each row, we're presented with the posterior in the given parameter. The gradients represent degree of confidence.\label{fig:higson}}
\end{figure}




\subsection{Cosmological Simulations.}\label{sec:orgb81c159}
After an initial run of \texttt{Cobaya}, we have obtained the marginalised
posteriors of all the key parameters of the \(\Lambda\)CDM model,
as well as the nuisance parameters.

Ignoring any off-diagonal elements of their co-variance, we have
constructed a mixture re-partitioned prior, containing a Gaussian
with our best estimates, a uniform containing the original
boundaries. A second run was thus performed.

Benchmarking on a cluster using time is impractical. Instead we
measured the number of likelihood calls for each invocation of
\texttt{PolyChord.run\_polychord()}.

The result is a \textbf{substantial} reduction in run-time.




\section{Conclusions}\label{sec:orgdf2cbd9}

\subsection{Results}\label{sec:orgc48c55d}
Our project's purpose had been to investigate the performance
increase attainable by algorithmic optimisations of the inputs to
nested samplers.

We have identified a general prescription, named superpositional
mixture repartitioning that netted the same if not greater
performance improvement as power posterior repartitioning (PPR).

We have also established that the aforementioned prescription had a
number of advantages:
\begin{enumerate}
\item it allows multiple priors to be mixed, while PPR only allows
one.
\item it permits a broader class of functions, than are practical for
PPR, e.g.~ones where \(Z_{\pi}(\beta)\) cannot be represented in
closed form.
\item it copes with functions having different domains. PPR cannot.
\item it is abstract, i.e.~the prior iCDF is a superposition iCDFs of
the constituents priors. For PPR the iCDF needs to be computed.
\item it supports an unbiased reference (uniform) prior exactly. PPR
tends to an unbiased reference as \(\beta\rightarrow\beta_{0}\).
\item it is able to mitigate improper re-scaling of the likelihood. If
one of the priors is improperly normalised, the offset from the
true evidence is reduced as \(n_{live}\rightarrow\infty\). PPR
does not.
\item it is resilient to human error.
\item it is easier to implement and requires little to no mathematical
input from the user, beyond ensuring the three properties.
\end{enumerate}


\subsection{Further research}\label{sec:org8314ddf}
The proposed algorithm of superpositional mixtures, maps neatly
onto concepts of quantum computation. Indeed one can model two
different priors as states of qubits. The benefits are potentially
immeasurable, as the greatest weakness of the classical algorithm
is that we're effectively sacrificing resolution in the posterior
by sampling only from one prior exclusively. Quantum superposition
allows us to do both at the same time, while quantum entanglement
ensures that the deterministic requirements set by
\citeauthor{Skilling2006} are met.

The necessary consequence of repartitioning is that the posterior
samples have a greater prior space to explore, and thus, while the
option to skip areas of negligible evidence allows us to compress
the priors significantly more quickly, the tail-end of the nested
sampling is also affected. A potential solution to this is to treat
the introduced parameters separately at this stage.

One such treatment may be to use the posterior distribution at the
point of discovery to freeze the choice parameters. Their
covariance may represent a volume in a \(\bm{\theta}\) space that
corresponds to the remaining evidence, and as such, crudely
approximate the remainder while sampling from a lower dimensional
space compounding to the physical (i.e.~not repartitioning-related
parameters of the theory).

Additionally, we have assumed that nested sampling converges the
fastest if the prior is also the posterior. However, a simple
example of a spherical Gaussian in three dimensions shows the same
characteristic tail at the end of the execution. It may be
necessary to look into priors that are tailor made to accelerate
that convergence. Naturally, they would also depend on the sampling
technique used: the prior that accelerates rejection sampling would
be different to one that accelerates slice sampling.

An additional avenue to explore would be to ask whether the same
sampling technique is appropriate for all stages. Slice sampling is
ideal for applications with prior space with large
dimensions. However, Metropolis-Hastings may be more suitable for
the extinction phase, and may thus eliminate the tail altogether.

Among the less-important investigations that could be carried out,
one might investigate an extension of the re-sizeable uniform
prior. Indeed one of the main reasons for its impracticality is the
sharp reduction to zero, that cannot be compensated for in the
likelihood. However, one should expect that this is possible to
compensate for by using a distribution that's constructed to be
non-zero in the entire domain of the original uniform prior:
e.g.~by having edges that tend to zero at the boundaries. A
suggestion might be a smooth top-hat, or a combination of error
functions.

\subsection{Applications}\label{sec:orgc67317e}
Nested sampling is a universal algorithm that can be applied to any
problem involving either direct parameter estimation (e.g.~analysis
of Planck data), or indirectly such as neural-network based machine
learning.

To clarify the latter point, the process of training a neural
network involves a process of estimating the connection strengths
between layers of states. Normally training is done via a negative
feedback process, where the connections that correspond to the
right answer are reinforced, whilst connections leading to
incorrect ones are reduced in strength. In the formalism of
Bayesian inference, the connection strengths are the parameters,
the prior is uniform and the sampling is done via
Metropolis-Hastings anticipating a logistical distribution. As we
are able to accelerate this process in Bayesian formalism, we
should also be able to modify the standard algorithms to make use
of repartitioning.

Moreover, the subject matter of this paper --- superpositional
mixture repartitioning with stochastic sampling may be used to
create classes of neural networks: as of now information obtained
from training one network cannot be re-used when training another,
unless the two networks have identical architecture and solve
identical problems. One cannot use the weights of a network
analysing faces as the initial values for analysing objects,
without that resulting in a strong bias.

It may be possible to use the values of node connection strengths
from networks that are used for similar problems, by virtue of the
stability offered by repartitioning. We can regard that as one of
the priors in the mixture, and hence improving performance where
the guess is indeed accurate, without compromising the result if it
is not. Of course such neural networks will need to have a similar
number of physical connections, and hence have similar if not
identical architecture.

\bibliography{bibliography}
\bibliographystyle{mnras}
\end{document}