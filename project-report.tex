% Created 2020-04-08 Wed 09:06
\documentclass[usenatbib]{mnras}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{pgfplots}
\usepgfplotslibrary{groupplots,dateplot}
\usetikzlibrary{patterns,shapes.arrows}
\pgfplotsset{compat=newest}
\usepackage{dsfont}
\usepackage{xcolor}
\author{Aleksandr Petrosyan, William J. Handley}
\date{\today}
\title{Cosmological parameter estimation using Bayesian accelerated machine learning}
\hypersetup{
 pdfauthor={Aleksandr Petrosyan, William J. Handley},
 pdftitle={Cosmological parameter estimation using Bayesian accelerated machine learning},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.3 (Org mode 9.1.9)}, 
 pdflang={English}}
\begin{document}

\maketitle
\begin{abstract}
TODO
\end{abstract}

\section{Introduction}
\label{sec:orgbc2b3ed}

The standard model of the universe and its evolution in modern
cosmology is the accepted \(\Lambda\)CDM model \citep{Condon2018},
so named after the main components of the universe according to
it. It has six major parameters: physical baryon density parameter;
physical dark matter density parameter; the age of the universe;
scalar spectral index; curvature fluctuation amplitude; and
re-ionization optical depth. It is the task of the present study to
evaluate how well does \(\Lambda\)CDM agree with observations from
the Planck mission \citep{planck}, as well as provide estimates for
the main parameters. It is also our goal to find methods for
accelerating said process. In this section we shall describe the
main approaches one may take to answering these questions, as well
as refinements made to them.

The problem of reconciling theoretical predictions with experimental
observations is the fundamental underpinning of any modern science,
be it Physics, or Biology. The methods and the general statistical
frameworks used for such reconciliation have changed almost as much
as the sciences themselves. Indeed, while a simple qualitative ``all
objects in vacuo accelerate at a rate independent of their mass'',
may have been sufficient for Galileo, modern problems necessitate
modern solutions. Although the slightly more informative ``the
acceleration of free fall was measured \(g = 9.81 \pm 0.01\) is an
improvement, it leaves much to be desired. For example, we
implicitly assume that the distribution of \(g\) is symmetrical
around \(\left \langle g \right \rangle = 9.81\). This may not be
the case if we're using free fall to measure \(g\), as almost every
source of error: slow reaction times, air drag, inconsistent
release; would lead to an underestimate of \(g\). 

To establish a law of physics, one needs finer tools and more
precise language to make all of the implicit assumptions explicit,
so one can judge whether or not the conclusions are justified given
the observations.

Enter Bayesian inference. It is based on the mathematical result
obtained by \cite{1763}, and was refined over the following two
centuries. This approach has proven quite fruitful in computational
problems \citep{Wolpert2004}, particularly in Machine learning, and is
slowly making its way into physics, a field traditionally dominated
by frequentist statistics. Without going into too much detail for
the reasons behind Bayesian probability's success, we should point
out that it is able to reproduce the results of traditional
inference techniques, while putting them into a more general
framework, making the delineations of objectivity and subjectivity
explicit.


By performing a full Bayesian analysis one can find quantitative
answers to questions that otherwise could only be answered
qualitatively.  For example: how consistent a model is with our
observations is quantified in \emph{evidence}. How liable are each
individual values of the model parameters is quantified in the
\emph{posterior}. Moreover, there exists a mathematical object
representing  the vast body of experience we  have accrued from
other observations -- the \emph{prior}. 

Some aspects of a model may be for various reasons more interesting
than others. One may like to build a model that describes a
free-falling object in an evacuated tube, but be mainly concerned
with the gravitational aspects of the process. Bayesian worldview
allows one to \emph{marginalise} the model parameters that one does not
care about: so-called \emph{nuisance} parameters. Mathematically this is
represented by conditional probabilities, and as the name suggests,
the cornerstone of the mathematics of such objects is Bayes'
theorem. 

The above advantages make Bayesian inference a particularly
convenient methodology for estimating cosmological
parameters. Although \$\(\Lambda\)\$CDM has few parameters, the accurate
model describing the physical processes that Planck \citep{planck}
observed needs at least 27 parameters, not counting the calibration
parameters for the experimental apparatus, and other, more
complicated models, which lead to models with 42 parameters.

A full Bayesian inference for such a large parameter space is a
computationally expensive endeavour, particularly very little prior
knowledge. Hence a large number of algorithms were developed to
accelerate the computation: Metropolis-Hastings \citep{Metropolis}
used in conjunction with the Gibbs sampler
\citep{Metropolis-hastings-gibbs}, Hybrid (Hamiltonian) Monte-Carlo
\citep{1701.02434,Duane_1987} and more recently Nested Sampling
\citep{skilling2006}, which will be our focus.

Nested Sampling, as described by \citeauthor{skilling2006} is rather
abstract, and multiple algorithmic-ally distinct implementations of
the idea exist including:
\begin{itemize}
\item MultiNest \citep{Feroz2009MultiNestAE},
\item nestle \citep{nestle}
\item dyNesty \citep{Speagle_2020},
\item PolyChord \citep{polychord}.
\end{itemize}
Although the optimisation proposed in this paper is quite general,
in that it accelerates all implementations of nested sampling, we
shall primarily focus on PolyChord. Among the many reasons, is that
the slice sampling algorithm used in PolyChord, is the one that is
most liable to be adversely affected by the stochastic nature of the
re-partitioning proposed here.

\cite{chen-ferroz-hobson} noted that the nested sampling algorithm,
unlike other Markov-chain Monte-Carlo algorithms, is sensitive to
how the two conditional probabilities \emph{likelihood} and \emph{prior} are
defined, with respect to the posterior distribution. Hence the name
of the technique -- \emph{automatic power posterior re-partitioning}
(PPR). While \citeauthor{chen-ferroz-hobson} used PPR to improve the
stability of convergence for prior distributions that may have been
at variance with the true posterior, we shall show that it can be
used to accelerate the execution of the nested sampling
algorithm. 

The purpose of this paper is to present a mathematical framework
that encapsulates the idea and explores the extents of its
utility. In particular, we shall describe how one may achieve better
convergence stability and better performance, using mixture
re-partitioning, a technique that we've devised specifically for
improving the performance of nested samplers.

In the following sections we shall (mostly) focus on the theoretical
background, and an extension (more precisely generalisation) of the
idea of posterior re-partitioning, its advantages, applicability and
how it can be used to improve run-time characteristics of samplers
such as PolyChord. Lastly we shall present the results of using such
methods when applied to a modern Cosmological parameter estimator
such as Cobaya \citep{cobaya}.

\section{Background theory}
\label{sec:orgb7f80d5}

\subsection{Brief primer on Bayesian inference.}
\label{sec:org82010dc}

This topic has been discussed at length in literature
\citep{jeffreys2010scientific}, so we shall restrict ourselves to the
bare necessary definitions and concepts.

Let a scientific theory that we're interested in testing, provide a
model of a process model \({\cal M}\), that predicts what data \(\lbrace {\cal M}(\vec{\theta})\rbrace\) one observes, based on the
parameters \(\vec{\theta} = \lbrace \theta_1, \theta_2, \ldots,
   \theta_n \rbrace\) (we shall drop the vector, the nature of
\(\theta\), should be obvious from the context) and the (actual)
observed data -- \(D\).

One can define the following conditional probabilities given
in \autoref{table-defs}. Using these definition \citeauthor{1763} 's theorem
becomes
\begin{equation}
 {\cal L} \pi (\theta) = {\cal Z} {\cal P} (\theta).
\label{eq:bayes} 
\end{equation}
Notice that the \emph{evidence} \({\cal Z}\) is implicitly defined as

\begin{equation}\label{eq:def-z}
 {\cal Z} = \int_{\Psi} {\cal L}(\theta) \pi(\theta) d\theta, 
\end{equation}
where \(\Psi\) is the so-called prior space -- the domain of the
prior function. Although some authors
(e.g. \citeauthor{jeffreys2010scientific}) believe \({\cal Z}\) to be
no more than a normalisation factor, as one can see from its
definition in \autoref{table-defs}, it quantifies the consistency of
the hypothesised model with the observed data. Therefore, it's a
suitable measure of the ``goodness'' of a model: the higher the
value of \({\cal Z}\), the more likely is that the model accurately
describes the physical process in question.

\begin{table}[htbp]
\caption{Definitions of main quantities in Bayesian analysis. \label{table-defs}}
\centering
\begin{tabular}{lll}
\textbf{\textbf{Term}} & \textbf{\textbf{Symbol}} & \textbf{\textbf{Definition}}\\
\hline
Prior & \(\pi(\theta)\) & \(P ( \theta  \vert D)\)\\
Likelihood & \({\cal L}(\theta)\) & \(P ( D \vert \theta \cup M)\)\\
Posterior & \({\cal P}(\theta)\) & \(P ( \theta \vert D \cup M)\)\\
Evidence & \({\cal Z}\) & \(P ( D \vert M)\)\\
\end{tabular}
\end{table}

The two independent quantities, \({\cal L}\) and \(\pi\) defined in \autoref{table-defs} are the
inputs to the Bayesian Sampler. How they are specified depends on the algorithm, however, most nested samplers (e.g. PolyChord) find a convenient representation of log-likelihood: 
\begin{equation}
  L = \ln \cal L
\end{equation}
and the \emph{prior quantile} function, also referred to as the \emph{inverse
cumulative distribution function} (iCDF) for the prior distribution.
\begin{equation}
 \pi : HC \rightarrow \Psi,
\end{equation}
that is, a mapping from a unit hypercube where the distribution of
parameter images is uniform, onto the (non-uniform) prior space
that is the domain of integration of \({\cal Z}\). Choosing to work
with logarithms is a convenience: most likelihoods are Gaussian
(central limit theorem \cite{central-limit-theorem}), hence taking
the logarithm early allows us to avoid costly numerical
multiplications and divisions in lieu of additions and
subtractions. The reason for working with the quantile as opposed
to the probability density function (PDF) or the cumulative
distribution function (CDF) shall become clear later.

An important point is that within specification of likelihood and
prior there is some redundancy. One can easily see that by
considering another pair of input functions such that 
\begin{equation}
  \tilde{\cal L} \tilde{\pi} = \cal L \pi. 
\end{equation}
In the new representation, the value of \({\cal Z}\) is invariant
and by \autoref{eq:bayes}, so is \({\cal P}(\theta)\).  

Thus, most MC-MC Bayesian samplers are indifferent to precise
definitions of \(\cal L\) and \(\pi\), as long as their product --
the posterior, corresponds to an element of physical reality. One
notable exception and therefore of interest to us is nested
sampling.

\subsection{Nested Sampling.}
\label{sec:org807a4d0}

This algorithm is discussed in depth, so we shall restrict
ourselves to descriptions that are necessary for understanding how
and why posterior re-partitioning works.

We shall begin by noting that, Bayes' theorem reduces the problem
of parameter estimation to integration, so hypothetically the naive
approach would be to rasterise the prior space \(\Psi\) and
numerically evaluate the integral \({\cal P}\). However, in
hypotheses with many parameters, said problem is intractable by
uniform rasterisation (i.e. using a grid and enumerating all the
points), thus Monte-Carlo techniques are favoured.

For simplicity and without loss of generality assume that the prior
space is a unit hypercube. Draw, at random, \(n_\text{live}\)
points from the hypercube. One expects that the probability that
two points have the same likelihood is vanishing, so each of them
lies on a distinct iso-likelihood hyper-surface. Each will contain
on-average \(frac{1}{n_\text{live}}\)-th of the total volume of the
hypercube. More specifically, each shell's volume shall have some
deviation \(\Delta\), from said value, with an associated probability
distribution \(P(\Delta)\).

Subsequently, we may wish to pick another point at random, but
requiring that the likelihood of that point is higher than the
lowest likelihood of the initial choice, we can ``move'' the
outermost point inside. In \citeauthor{skilling2006} 's notation, the
aforementioned point with the lowest likelihood becomes ``dead''
and the new point becomes ``alive''. Moreover, our argument for
hyper-surfaces encasing a roughly equal volume still holds, so we
can expect that upon next iteration the prior volume encased in the
outermost hyper-surface is reduced by
\(\frac{1}{n_\text{live}}\)-th of the volume encased in the
previous outer-most shell.

More formally, this defines a sequence of approximations of the
prior volume encased in the outer hyper-surface:

\begin{equation}
  \begin{array}{rcl}
  X_{0} &=  &1 \\
  X_{1} &= &X_{0} \left(1- \frac{1}{n_\text{live}}\right)\\
  & \vdots & \\
  X_{i} &= &X_{i-1}\left(1- \frac{1}{n_\text{live}}\right)\\
  & \vdots &
\end{array}
\label{eq:recurrence-relation}
\end{equation}

which allows us to iteratively pick ``live'' points closer to
regions where the likelihood is high. A suitable termination
criterion therefrom is to stop when the prior volume encased in the
shell is lower than a predetermined fraction of the total hypercube
volume -- \(1\).

As was mentioned previously, the recurrence relation
\eqref{eq:recurrence-relation} is not exact. However, \(P(\Delta)\)
is a known distribution, dependent on the dimensionality of the
hypercube and the likelihood. Thus, one can for each value of
\(\epsilon>0\), deduce a value \(\delta(\epsilon) >0\), such that
\(P(\Delta > \delta) < \epsilon\). Hence, by choosing \(\epsilon\)
based on \(n_\text{live}\), one gets a characteristic value for the
error \(\delta\). Carrying these through the iterations allows us to
estimate the prior volume and hence the evidence up to some
precision.

This algorithm can be generalised to other priors and prior spaces
by virtue of coordinate transformations, which are represented by
iCDFs.

The algorithm's run-time is linearly dependent on
\(n_{live}\). However, in context of cosmological parameter
estimation, the more important number is the quantity of likelihood
evaluations, as the function \({\cal L}\) is the dominant cost; for
example, In Cobaya using the CLASS provided likelihood function one
evaluation can take upwards of a second. 

Naturally, under such circumstances, algorithms that minimise the
number of likelihood evaluations will offer the most
improvement. For example, rejection sampling: drawing a point at
random, and rejecting it based on the criteria mentioned, is less
efficient than slice sampling \citep{Neal_2003}.

So when does one terminate the fastest? One suspects that knowing
the posterior distribution, all the algorithm needs to do is check
the obtained values. So an ideal sampler would converge optimally
when the prior and the posterior coincide: 
\[\begin{array}{rl} {\cal P}(\theta) = \pi(\theta) & \forall \theta \end{array} \]
So if one has gathered data from free fall experiments, on earth
one would expect the posterior to be a normal distribution peaked
at \(g=9.81\), with standard deviation \(\sigma_{g} = 0.01\), which we
shall compactly refer to as \[{\cal P}(\theta) = G(9.81, 0.01)\].

However that is only partially true. According to Bayesian
statistics the prior knowledge: the constraints set on the model
parameters, are pare thereof, so by picking a different,
\emph{unrepresentative prior} the likelihoods will not correspond to the
same model. 

In our particular example, if the free-fall data was gathered on
the surface of the moon, and we use the earth prior for \(g\),
nested sampling would converge on a Gaussian peaked at \(g=9.81\),
with perhaps a broader standard deviation. Evidence would be the
main telltale sign that the algorithm has not produced a
statistically significant or meaningful result, but that too can be
masked by other parameters. Indeed, if one has set a gnerous
uniform prior on the air-drag coefficient, and admitted the
detector spacing as well as trigger timing to be nuisance
parameters, one will not see anomalies\footnote{this peculiarity of
statistical methods lead John Von Neumann to remark that four
parameters in a model were sufficient to produce a statistically
significant fit to an elephant. And that five would be consistent
with it moving its trunk.}.

This is the problem of \emph{unrepresentative priors} and
\citeauthor*{chen-ferroz-hobson} have developed power-posterior
repartitioning specifically as a mitigation of the aforementioned
issue.


\subsection{Power posterior re-partitioning}
\label{sec:org18d388f}

The basic idea is as follows. If we had two priors, one much
narrower than the other, we expect that the convergence in the
narrower one will be faster. After all, we're ignoring the bulk of
prior space where nothing happens. We also expect that the
likelihood of the values inside the smaller effective volume will
be enhanced. To see why this happens, consider that to have a
larger value of the prior, (or rather a more condensed one), in
order to keep the product \(\cal L \pi\) constant, one must have
reduced the value of \(\cal L\), conversely, if the value is not
reduced, it is larger than it would have been.


As such, \citeauthor{chen-ferroz-hobson} have proposed introducing an
extra parameter \(\beta\) that re-scales the prior:
\begin{equation}
  \tilde{\pi}(\theta) = \frac{\pi(\theta)^{\beta}}{Z(\beta)\{\pi\}},
\end{equation}
where \(Z(\beta)\{\pi\}\) is a normalisation factor, i.e. 
\begin{equation}
  Z(\beta)\{\pi\} = \int_{\theta \in \Psi} \pi(\theta)^{\beta}d\theta.
\end{equation}
According to their prescription, one also needs to modify the likelihood
\begin{equation}
  \tilde{\cal L}(\theta) = {\cal L}(\theta) Z(\beta)\{\pi\} \cdot \pi^{1-\beta}(\theta).
\end{equation}
One needs to take great care when choosing the domain of
\(\beta\). As \(\beta\) is an ordinary nuisance parameter it needs a
prior, and one has very few restrictions. Normally we expect a
uniform prior \(\beta \in [0, 1]\). If one is confident that the
original prior was representative one could introduce a non-linear
map that favours the value \(\beta=1\). If the original prior may be
too broad one could experiment and extend \(\beta>1\). One can also
extend it to \(\beta<0\), although practical cases where that may be
a sensible option are few.

Notice, however, that \citeauthor{chen-ferroz-hobson}, our argument
implicitly assumed that the prior we started with was
peaked. Indeed the sole difference between different values of
\(\beta\), for a uniform prior would be the normalisation, which by
construction we constrain to the original value.


Importantly the domains of all functions need to be the same. Let
\(D(f)\) denote the domain of the function \(f\), i.e. where the
function is both defined and \textbf{non-zero}. Hence
\begin{equation}
  D(\pi) = D({\cal L}) = \Psi = D({\cal P}),
\end{equation} 
meaning the posterior is within the domain of the prior and
likelihood, which will be important later.\label{domain-discussion}

This, for the cases that \citeauthor{chen-ferroz-hobson} have
originally considered, resolves the issue of non-representative
priors, because the evidence associated with the biased prior
reduces as \(\beta\rightarrow0\).

In the original form, this method is to prevent errors, by
sacrificing run-time performance. In practice, the overhead
associated with PPR is negligible, and even in the case of
univariate examples, where the relative impact is maximal, it's not
significant \cite[see numerical
examples]{chen-ferroz-hobson}. Even so, it can only do that under the assumption that
one's prior knowledge is not represented by a uniform distribution. 

Our first discovery pertains to what happens under an inverted
premise: can one gain performance by starting with a uniform prior,
and using PPR backwards to accelerate convergence?

Let us have a model, of which prior experience is ignorant. Under
such circumstances the prior is uniform (and unbounded, which we
shall ignore for now). Central limit theorem suggests, that the
model parameters' posterior is within a Gaussian:
\begin{equation}
 \pi (\theta) \propto \exp \left[-\left(\frac{\theta - \mu}{2\sigma}\right)^{2} \right],
\end{equation}
albeit not the values of \(\mu\) and \(\sigma\). We shall refer to
this function as the \emph{intuition}, or the \emph{biased prior}. Ordinarily
this intuition is subjective, and therefore can affect the
objectivity of our outcomes. However, with a proper methodology one
can have the best of both worlds: the performance associated with
knowing the result in advance, with the flexibility to entertain
other possible results.

One can achieve these results using PPR. Consider what happens on
the microscopic level, A point with fully random coordinates is
drawn from an \(n+1\) dimensional space where the effective
parameter vector contains \(\beta\) as the last parameter, treated
as any other component of \(\theta\). This randomises the prior, live
points that are closer to the true posterior distribution are
favoured, so are values of \(\beta\) which lead to points with
higher likelihood.  This feedback ensures that if the true
posterior is within the region of radius \(\sigma / \beta\) of the
chosen value of \(\mu\), then the new points are chosen
preferentially from that region. The re-normalisation of the
likelihood, ensures that the posterior distribution is not biased
towards the value of \(\mu\), but rather the true posterior; one
that we would have found had we used a uniform prior. If our
hypothesis was wrong, then the values of \(\beta \rightarrow 0\)
would be favoured. The effective prior would then tend to a uniform
distribution.

\begin{figure}
 \input{./illustrations/ppr.tex}
\caption{\label{org6cc774f}
A demonstration of the function \(\tilde{\pi}(\theta; \beta)\) for different values of \(\beta\). Note that we've started under the assumption that the distribution is a truncated Gaussian, i.e. that it is zero outside the range \((-1, 1)\). This manifests as sharp changes in curvature at the boundaries. Note that \(\forall \beta\), \(\int_{-1}^{1}\tilde{\pi}(\theta; \beta) = 1\).}
\end{figure}

Having demonstrated correctness, let's focus on performance. The
majority of the run-time of nested sampling with a uniform prior is
spent ``compressing'' the live points onto the posterior
distribution. With \(\beta>0\), the probability that points will be
chosen from high-likelihood regions is enhanced, so on average the
execution time should decrease. 


\subsection{Argument scaling}
\label{sec:orgc345a9e}

Power posterior re-partitioning in the case of a Gaussian
distribution (also a Cauchy distribution), can be thought of as
scaling the distribution using \(\beta\).

We shall discuss multiple forms, of such re-partitioning schemes,
and extend the idea to discontinuous distributions, such as a
re-sizeable uniform prior.  

So far, the main practical considerations for choosing such a
distribution is that for some attainable value of \(\beta\), the
distribution resolves to a reference. For that reason, for example
the Cauchy distribution is also more convenient to treat using a
power, because the manifest reduction to a uniform distribution is
obvious when raising the entire distribution to the power of
\(\beta\), and not when it pre-multiplies the breadth parameter
\(\gamma\).

A drawback of using power re-partitioning is that it's not always
possible to find an analytical result for \(Z(\beta)\{\pi\}\), indeed
in the case of trigonometric distributions, such \(Z(\beta)\{\pi\}\),
was proven to only be analytical if \(\beta\), is an integer, and
proven not to be analytical otherwise \citep{Liouville1837}. Mixture
re-partitioning on the other hand can easily cope with such
functions, as it only requires for them to be normalised once
(e.g. for \(\beta=0\) and \(\beta=1\)), and re-use the normalisation
factor.

\subsection{General automatic posterior re-partitioning.}
\label{sec:org7e7c5c6}

Let's recap the key components of posterior re-partitioning. We have
   a baseline prior, with its likelihood \((\pi(\theta), \cal L
      (\theta))\), and a parameterised pair of biased prior and
   likelihood \((\pi'(\theta; \beta), \cal L' (\theta;
      \beta))\). These need to satisfy the following requirements.

\begin{enumerate}
\item For some \(\beta_{0}\), \(\pi'(\theta; \beta) \equiv \pi(\theta)\)
similarly \({\cal L'(\theta, \beta) \equiv {\cal L}}\). This is
the \textbf{\textbf{specialisation property}}.\label{spec-prop}
\item The product of the parameterised pair is constant for all values
of \(\beta\) and by specialisation property : \(\pi'(\theta; \beta)
      \cal L' (\theta; \beta) = \pi(\theta), \cal L (\theta)\). This is
the \textbf{\textbf{normalisation property}}.\label{norm-prop}
\item We need there to be a guiding dynamical principle that favours
the representative prior, i.e. one that's closest to the
posterior distribution, which we call the \textbf{\textbf{convergence
property}}.\label{conv-prop}
\end{enumerate}

PPR satisfies all three properties as follows: \ref{spec-prop} is
fulfilled with defining \(\pi'(\theta; 0) =
   \pi(\theta)\). \ref{norm-prop} is fulfilled by construction and
\ref{conv-prop},  by noting that \(\lim_{\beta
   \rightarrow 0} \pi'(\theta; \beta) = \pi(\theta)\).

Whether, the extra complexity is offset by the speedup offered by
the correct bias, depends on both how accurate our bias is, and on
the dimensions of the problem. In most cases the complexity of the
likelihood calculation is negligible, as well as the extra
dimension.

Any functions that satisfy the above requirements should produce
the same result, and our goal is to identify which shall produce
better run-times.

\subsubsection{Additive mixtures.}
\label{sec:orgac83d58}
Consider a weighted sum of a uniform distribution with
a Gaussian, e.g. in one dimension
\begin{equation}\label{eq:additive-mix}
  \pi(\theta) = \dfrac{ \left\lbrace \frac{1- \beta} {b - a} + \beta \exp \left[ -\left(\frac{\theta - \mu}{\sigma} \right)^{2}\right]\right\rbrace \cdot TH(\theta; a, b)}{Z}.
\end{equation}
where \(TH(\theta;a,b)\) is the top-hat function. Integrate to
obtain the normalisation factor \(Z(\beta)\{\pi\}\), used to
re-scale \({\cal L}\). Recall, however, that we use the inverse of
the prior cumulative distribution, and while the inverses of both
priors are manifest, we cannot easily compute the inverse of the
sum. In general one can't even prove that for two arbitrary
distributions the inverse of the sum exists.

\begin{figure}
  \input{illustrations/additive_mixtures.tex}
\caption{\label{org7ce5bf9}
An illustration of the additive mixture repartitioning. PPR for the same value of \(\beta=0.3\), added for comparison.}
\end{figure}

This, while inconvenient, can be mitigated. Indeed, since the
probability density functions (PDF) \(\pi_{i}(\theta; \beta) >0\),
the cumulative distribution functions (CDF)
\(CDF{\pi}_{i}(\theta;\beta) = \int_{\Psi} \pi_{i}(\theta; \beta)
	d\theta\) are monotonic, so is their sum, hence one could invert
the CDF numerically. This is extra work that we didn't have to
perform in the PPR case, because raising a Gaussian to a power
\(\beta\), is effectively the same as re-scaling its argument by
\(\sqrt{\beta}\), which transfers to the CDF.

However, one significant improvement over PPR is in
likelihoods. For two priors \(\pi_{1}\) and \(\pi_{2}\)
Normalising the likelihoods is trivial:
\begin{equation}
{\cal L}(\theta; \beta) = \frac{{\cal L}_{1}(\theta) \pi_{1}(\theta)}{\tilde{\pi}(\theta; \beta)}.
\end{equation}
where we've assumed that \({\cal L}_{1}(\theta)\pi_{1}(\theta)
	= {\cal L}_{2}(\theta) \pi_{2}(\theta)\). This generalises
straightforwardly to the case where we have more than one
prior. The likelihood is also a well-behaved function
in the prior space, (because we've required the priors be
non-zero in their domain), which is not always true for every
value of \(\beta\) and every prior in PPR.

Another advantage is that by construction the normalisation
factor \(Z \{ \pi\}(\beta) = 1\) for arbitraty \(\beta\). This
saves considerable effort: one does not care if correlatedness
of the Gaussian, alongside orientation issues can be corrected
for analytically, as one would with PPR\footnote{one couldargue
that correlatedness is irrelevant, as one can always
diagonalise the covariance matrix. The problem, however, is
thus transferred onto the boundary, where for a narrow prior
the orientation of the rectangle's edges in the covariance
eigenbasis can cause issues.}.

A flaw, which additive mixtures share with PPR, is that the
probability of having no bias is negligible. There's always a
preferred direction: if our original prior was uniform, the
probability of having no bias, is the probability of drawing the
value \(\beta=0\) at random. It is not nil, in our case, where
\(\beta\) can only be machine-represent-able 64-bit floating point
number, however this is sufficient to bias the sampler in almost
all cases.

One needs to be aware of this limitation when choosing which
mixing scheme to use. Sometimes, the smooth prior distribution and
likelihood are more beneficial; other times, the ability to with
some probability sample from a completely uniform prior is more
valuable. 

\subsubsection{Re-sizeable-bounds uniform prior.}
\label{sec:orgfb8830c}

The three requirements outlined at the beginning of this section
are not necessary and sufficient. As we have noted on page
\pageref{domain-discussion}, the domains of all functions need to be
consistent, otherwise Bayes' theorem no longer holds, and our
analysis is invalid. The mathematical implications of neglecting
function domains have in the context of Quantum mechanics. been
discussed by \cite{Gieres_2000}.

To illustrate, consider a uniform prior with the following
parametrisation.
\begin{equation}
  \tilde{\pi}(\theta; \beta) =
  \begin{cases}
	\frac{1}{\beta(b-a)} & \text{if}\ x \in [\beta a, \beta b] \\
	0 & \text{otherwise}.
  \end{cases}
\end{equation}
Although there are no issues when \(\beta>1\) (we simply set \({\cal
	\tilde{L}}=0\), one can immediately spot the issues with \(\beta \in (0,1)\);
and \(\beta=0\) is altogether nonsensical.

This issue indicates that the prescription of keeping \(\pi {\cal
	L} = \text{Const.}\) is not complete. Nevertheless, such a scheme
may be salvaged, with counter-intuitive extensions, e.g. for a
point \(\theta_{0} \notin \Psi\), we don't expect \({\cal
	L}(\theta_{0}) \rightarrow \infty\), but as we shall see in the
next section, \({\cal L}(\theta_{0}) \rightarrow 0\).

The first crucial step is to recognise that the algorithm draws
from a unit hypercube with uniform probability, and that the prior
is an artifact of a coordinate transformation which we referred to
as the prior quantile.

Let \(u\) be a point in unit hypercube \(\Psi_{C}\). The quantile
defines a mapping functionally dependent on the PDF of the prior
\(C(\beta)\lbrace \tilde{\pi}\rbrace:u \mapsto \theta\), such that
the uniform distribution of \(u\) leads through
\(C_{\beta}\{\tilde{\pi}\}(u)\) to a \(\tilde{\pi}(\theta;\beta)\)
distribution of \(\theta \in\Psi(\beta)\).Note that we replaced the
parametrisation of the function \(\tilde{\pi}\) with an explicit
parametrisation of the coordinate transformation, specifically
\begin{equation}
  \pi(C(\beta)\{\tilde{\pi}\}(u)) \equiv \tilde{\pi}(\theta; \beta),
\end{equation}
where 
\begin{equation}
  \tilde{\pi} =  \pi \circ C(\beta) \{ \pi \} 
\end{equation}
is a parameterised distribution resulting from a parameterised
coordinate transformation of an un-parameterised prior PDF.

We shall make \citeauthor{1763} 's theorem be defined only in the
hypercube
\begin{equation}
{\cal \hat{P}}(u) = {\cal P}(C(\beta_{0}){\tilde{\pi}}^{-1}(\theta)) = \frac{\hat{\pi} (u) {\cal \hat{L}}(u)}{\int_{\Psi}{\cal \hat{L}}(u) \hat{\pi}(u) du},
\end{equation}
which is always true, regardless of the re-partitioning
scheme. Trivially, the functional form of \(P(\theta)\) is not the same
as \(P(u)\); it's related via a co-ordinate transform, which in our
case contributes a Jacobian factor \(J(\beta)\{\tilde{\pi}\}\) to the
evidence. But since we're interested in the posterior in the
coordinates \(\theta\), given by the transformation \(C(\beta_{0})\{\tilde{\pi}\}\),
while the prior and the likelihood are in the from corresponding
to \(\beta\).

Finally, 
\begin{equation}
 {\cal P}(\theta) = \frac{J(\beta_{0})}{J(\beta)} \frac{\pi(\theta; \beta) {\cal L}(\theta; \beta)}{\int \pi(\theta; \beta) {\cal L}(\theta; \beta) d \theta}.
\end{equation}
So we expect that for the simple case of scaling the uniform box
prior with \(\beta\), that we need to re-scale the likelihood by
\(\beta^{2n}\). The second Jacobian factor enters the likelihood because
we have normalised \(\pi(\theta)\), but not \(\pi(\theta; \beta)\). This is hinted at in
the notation, (no tilde), and when accounted for, gives  the correct
posterior and evidence as seen in the experiments. 


\subsubsection{Stochastic superpositional re-partitioning.}
\label{sec:org4168f7e}

Hence we come to the concept of \emph{stochastic superposition-al
posterior repetition} (SSPR). Consider \(\tilde{\pi}(\theta)\) and
\({\cal \tilde{L}}\) which satisfy the normalisation
condition. We construct the parameterised prior like so
\begin{equation}
  \pi(\theta; \beta)  = \begin{cases}
	\pi(\theta) & \text{with probability } \beta\\
	\tilde{\pi}(\theta) & \text{with probability} (1- \beta)
	\end{cases}
\end{equation}
and similarly the likelihood.  The specialisation and
normalisation conditions are trivially satisfied, and the
convergence condition shall be argued later, so this
re-partitioning is valid.

There are difficulties with implementing this scheme,
however. Both the likelihood and the prior are well-defined
single-valued functions, so simply drawing a random number at each
evaluation is not acceptable. Moreover, one needs to make sure
that the branches are simultaneously chosen in both functions, so
as to ensure that the normalisation condition is satisfied. One
way to ensure these are met, is by choosing the branch
deterministic-ally, based on the vector \((\theta; \beta)\). 

To avoid biasing the nested sampler, we must preserve the
uniformity of the distribution. In other words, we must make sure
that the patches belonging to the same branch are interspersed and
are on average the size of regions mapping to the same branch are
the same and of the order of the resolution of the grid. In other
words, for the case \(\beta=1/2\), we wish to have a chequerboard
pattern of branching. 

Note, however, that the prior is no longer normalised. Indeed, for
different values of \(\beta\), integrating over the entire phase
space \(\Psi(\beta)\), one would expect not to obtain unity. And
although intuition might suggest that the normalisation factor
would depend on \(\beta\), as our experiments show this is not the
case. In this particular implementation, the total accessible
prior space volume is restricted by mutual exclusivity. On the
other hand, the posterior and evidence are both fixed by the
normalisation requirement of re-partitioning, so one does not
expect any scaling on \({\cal L}\). 

The greatest advantage that mixture repartitioning nets is
that it is model agnostic: one could, for example, use PPR as
part of a mixture of priors, or even a mixture of
mixtures. One, should judge which mixing method suits their
needs, is it better to have a large bias some of the time, or
a little bias all of the time?

In general,  if one has \(m\) models in a mixture, the likelihood becomes 
\begin{equation}
  {\cal L}(\theta; \beta)  = \begin{cases}
	{\cal L}_{1}(\theta) &  \text{with probability } \beta_{1}\\
		    &\vdots\\
	{\cal L}_{m}(\theta) & \text{with probability} (1- \sum_{i}\beta_{i})
	\end{cases}
\end{equation}


A more important question is of bounded-ness. As we've discussed
(page \pageref{domain-discussion}), when dealing with re-partitioning
schemes such as re-sizeable uniform priors, extra care must be
taken to account for the Jacobian factors arising from a change of
coordinates. Mixture re-partitioning, however, embeds the solution
into its formalism. For example, if a point in the posterior
distribution \(\theta_{e}\), is not represented in the prior, i.e.
\(\pi(\theta_{e}) = 0\), while \({\cal P}(\theta_{e}) \ne 0\), then
one intuitively expects \({\cal L}(\theta_{e}) \rightarrow
	\infty\). In mixture re-partitioning, however, if that same point is
represented in one prior and not the other, the others simply
become unrepresentative, and are selected against by the algorithm
if and only if \({\cal L}(\theta_{e}) = 0\), in the unrepresentative
branch. Thus the value is truly represented, just in a different
prior branch.

\begin{figure}
 \input{./illustrations/mixture.tex}
\caption{\label{orgabc04c1}
An example of a mixture re-partitioning. Notice that the mixture is not normalised to emphasise the coincidence of values with both the uniform distribution and a Gaussian.}
\end{figure}



\section{Method}
\label{sec:orgbc256bd}
In this section we shall describe in detail the types of simulations
and bench-marking that was done. As this project is highly
computational, Cosmological issues are discussed only incidentally,
and only with regard to their computational complexity, not the
Physics.

We have chosen to use Cobaya \citep{cobaya}, with CLASS to provide the
theoretical framework for analysing the Planck \citep{planck}
data. Our primary goal is to improve the performance of the
analysis.

We shall first describe how one would measure the performance of
such a run, then show the small-scale simulation results. Finally,
we shall discuss the results obtained by running Cobaya with the
suggested optimisations on the CSD3 cluster (University of Cambridge).


\subsection{Performance and bench-marking}
\label{sec:orgd864a1f}
One cannot use CPU time as a reliable indicator of
performance. There are multiple factors leading to unpredictable
overheads, and these can be practically averaged out on a small
scale model, in case of large distributed systems such as a CPU
cluster, with multiple processes, and with each run taking upwards
of an hour, this metric is beyond the realm usefulness.

Due to the sheer complexity of the Cosmological data and functions
involved in the computation, the usual asymptotic description
common in computer science is insufficient. 

First, note that in Cobaya  the run-time is dominated
by log-likelihood evaluations. A typical run in 3 dimensions
requires \(O(10^{3})\), likelihood calls, and if each of them takes a
second to evaluate, a simple run becomes impractical. 

So a natural choice for a performance metric is using the number of
log-likelihood evaluations. 

Note, however that this does not account for potential extra
complexity introduced by the re-partitioning. For example for PPR,
the effect of adding the extra parameter can be reduced to
\begin{enumerate}
\item one multiplication in the argument of the prior.
\item evaluation of the normalisation factor, which involves standard
numerical functions,
\item addition of the normalisation factor to each log-likelihood call.
\end{enumerate}

The corresponding overhead for mixture modelling is
\begin{enumerate}
\item hashing the vector \(\theta\).
\item generating a pseudo-random number using the hash as seed.
\item performing \(m-1\) conditional checks,
\item addition of \(\ln m\), to the likelihood.
\end{enumerate}

In both cases there's also a minuscule overhead associated with
lengthening the state vector \(\theta\)\footnote{in mixture modelling one could either introduce \(m+1\)
parameters, and perform the hashing once, at the cost of adding an
extra branch index, or add \(m\), parameters but perform the hashing
twice. To choose, mind that the extra branch index parameter, may
adversely impact the convergence as its posterior needs to be computed
just like any other nuisance parameter's.}.  Although these may
become important in low dimensional problems, they are overshadowed
in all practical applications of nested sampling, and thus we shall
ignore them.

Another information-theoretic performance metric that one could use
is the Kullback-Leibler divergence \({\cal D}\). A thorough
explanation of the concept can be found at \cite{Kullback_1951}, but
for our purposes, this is a quantity allowing to compare the prior
to the inferred posterior. The larger the value, the more Shannon
entropy is associated with moving from prior to posterior. 

To understand why K-L divergence is useful, consider that under
ideal circumstances inference with the posterior also the prior is
optimal. Hence, justifiably we expect priors with the smallest
\(\mathcal{D}\) to converge faster. This is a useful worldview when
considering general Bayesian inference, but its applicability to
nested sampling is limited. The performance of a nested sampler
depends on many factors besides the entropy. For example, as we've
shown in a preliminary experiment, \footnote{\texttt{./toy-models/2/2.1 Repartitioning with power posterior.py}} nested sampling can
converge faster if the distribution is narrower than the posterior
(PPR takes care of the correctness). 



\subsection{Correctness}
\label{sec:orgc6cae66}
One simple and unreliable way of determining the correctness of a
run is to compare the posteriors of two runs: if the means of two
runs are within one standard deviation of each other, then the
posteriors can be assumed to coincide.

Consider, however, what would happen, if one were to use a Gaussian
prior without posterior re-partitioning on a data set, which was
previously analysed using a uniform prior. One would expect the
posterior to have tighter constraints, smaller variances and for
the evidence to be much higher. Of course, it's normal if said
Gaussian truly represents prior knowledge, but as was mentioned in
previous sections, this is an error for any form of posterior
re-partitioning: it usually means that the re-scaling of the
likelihood is incorrect. Hence we must include (or rather base our
comparison on) the estimated evidences into consideration.

\begin{figure}
\input{./illustrations/histograms.tex}
\caption{\label{orga1047c1}
An illustration of the evidence distributions of different types of repartitioning schemes. The Uniform reference obtained a distribution centered around \(\log {\cal Z} = -62 = - \log V(\Psi)\) (see \autoref{eq:evidence}, where \((a,b)=(-6, 6)\cdot 10^{8}\) and \(G=\mathds{1}_{3}\)). Note that both mixture modelling and PPR have found the same value, and the distributions are more sharply peaked. Also notice that if the repartitioning is done incorrectly, the evidnece will also be estimated incorrectly. Finally, Both the true posterior and the mixture repartitioning have terminated without completing a single nested sampling iteration. This was sufficient to (correctly) determine the evidence, but it did not produce all the requisite chains, and hence no histogram could be produced.}
\end{figure}

Unfortunately, while a full analysis of the posterior distributions
would be much more in the spirit of Bayesian analysis, the data-sets
being are huge, so one cannot practically include all of
the ``triangle plots'' to prove the correctness of a run. We shall
provide one example, and drop the discussion: one should assume
that the posteriors coincide unless otherwise specified\footnote{to]] save time, the comparison had been automated: two Guassian
posterior distributions are said to coincide if and only if the means
were within one (the smaller) standard deviation  of each
other.}. 

\begin{figure}
 \includegraphics[width=0.5\textwidth]{./illustrations/misfit.pdf}
\caption{\label{orgce170d8}
An example of a posterior distribution generated with power posterior re-partitioning, based on data from planck. The posteriors are near identical, and a slight misfit can be explained with arithmetic rounding errors, and run-to-run variance of the position of the live points (see top right figure).}
\end{figure}




\subsection{Qualitative observations.}
\label{sec:orgb9c2e2e}
Last but not least, an interactive cartoon of the convergence
process for as many parameters as one likes can be obtained from

\begin{verbatim}
NestedSamples().gui()
\end{verbatim}
This allows us to see how the points move during the execution of
nested sampling. A more crude picture can be obtained from the plot
of \(\ln Z\) vs \(\ln X\), (which is also present, and used as a
timeline).

Based on the typical shape of the curve, we shall distinguish the
following stages of the algorithm's convergence. 

While \(\ln Z \approx 0\), nested sampling is in its \emph{prior
compression} stage.  Afterwards the algorithm undergoes \emph{discovery}
where most live points enter the typical set and their number is
permanently reduced. The last stage is the \emph{extinction stage},
colloquially referred to as the \emph{tail}.


\subsection{Simulations}
\label{sec:orga828b1e}
\subsubsection{Toy models}
\label{sec:org2b0d20f}

We shall begin our analysis with help of a simplified model that is
general-enough to share features with the Cosmological scale
problem, but also practical to investigate in depth, with multiple
variations.

Our original model is a Gaussian peak. By choosing the uniform
prior as a baseline, and setting the log-likelihood as:
\begin{equation}
  \ln {\cal L}(\theta) = - \frac{1}{2} \left\{(\theta - \mu)^{T}G^{-1}(\theta-\mu)  + \ln \det \left| 2\pi G\right| \right\}
\end{equation}
where the covariance matrix \(G\), specifies the extent of the peak,
and the vector \(\mu\), its location. We thus expect the posterior
to be a truncated and re-scaled Gaussian. However its typical set
is still approximately at a distance of the square root of the
diagonal elements of the covariance matrix form the peak, which we
shall refer to as \emph{one standard deviation}.

The covariance matrix is positive semi-definite and symmetric,
hence it can be diagonalised \citep{taboga2017lectures}. If the
covariance matrix is diagonal, the Gaussian distribution is called
uncorrelated. If all diagonal elements are equal, then the
Gaussian is spherical with characteristic diameter given by \(2
	\sigma = 2\sqrt{G}\), where \(G = G \mathds{1}\).

Notice that in this description we have completely neglected any
notion of ``data'', consequently, we don't need to worry about
generating said data, and the extra overheads associated with
\(\chi^2\) fitting.

Under such circumstances it's a matter of integrating \ref{eq:def-z}
to obtain the evidence. Most generally for a correlated Gaussian
likelihood the volume associated is 

\begin{equation}\label{eq:evidence}
   {\cal Z} = \frac{\left( \sqrt{ \det \left| 2\pi G \right|} \right)^{n}}{\mathbf{b}-\mathbf{a}}  
\end{equation}
where \(n\) is the number of parameters in the model.

The internal implementations of all our repartitioning schemes
contain two gaussians: one for the likelihood, and one
entering the repartitioning scheme to improve run-time. These
would be different in general and our simulations will reflect
that in the following ways.

The easiest to account for are translational offsets. One only needs to
modify the values of \(\theta' = \theta - \Delta\) entering \(\ln
	\mathcal{L}(\theta')\). 

One can, without loss of generality assume that one of the
Gaussians is uncorrelated (also WLOG, it's spherical);
effectively we need to apply a coordinate transformation
defined by the eigenvectors of the covariance matrix. We
cannot however assume that both are uncorrelated, nor that the
orthonormal vectors defining the Gaussian are aligned with the
boundaries of the uniform prior. Fortunately, these
complications contribute little. As we shall see, any
repartitioning scheme is easily able to cope with crude
approximations of the orientation and shape of the peak, and
run-time is affected negligibly. Consequently, outside of one
experiment, we shall ignore any deviations from a spherical
Gaussian.





\subsubsection{{\bfseries\sffamily TODO} Cosmological simulations}
\label{sec:orga616610}
For the Cosmological parameter estimation Cobaya \citep{cobaya} with
CLASS \citep{Blas_2011}, and PolyChord \citep{polychord} as a sampler
were chosen. The main reason being the high modularity of the code,
which allows a neater implementation of the re-partitioning
mac
\section{Results and Discussion.}
\label{sec:orgebb3e99}
The first test case is an uncorrelated spherical Gaussian posterior
in three dimensions \(\mathcal{P}(\theta) = G(\theta; \mu = (1,2,3),
  \sigma = 1)\). The corresponding evidence (\autoref{eq:evidence}) is
\(\mathcal{Z}\approx-62.3\). First we shall assume that the mean and
standard deviation of all the repartitioning schemes is exactly the
same as that of the posterior. 

All but one re-partitioning scheme yielded the correct
evidence. The resizeable uniform prior model was constructed to
systematically overestimating the evidence (\autoref{fig:hist}),
which is due to underestimating the normalisation factor for
\(\mathcal{L}\).\footnote{the boundary dependence was omitted.}


We shall now show that repartitioning is able to drastically reduce
the run-time compared to using a uniform prior. More specifically,
guessing a posterior distribution and using re-partitioning, one may
reduce the initial compression stage to virtually none.

     Having proven the correctness of the runs, let's turn to
   performance and benchmarks. The central metric is the number of
@@ -991,19 +996,69 @@ TODO
   case of exact coincidence of the mean and the standard deviation
   produces a respectable speed-up. 

The next trial involves a variable offset, where convergence to the
correct posterior and evidence is not guaranteed even with the
correct normalisation. 


\begin{figure}
  \input{illustrations/benchmark.tex}
\caption{\label{org9c12dda}
comparison of likelihood calls necessary for obtaining the correct evidence for the case of a spherical uncorrelated Gaussian posterior. Note that almost all series scale linearly with the number of live points.}
\end{figure}




The next trial involves a variable offset, where convergence to the
correct posterior and evidence is not guaranteed even with the
correct normalisation.    

For this case, we have taken a Gaussian in a box of
\(1000\times1000\times1000\), and generated two nested sampling data
ranges. The ``offset'' posteriors are moved relative to the mean of
the prior. The parameter labeled '1' is offset by double the amount
of parameter '0'. 

The exemplary results are given in \autoref{fig:convergence}.

The main notable feature is the inaccuracy of the posterior for
power posterior repartitioning. One does expect it to produce the
correct posterior distribution if the offset is large compared to
the width of the peaks. If the offset is \(O(2\sigma)\), the
posterior is merely shifted, but if the shift is larger,
e.g. \(O(4\sigma)\), two peaks can be resolved. Unfortunately for
PPR, the evidence was also computed incorrectly: \(\ln {\cal
  Z}\approx -25.4 \pm 0.2\), vs the reference \(\ln {\cal Z} = -22.7 \pm
  0.4\).  Making matters even worse, the smaller of the two peaks is
actually the correct posterior.

This is the fatal flaw of PPR, that prevents it from being useful in
any real application. The constant, ever present bias implicit to
using a Gaussian prior causes the sampler to be strongly biased
towards the original value. It can be redeemed: the mixture model in
the same figure involves the same PPR as one of two models. It
produced the correct result, while also, remarkably managing to do
so faster than a uniform prior would allow\footnote{another method could
involve a prior on \(\beta\), that strongly favours values close to
\(\beta=0\).}.

\begin{figure}
\includegraphics[width=0.5\textwidth]{./illustrations/convergence.pdf}
\caption{\label{orgff2fb8c}
An illustration of how offsets affect the convergence of nested sampling under different kinds of repartitioning. The offset models should produce an offset posterior, whilst sharing the prior with the model runs. The mixture is of the present uniform model and PPR.}
\end{figure}



One last discussion is that of so-called posterior mass. This allows
us to judge how quckly does the algorithm converge to the correct
values \cite{higson2018nestcheck}, as well as diagnose pathological
issues, specific to nested sampling. 

The plot on \autoref{fig:higson} showcases typical behaviour for
both a standard uniform-prior sampling, and the mixture
repartitioning. 

\begin{figure}
\includegraphics[width=0.5\textwidth]{./illustrations/higson.png}
\caption{An evolutionary insight into the behaviour of nested sampling. The \color{red} red \color{black} series corresponds to mixture repartitioning, while the \color{blue} blue \color{black} series -- to a reference uniform. All plots are given in \(\ln X\), where \(X(\mathcal{L}) \in [0,1]\) is the fraction of the prior with likelihood greater than \(\mathcal{L}\). The top plot is the relative posterior mass, which is the total weight assigned to samples from the region. In each row, we're presented with the posterior in the given parameter. The gradients represent degree of confidence. \label{fig:higson}}
\end{figure}

Firstly, note that the compression with repartitioning happens
much more quickly, consistent with our observations of run-time
reduction. Secondly, notice that the repartitioned series has a
much longer ``tail'', i.e. has a longer extinction phase. This is
a result of introducing extra nuisance parameters. Finally, notice
that the confidence intervals for the parameters' distributions
are near identical. This is a sign that the obtained posteriors
are more precise. Knowing that the means are \({0, 4, 8}\) with
parameter covariance matrix \(G = 1\), we can also confirm the
accuracy. 





\section{Conclusions}
\label{sec:org076aef0}

\subsection{Results}
\label{sec:org65ccc8b}
Our project's purpose had been to investigate the performance
increase attainable by algorithmic optimisations of the inputs to
nested samplers. 

We have identified a general prescription, named superpositional
mixture re-partitioning that netted the same if not greater
performance improvement as power posterior re-partitioning (PPR). 

We have also established that the aforementioned prescription had a
number of advantages:
\begin{itemize}
\item it allows multiple priors to be mixed, while PPR only allows one.
\item it permits a broader class of functions, than are practical for
PPR, e.g. ones where \(Z_{\pi}(\beta)\) cannot be represented in
closed form.
\item it copes with functions having different domains. PPR cannot.
\item it is abstract, i.e. the prior iCDF is a superposition iCDFs of
the constituents priors. For PPR the iCDF needs to be computed.
\item it supports an unbiased reference (uniform) prior exactly. PPR
tends to an unbiased reference as \(\beta\rightarrow\beta_{0}\).
\item TODO it is able to mitigate improper re-scaling of the
likelihood. If one of the priors is improperly normalised, the
offset from the true evidence is reduced as
\(n_{live}\rightarrow\infty\). PPR does not.
\item it is resilient to human error.
\item it is easier to implement and requires little to no mathematical
input from the user, beyond ensuring the three properties.
\end{itemize}


\subsection{Further research}
\label{sec:org2b3d2e3}
The proposed algorithm of superpositional mixtures, maps neatly
onto concepts of quantum computation. Indeed one can model two
different priors as states of qubits. The benefits are potentially
immeasureable, as the greatest weakness of the classical algorithm
is that we're effectively sacrificing resolution in the posterior
by sampling only from one prior exclusively. Quantum superposition
allows us to do both at the same time, while quantum entanglement
ensures that the deterministic requirements set by
\citeauthor{skilling2006} are met.

The necessary consequence of repartitioning is that the posterior
samples have a greater prior space to explore, and thus, while the
option to skip areas of negligible evidence allows us to compress
the priors significantly more quickly, the tail-end of the nested
sampling is also affected. A potential solution to this is to treat
the introduced parameters separately at this stage.

One such treatment may be to use the posterior distribution at the
point of discovery to freeze the choice parameters. Their
covariance may represent a volume in a \(\theta\) space that
corresponds to the remaining evidence, and as such, crudely
approximate the remainder while sampling from a lower dimensional
space compounding to the physical (i.e. not re-partitioning-related
parameters of the theory).

Additionally, we have assumed that nested sampling converges the
fastest if the prior is also the posterior. However, a simple
example of a spherical Gaussian in three dimensions shows the same
characteristic tail at the end of the execution. It may be
necessary to look into priors that are tailor made to accelerate
that convergence. Naturally, they would also depend on the sampling
technique used: the prior that accelerates rejection sampling would
be different to one that accelerates slice sampling.

An additional avenue to explore would be to ask whether the same
sampling tecnhique is appropriate for all stages. Slice sampling is
ideal for applications with prior space with large
dimensions. However, Metropolis-Hastings may be more suitable for
the extinction phase, and may thus eliminate the tail altogether.

Among the less-important investigations that could be carried out,
one might investigate an extension of the re-sizeable uniform
prior. Indeed one of the main reasons for its impracticality is the
sharp reduction to zero, that cannot be compensated for in the
likelihood. However, one should expect that this is possible to
compensate for by using a distribution that's constructed to be
non-zero in the entire domain of the original uniform prior:
e.g. by having edges that tend to zero at the boundaries. A
suggestion might be a smooth top-hat, or a combination of error
functions.

\subsection{Applications}
\label{sec:orgd8df224}
Nested sampling is a universal algorithm that can be applied to any
problem involving either direct parameter estimation (e.g. analysis
of Planck data), or indirectly such as neural-network based machine
learning.

To clarify the latter point, the process of training a neural
network involves a process of estimating the connection strengths
between layers of states. Normally training is done via a negative
feedbakc process, where the connections that correspond to the
right answer are reinforced, whilst connections leading to
incorrect ones are reduced in strength. In the formalism of
Bayesian inference, the connection strenghts are the parameters,
the prior is uniform and the sampling is done via
Metropolis-Hastings anticipating a logistical distribution. As we
are able to accelerate this process in Bayesian formalism, we
should also be able to modify the standard algorithms to make use
of repartitioning.

Moreover, the subject matter of this paper superpositional mixture
repartitioning with stochastic sampling can be used to create
classes of neural networks: as of now information obtained from
training one network cannot be re-used when training another,
unless the two networks have identical architecture and solve
identical problems. One cannot use the weights of a network
analysing faces as the initial values for analysing objects,
without that resulting in a strong bias. 

It may be possible to use the values of node connection strengths
from networks that are used for similar problems, by virtue of the
stability offered by repartitioning. We can regard that as one of
the priors in the mixture, and hence improving performance where
the guess is indeed accurate, without compromising the result if it
is not. Of course such neural networks will need to have a similar
number of physical connections, and hence have similar if not
identical architecture.

\bibliography{bibliography} 
\bibliographystyle{mnras}

\section{Appendices}
\label{sec:orga7de2dd}

\subsection{Why do we need to alter the likelihood.}
\label{sec:orged5162b}
One may ask why such a change of the likelihood is at all
necessary. Indeed, the likelihood may be chosen based on a precise
theory of error, e.g. a least-squares fit argument based on
Gaussian assumptions. Why does changing the prior knowledge
necessitate the change of likelihood?

In addition to what was mentioned in answer to a similar question
at the end of the previous subsection, there's an intuitive way of
answering this question. Consider a posterior distribution that at
no point takes the value nil (e.g. a Gaussian).]. If we constrain one
prior \(\pi\) to lie within one standard deviation of the peak,
(e.g. a sphere of radius \(\sigma\)), and another that spans twenty
standard deviations. If we picked 20 points at random from one and
the other, we shall expect that the iso-likelihood hyper-surfaces
would encase drastically different volumes. Moreover, finding a
point that's within one standard deviation from the perspective of
the broader prior is a much more significant result than finding
one from the narrower one. Indeed, we will not expect the posterior
distributions to be the same, but nested sampling would produce a
narrower peak based on the ``same'' model\footnote{from a frequentist's point of view, our prior knowledge is
irrelevant. But even a frequentist would agree that the value obtained
by changing the prior would not be the same.}. 

Of course, a Bayesian would say that if our true prior knowledge
was represented by the narrower prior, we would indeed need to
consider the posterior distribution to be the true one, as it
combines information that we've obtained earlier with information
that can be extracted from the data. In other words, it would be
the correct value for the person who indeed constrained the values
of model parameters to the one standard deviation, based on \emph{other
data}. Simply picking a prior out of thin air would bias the
result, hence the necessity to repartition. 


\subsection{Code}
\label{sec:orgdc789df}

All of the illustrations, figures, code that generated them along
with a generalised framework for mixing any kinds of priors into a
properly re partitioned posterior is available at the Git
repository: \url{https://github.com/appetrosyan/LCDM-NS} \cite{sspr}. 

All the preliminary testing was done in the \texttt{toy-models}
section. Code that generates simple dependency-less examples is in
the \texttt{illustrations} folder, code that generates the benchmarks and
correctness testing is given in the \texttt{framework} folder. Finally,
the modifications to Cobaya were done in-situ, therefore the fork
of Cobaya that contains a branch with posterior re-partitioning is
available as a \texttt{git} sub-module.

The current project depends on PolyChord \cite{polychord}, Cobaya
\cite{cobaya}, anesthetic \cite{anesthetic} and their respective
dependencies \cite{Blas_2011}.
\end{document}