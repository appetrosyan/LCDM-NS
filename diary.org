#+TITLE: Project Diary for "Cosmological parameter estimation using Bayesian accelerated Machine learning"
#+AUTHOR: Aleksandr Petrosyan
#+BIBLIOGRAPHY: bibliography.bib
#+OPTIONS: <:t broken-links:mark c:t d:t date:t p:t todo:t

This is a GNU emacs Org file. It's a sophisticated formate used mainly
for literate programming, and for creating outlines, time management,
project management and importantly for the purposes of the present
project: Facilities to track and timestamp every aspect of the
aforementioned features.

This file contains the notes and the project diary for the
``Cosmological Parameter estimation using accelerated Bayesian machine
learning'' project. The diary is a time-stamped list of activities
with links to their results.

The descriptions of activities are terse.

The reason is, that the entire point of keeping a diary is to be able
to tell if I had a *plan*, and have enough information to *reproduce*
the research. For the first, the timestamps as well as the scheduled
and deadline keywords are enough to tell when and how much time was
spent on each topic. For the latter, this diary's notes should provide
the minimum theoretical knowledge to make sense of the code. Since the
code itself is timestamped (it's a git repository), is based on simpe
intuitive concepts that do not normally require explanation. The few
cases that do, are covered in the [[*Notes][notes]].

Any theoretical findings are found and *properly* explained in the
[[file:paper.org][paper.]] Code is self-documenting, where explanations are needed, they
are provided in the docstrings.

Surveyed literature is compressed into the [[*Notes][Notes]] section. So are any
calculations necessary to understand how the re-partitioning works.


* Michaelmas
** Week 3
*** DONE Meet Dr Handley
    SCHEDULED: <2019-10-25 Fri>
    Discussed the project. Talked about Nested
    Sampling. Applications to Machine Learning.

    Primordial power spectrum, and discretisation. This causes the
    Fourier transform /multipole plot/ of the CMB. Multipole
    moment related to size of blotches on the picture.

** Week 4
*** DONE Read the Abstract. ,
    SCHEDULED: <2019-10-28 Mon>
    Read [[https://arxiv.org/abs/0803.4089][Bayes in the sky]] Read about Bayesian analysis.
    $\Omega_{k}$ more significant than thought.
*** DONE Install PolyChord, Cobaya, anesthetic, CosmoChord.
    SCHEDULED: <2019-10-26 Sat>
    Issues with Mac OS X/XCode.
    GFortran was installed but not on PATH.
    PolyChord not Pip installable.
    #+BEGIN_SRC bash
    C=cc CXX=cxx python setup.py install â€”user
    #+END_SRC
    doesn't work on OS X.

    Use Python3.
*** DONE EMail will and create a PR to fix Python version.
    SCHEDULED: <2019-10-27 Sun>
	Fixed. Merged. Working.
*** FAILED Reproduce plots.
    SCHEDULED: <2019-11-08 Fri>

	Laptop inoperative due to Liquid damage.
** Week 5
   Laptop inoperative due to liquid damage.  Until week 2 of Vacation
   laptop remained under liquid damage. These entries filled
   retroactively from Todoist.
*** Talk on nested sampling <2019-11-14>
    It's a method of inference. Much the same as least squares
    fitting, except it's not assuming the central limit theorem and
    works for arbitrary distributions of error.


*** DONE Start final report writeup <2019-11-19>
    See git history.
*** DONE Finish reading Skilling.
    SCHEDULED: <2019-11-15 Fri>
    See notes.
*** DONE Implement Skilling's algorithm
    SCHEDULED: <2019-11-17 Sun>
    Tried to do on the PWF, using example code at the end of the paper. cite:skilling.
    Hard to do. Need laptop repaired. C code in the repository.
*** Note from meeting. <2019-11-15 Fri>
    Implementing Skilling's algorithm wasn't necessary.
*** Note from meeting. <2019-11-15 Fri>
    The idea of re-partitioning. Read [[https://arxiv.org/pdf/1908.04655.pdf][Chen-Feroz-Hobson]].
*** Note from meeting <2019-11-15 Fri>
    Project writeup

** Week 7

*** DONE Implement Multivariate Gaussian Likelihood <2019-11-17>
    Used example code as template.
    See toy-models/multivariate-gaussian.py
*** DONE Investigate the C++ front-end. <2019-11-19>
    PolyChord works as a framework. Unable to control many things
    including verbosity of output.
*** DONE Project report. <2019-11-21 Thu>
	[[http://www.mrao.cam.ac.uk/~wh260/Galileo/
	][Example writeups]].
** Week 8
*** DONE Finalise Project report. <2019-11-25 Mon>
*** DONE Proof read the report <2019-11-29 Fri>
*** DONE Submit the report. <2019-12-04 Wed>
    Good staplers are not in Cavendish. Had to re-print and re-submit
    because the one in Kavli chewed up the paper and the one at
    Rayleigh library was not functional.
** Vacation Weeks
*** DONE Re-install software <2019-12-04 Wed>
    a) =polychord= (GitHub)
    b) =anesthetic= (=pip=)
    c) =fgivenx= (=pip=)
*** DONE Line Fitting example. <2019-12-09 Mon>
    See =0/extended-example.py=.
*** DONE Set up CSD3 login information. <2019-12-19 Thu>

    PI Name: Will Handley
    PI Status: Research fellow
    PI Email: wh260@cam.ac.uk
    PI Phone: +44-(0)1223-764042
    PI Department: Cavendish Laboratory (Astrophysics)
    PI School: Physical Sciences

    Research Group: Astrophysics
    Department: Cavendish Laboratory
    School: Physical Sciences
    Service Level: Non-paying (SL3) only
    Project: (Leave blank)

    End Date: 01/01/2021 (To give us time to write up)
    Compute Platforms: (leave blank)
    Dedicated nodes: (None)
    PI Declaration: tick yes
*** DONE Read about Bayesian statistics. \(\chi^{2}\) test.
    Notes.
* Lent
** Week 1
*** Meet Dr Handley <2020-01-17 Fri>
    Talked about the line fitting example.
*** DONE Re-factor the line-fitting examples
    SCHEDULED: <2020-01-17 Fri>
    :LOGBOOK:
    CLOCK: [2020-01-17 Fri 22:58]--[2020-01-18 Sat 00:25] =>  1:27
    CLOCK: [2020-01-17 Fri 20:32]--[2020-01-17 Fri 22:40] =>  2:08
    :END:
    [[./toy-models/0/0.1 extended_example.py]].  Do I need to generate the
    data? Can I use the parameter covariance matrix to emulate the
    data?

    better to have a data generator and not use it than to not have it
    and need it.
*** DONE Implement the Data generator
    SCHEDULED: <2020-01-20 Mon>
    :LOGBOOK:
    CLOCK: [2020-01-22 Sat 09:15]--[2020-01-22 Wed 13:15] =>  4:00
    CLOCK: [2020-01-21 Tue 09:03]--[2020-01-21 Tue 16:30] =>  7:27
    CLOCK: [2020-01-20 Mon 09:17]--[2020-01-20 Mon 12:20] =>  3:03
    :END:
    DEADLINE: <2020-01-22 Wed>
    Implemented! It works!
    [[./toy-models/0/0.2 DataCovarianceWithGenerator/DataGenerator.py]]
    Probably overengineered.

    Simple noise overlayed on top of data predicted by model. Use
    chi-squared likelihood fit from
    [[./toy-models/0/0.2 DataCovarianceWithGenerator/Polychord.py]].

    Credit

    [[http://jrmeyer.github.io/machinelearning/2017/08/18/mle.html]]

    update: ln_z is an exceedingly bad name for the loglikelihood.
** Week 2
*** DONE meet Dr Handley.
    SCHEDULED: <2020-01-22 Wed>
    The data generator isn't necessary. I was using too many live
    points. $200$ live points is good-enough for publication quality
    results. Reduce that to 20. Try it with Planck chains.

    [[https://doi.org/10.5281/zenodo.3371152]]

	Try to see if the parameter covariance matrices yield the same
	results as the Planck chains.
*** DONE Refactor the following script. Check that posteriors agree with paper
	SCHEDULED: <2020-01-23 Thu>
	:LOGBOOK:
	CLOCK: [2020-01-29 Wed 21:30]--[2020-01-29 Wed 22:00] =>  0:30
	CLOCK: [2020-01-23 Thu 15:27]--[2020-01-23 Thu 19:35] =>  4:08
	CLOCK: [2020-01-23 Thu 21:27]--[2020-01-23 Thu 23:35] =>  2:08
	:END:

	#+BEGIN_SRC python
	  import numpy
	  import pypolychord
	  from pypolychord.settings import PolyChordSettings
	  from pypolychord.priors import UniformPrior

	  nDims = 2
	  nDerived = 0
	  mu = numpy.array([0,0])
	  Sig = numpy.array([[1,0.99],[0.99,1]])
	  invSig = numpy.linalg.inv(Sig)
	  norm = numpy.linalg.slogdet(2*numpy.pi*Sig)[1]/2


	  def likelihood(theta):
		  """ Simple Gaussian Likelihood"""
		  logL = - norm  - (theta - mu) @ invSig @ (theta - mu) / 2
		  return logL, []


	  def prior(hypercube):
		  """ Uniform prior from [-1,1]^D. """
		  return UniformPrior(-20, 20)(hypercube)


	  settings = PolyChordSettings(nDims, nDerived)
	  settings.file_root = 'gaussian_2'
	  settings.nlive = 200
	  settings.do_clustering = True
	  settings.read_resume = False

	  output = pypolychord.run_polychord(likelihood, nDims, nDerived, settings, prior)
	  from anesthetic import NestedSamples
	  samples = NestedSamples(root='./chains/gaussian')
	  fig, ax = samples.plot_2d([0,1])
	  samples_1 = NestedSamples(root='./chains/gaussian_1')
	  samples_1.plot_2d(ax)
	  samples_2 = NestedSamples(root='./chains/gaussian_2')
	  samples_2.plot_2d(ax)



	  samples = NestedSamples(root='/data/will/tension/runs/lcdm/chains/planck')
	  samples.plot_2d(['logA', 'ns'])

	  params = samples.columns[:27]
	  Sig = samples[params].cov().values
	  mu = samples[params].mean().values
	  invSig = numpy.linalg.inv(Sig)
	  norm = numpy.linalg.slogdet(2*numpy.pi*Sig)[1]/2
	  nDims = len(mu)

	  ranges = numpy.array(
				  [[0.019,0.025],
				  [0.095,0.145],
				  [1.03,1.05],
				  [0.01,0.4],
				  [2.5,3.7],
				  [0.885,1.04],
				  [0.9,1.1],
				  [0,200],
				  [0,1],
				  [0,10],
				  [0,400],
				  [0,400],
				  [0,400],
				  [0,400],
				  [0,10],
				  [0,50],
				  [0,50],
				  [0,100],
				  [0,400],
				  [0,10],
				  [0,10],
				  [0,10],
				  [0,10],
				  [0,10],
				  [0,10],
				  [0,3],
				  [0,3]])

	  def prior(hypercube):
		  """ Uniform prior from [-1,1]^D. """
		  return ranges[:,0] + hypercube * (ranges[:,1]- ranges[:,0])


	  settings = PolyChordSettings(nDims, nDerived)
	  settings.file_root = 'gaussian_2'
	  settings.nlive = 200
	  settings.do_clustering = True
	  settings.read_resume = False

	  samples = NestedSamples(root='/data/will/tension/runs/lcdm/chains/planck')
	  samples.plot_2d(['logA', 'ns'])

	  output = pypolychord.run_polychord(likelihood, nDims, nDerived, settings, prior)

	#+END_SRC

	Produces weird misfit.
	[[./toy-models/1/Comparison of run with uniform prior and paper.pdf]]

	update: EUREKA! Neither PolyChord nor anesthetic when loading the
	samples from disk actually re-name the parameters. I was simply
	comparing the runs of different parameters, no wonder their
	posteriors had nothing in common.

	Need to use a =numpy.ndarray= as input. Convert into a pandas data
	frame and convert it back in the output.

	Updated code is
	[[./toy-models/1/1.0 Example of parameter covariance.py]]
*** DONE Implement Uncorrelated Gaussian
	:LOGBOOK:
	CLOCK: [2020-01-27 Mon 09:36]--[2020-04-19 Sun 00:42] => 1983:06
	:END:
	Be careful: \(erf\) maps \(R\) to \([-1,1]\) but we need a mapping
	from \([0,1]\) back to \(R\). So use \(\text{erf}^{-1}(2x - 1)\).

	See [[./toy-models/1/1.1 parameter-covariance - with Gaussians.py]]
*** DONE Implement Correlated Gaussian <2020-01-28>
	:LOGBOOK:
	CLOCK: [2020-01-28 Tue 10:42]--[2020-01-28 Tue 17:43] =>  7:01
	:END:
	Chloesky decomposition makes the off-diagonal elements of the
	\(\Sigma\) matrix more important. This is because the covariance
	matrix has certain properties: it's positive definite,
	invertible ---  we can't just invent one.

	\begin{equation}
	  \Sigma =
	  \begin{pmatrix}
		1 & 0.5 & 0.5\\
		0.5 & 1 & 0.5\\
		0.5 & 0.5 & 1
	  \end{pmatrix}
	  =
	  \begin{pmatrix}
	  1 & -1 & -1 \\
	  1 & 1 & 0 \\
	  1 & 0 &1
	  \end{pmatrix}
	  \frac{\text{diag} (2, 0.5, 0.5)}{3}
	  \begin{pmatrix}
	  1 & 1 & 1 \\
	  -1 & 2 & -1 \\
	  -1 & -1 & 2
	  \end{pmatrix}
	\end{equation}
	Has all of the aforementioned properties. It is
	diagonaliseable. This is an ellipsoid stretched across the corners
	and the edge midpoints of a unit hyper-cube. This has all of our
	needed properties.
** Week 3
   This week was highly unproductive. Not my fault. If you're
   interested to know, I passed out from exhaustion and malnutrition,
   and decided to let it go easy for one week.

   IF you're also interested in *why* I passed out, and *why* I was malnourished, I suggest reading :
*** DONE Implement PPR. <2020-02-03 Mon>
	DEADLINE: <2020-02-07 Fri>
	SCHEDULED: <2020-02-03 Mon> [[*Calculations neeeded for doing PPR.][Calculations neeeded for doing PPR.]]
	I've done this by hadn as well. Originally made the mistake of
	assuming that the Gaussian is unbounded. I then realised that it
	wasn't even in the ordinary cases that I've considered previously.

	The correction was of the form containing \(\text{erf}^{-1}(\ldots)\) while I assumed it was the intergral over all of space \(\sigma/\beta\).
** Week 4
*** DONE implement Additive mixtures.
	DEADLINE: <2020-02-20 Thu>
	SCHEDULED: <2020-02-05 Wed>
	Additive mixtures are not invertible. I can try and do it
	numerically, the functions are monotonic, but this is unstable. It
	always results in

	#+BEGIN_SRC bash
	PolyChord Warning: non-deterministic log-likelihood.
	#+END_SRC

	UPDATE: Needed to use

	#+BEGIN_SRC python
	from scipy.optimize import root

	 ....
	pdf_tilde = lambda x : pdf1(x) + pdf2(x)
	quantile = lambda cube: root(pdf_tilde, pdf1inv(cube) + pdf2inv(cube), method='linearmixing')
	#+END_SRC

	for additive mixtures. It still occasionally causes
	non-deterministic issues. This is really no different to PPR.

	Maybe there's a better way?

	Update: Dr Handley says that I should try to piece-wise
	invert. This is terrible! This gives me nothing like the original
	distribution.

	[[[[./toy-models/2/convergence - repartitioned vs uniform.pdf]]]]

	This is terrible.
** Week 5
*** DONE Test the Lasenby parameter.
	SCHEDULED: <2020-02-10 Mon>
	The `Lasenby' parameter is the procedure described by
	Chen-Ferroz-Hobson to do re-partitioning, except we don't re-scale
	the log-likelihood.

	This should not run faster. But it does.

	[[./toy-models/2/run_time - repart vs uniform.pdf]]

	[[./toy-models/2/2.0 Comparison of runtime with Lasenby parameter.py]]
	Has all of the properties we need. It is diagonaliseable. This
	is an ellipsoid stretched across the corners and the edge
	midpoints of a unit hyper-cube. This has all of our needed
	properties.


*** DONE Investigate why the Lasenby parameter speeds up the calculation.
	DEADLINE: <2020-02-16 Sun>
	SCHEDULED: <2020-02-12 Wed>
	See [[*Why PolyChord converges faster for more informative priors.][Why PolyChord converges faster for more informative priors.]]
	This is weird. it should not happen. Or should it?

	Maybe we are changing the prior? See, we don't have to change
	\({\cal L}\), it would happen as if we had the more informative
	prior.
*** DONE Convince Dr Handley that we don't need to change the Likelihood.
	SCHEDULED: <2020-02-15 Sat>
	Give the example of $\beta\rightarrow 1$ not giving us the same
	Gaussian.

	The evidence is different, but so is our prior. Why should we keep
	${\cal P}$ constant instead of insisting that ${\cal L}$ is
	constant?

	Why can't we assume that the prior is narrower? Why do we have to
	insist that our likelihood changes not the posterior? The
	likelihood is what we are given from nature, neither the data, nor
	the model change. Why when changing the prior do we need to change
	${\cal L}$?

	UPDATE: see [[*When and why do repartitioning. <2020-02-19 Wed>][When and why do repartitioning.]]
*** TODO Prove or disprove the following snippet
	 Dr Handley seems to think that the re-partitioning is necessary.
	#+BEGIN_SRC python
	from numpy import pi, log, sqrt, exp
from scipy.special import erfinv, erf
import numpy
import pypolychord
from pypolychord.settings import PolyChordSettings
from anesthetic import NestedSamples
import matplotlib.pyplot as plt

#| Define a four-dimensional spherical gaussian likelihood,
#| width sigma=0.1, centered on the 0 with one derived parameter.
#| The derived parameter is the squared radius

nDims = 4
nDerived = 0
sigma = 0.1
mu = 0

def loglikelihood(theta):
    """ Simple Gaussian Likelihood """
    return -(theta-mu) @ (theta-mu)/2/sigma**2 , []

#| Define a box uniform prior
thetamin, thetamax = -1e3, 1e3
def prior(hypercube):
    return thetamin + hypercube*(thetamax-thetamin)

#| The true logZ should take this value
def true_logZ():
    return nDims*(log(2*pi*sigma**2)/2.0 - numpy.log(thetamax-thetamin))

#| Run polychord
settings = PolyChordSettings(nDims, nDerived)
settings.file_root = 'gaussian'
settings.read_resume = False
output = pypolychord.run_polychord(loglikelihood, nDims, nDerived, settings, prior)
samples = NestedSamples(root='./chains/gaussian')

#| Check that the log-evidence is similar to the true
plt.subplots()
plt.hist(samples.logZ(1000))
plt.axvline(true_logZ(),color='k')
plt.show()


#| Define a transformation for a gaussian prior
def power_gaussian_prior(hypercube, beta, mu, sigma, a, b):
    return mu + sqrt(2/beta)*sigma*erfinv((1-hypercube)*erf((a-mu)*sqrt(beta/2)/sigma) + hypercube*erf((b-mu)*sqrt(beta/2)/sigma))


#| The prior density of the power gaussian prior
def log_repartitioned_prior_density(theta, beta, mu, sigma, a, b):
    return -beta * (theta-mu)**2/2/sigma**2 - log(pi*sigma**2/2/beta)/2 - log(erf((b-mu)*sqrt(beta/2)/sigma)- erf((a-mu)*sqrt(beta/2)/sigma))


#| Repartitioned prior
betamin, betamax = 0, 1
def repartitioned_prior(hypercube):
    x = hypercube[:nDims]
    b = hypercube[-1]
    beta = betamin + (betamax-betamin)*b
    theta = power_gaussian_prior(x, beta, mu, sigma, thetamin,thetamax)
    return numpy.concatenate([theta ,[beta]])


#| Repartitioned loglikelihood
def repartitioned_loglikelihood(theta):
    t = theta[:nDims]
    beta = theta[-1]
    logl, phi = loglikelihood(t)
    logl += len(t) * -numpy.log(thetamax-thetamin)
    logl -= log_repartitioned_prior_density(t, beta, mu, sigma, thetamin, thetamax).sum()
    return logl, phi


settings = PolyChordSettings(nDims+1, nDerived)
settings.file_root = 'repartitioned_gaussian'
settings.read_resume = False
output = pypolychord.run_polychord(repartitioned_loglikelihood, nDims+1, nDerived, settings, repartitioned_prior)
repartitioned_samples = NestedSamples(root='./chains/repartitioned_gaussian')

#| Check evidences. The repartitioned evidence is correct and much more accurate
plt.hist(repartitioned_samples.logZ(1000))
plt.axvline(true_logZ(),color='k')
plt.show()

#| Check parameters. These look right
fig, axes = samples.plot_2d([0,1])
repartitioned_samples.plot_2d(axes)
plt.show()

#| Examine run. This looks typical
samples.gui()

#| Examine repartitioned run.
#| This is very unusual. The tail is huge, which is why the runtime (equivalent to ndead, nlike, or logX in the plot) is not faster. If you examine the slider with parameter 4 (i.e. beta) set, then you can see as nested sampling progresses past the posterior bulk, the live points have a lower and lower beta.
repartitioned_samples.gui()

	#+END_SRC

	This is a reference implementation of PPR. It evaluates the
	correct eviendece. It plots the correct histogram.

	Well, for one the likelihood is not normalised. This script just
	fudges evidence.

	This can't be correct.

	UPDATE: it's not. There are mistakes in the implementation of
	PPR. The corrected version is given in

	[[./toy-models/lnZ-histograms.pdf]]

	This doesn't give you the same evidence as the Gaussian and the
	Gaussian has nothing to do with the evidence that we obtain the
	calculation is wrong.
** Week 6
*** DONE Meet Dr Handley
	:LOGBOOK:
	CLOCK: [2020-02-19 Wed 19:20]--[2020-02-19 Wed 20:20] =>  1:00
	:END:
	The Gaussian isn't the prior we start with. This is the way it is
	in Chen-Ferroz-Hobson, but our premise is that we start with a
	uniform prior and cheat. This is not guaranteed to give us the
	correct posterior, and this will give us a biased posterior if
	there's an offset between the prior and the likelihood.

	This is wrong, but that's what we want to do. Maybe SSPR does that
	better.

*** DONE implement SSPR.
	:LOGBOOK:
	CLOCK: [2020-02-23 Sun 11:55]--[2020-04-23 Thu 19:29] => 1447:34
	:END:
	It works.
**** DONE Refactor everything into models.
	 see =./framework/=.
**** DONE Create a new interface for PolyChord
	 :LOGBOOK:
	 CLOCK: [2020-02-23 Sun 12:29]--[2020-02-23 Sun 19:30] =>  7:01
	 :END:
	 See [[./framework/polychord_model.py]]
**** DONE Implement  PPR as a model.
	 :LOGBOOK:
	 CLOCK: [2020-02-23 Sun 19:34]--[2020-02-23 Sun 22:00] =>  2:26
	 :END:
	 see [[./framework/ppr_model.py]]
**** DONE implement Re-sizeable box uniform.
	 :LOGBOOK:
	 CLOCK: [2020-02-25 Tue 13:35]--[2020-02-25 Tue 19:00] =>  5:25
	 :END:
	 see [[./framework/resize_box_uniform.py]]
**** DONE implement Stochastic mixing
	 :LOGBOOK:
	 CLOCK: [2020-02-27 Thu 11:36]--[2020-04-23 Thu 19:38] => 1352:02
	 :END:
	 Implementations using =hash()= are fragile. Need to think of
	 alternatives. Maybe =rand()= with a seed? Too much work?
**** DONE implement stochastic mixing of more than one model
	 :LOGBOOK:
	 CLOCK: [2020-02-29 Sat 19:38]--[2020-04-23 Thu 20:04] => 1296:26
	 :END:
	 The $\beta_{i}$ parameters are not probabilities of each branch, but
	 they are related to them, but collectively, not individually.

	 Instead of assuming that they are, do the following

	 #+BEGIN_SRC python
	 index = 0
	r = abs(hash(tuple(t)))
	for p in ps:
	    if r > p:
		break
	    index += 1
	_nDims = self.models[index].eff_nDims
	 #+END_SRC

	 EDIT: there's a bug. Python's hash function isn't what it
	 seems. It's pseudo random for integers, but for floats in (0,1)
	 this is not so. Of course it only affects floats if and only if
	 the nDims=1, because the hash(tuples) are xors of the constituent
	 coordinate vectors' hashes and nearby vectors hash to different
	 values depending on their bit representation. The more elements
	 in the vector the more likeliy this is not to be a problem.

	 UPDATE: This is a problem. Neraby vectors hash to the same
	 representation, so instead of having a chequerboard like-pattern
	 we essentially get A Gaussian in the lower half, and a uniform in
	 the upper half determined by $\beta_{0}$.

	 I didn't pick up on this, because the Gaussian is also piecewise
	 superpositional. The probability is correct. In instead of a
	 chequerboard pattern I painted half the board black. FIxed by
	 moving to a Mersenne Twister based random. See
	 [[./framework/general_mixture_model.py]].
** Week 7
*** DONE start writeup.
	:LOGBOOK:
	CLOCK: [2020-03-04 Wed 10:52]--[2020-03-04 Wed 18:52] =>  8:00
	CLOCK: [2020-03-02 Mon 20:04]--[2020-03-02 Mon 23:30] =>  3:26
	:END:
	Created the Latex template, written a stub for introduction.
*** DONE Talk to Lukas Hergt about Cosmological inference machines.
	SCHEDULED: <2020-03-09 Mon 16:00>
	Four possible choices.
	- CosmoChord -- Fortran -- Written by Dr Handley
	- Monte Python -- Python -- Good name. Probably well-written.
	- Cobaya -- Python -- Really well-written and is the successor of
      the old code.
	- Writing one from scratch.
	It's not as crucial to get the stuff done right, as it is to get
	it done quickly. So maybe go with the library with the most
	support: Cobaya.

	It's input is arranged into a Yaml file (edit: it also is a python module).

	What I need to worry about is that the {lowl-EE likelihood,
	curvature, PolyChord} setting. This causes segfaults.

	Why?

	plik = Planck Likelihood
	clik = Camspec likelihoods
	[[https://arxiv.org/pdf/2002.06892.pdf]]

	[[https://cobaya.readthedocs.io]]
	low/high l - low/high multipole moment.
	TT - temperature
	EE - polarisation

*** TODO debug the segfault.

	This is caused by an extra nuisance parameter being passed. Need
	to figure out which parameter is extraneous get rid of it, and
	create a pull request to fix the code.

**** TODO Figure out which parameter is extraneous.

**** TODO prevent it from being passed

**** TODO Create a PR.

**** TODO Merge the PR.

** Week 8
   This week was when I was told that I need to pack my bags and
   leave.

   Yeah Sure. And what if I get stuck in quarantine for two weeks and
   can't work? What about the exams? What if I'm not able to return
   during term time? So what if I get sick? There's nobody to care for
   me anywhere in the world! I can handle being sick.

   I'm going to fail. There are no questions about it. I'm going to
   fail. They're going to fail me. Isn't it obvious? As if the
   brittish kids didn't have enough of an advantage.


* Easter ''Vacation''
** Week 1.
*** DONE Set up SSH access to CSD3.
    :LOGBOOK:
    CLOCK: [2020-03-18 Wed 12:19]--[2020-03-18 Wed 19:00] =>  6:41
    :END:
    Covid 19 caused a major disruption.  I was forced out of College,
    required to return to Armenia. Spent the entire week under
    quarantine.  Thankfully had some internet access. I'm sure no-one
    is going to account for the time/stress incurred losses. Why would
    they. That would be putting the Brits that only need to stay in
    their homes at an unfair disadvantage.
**** DONE Find out how to bypass restriction.
     :LOGBOOK:
     CLOCK: [2020-05-18 Mon 13:23]--[2020-04-18 Sat 18:23] => -715:00
     :END:
     Can't =ssh=. Connection rejected. Cambridge VPN not helping. Can't
     connect to it either.
**** DONE Write a script to probe for forwarding ports
     :LOGBOOK:
     CLOCK: [2020-03-18 Wed 13:48]--[2020-03-18 Wed 18:24] =>  4:39
     :END:
     Found a forwarding port on the router. Use
     #+BEGIN_SRC bash
     ssh -R 52.194.1.73:8080:localhost:80 ap886@login-hpc.cam.ac.uk
     #+END_SRC
     to connect.
*** DONE install Cobaya
    :LOGBOOK:
    CLOCK: [2020-03-19 Thu 19:00]--[2020-03-21 Sat 22:30] => 51:30
    :END:
    Did it very thoroughly. MKL not identified. Will need to
    debug. Later. =mpirun cobaya= works. It doesn't output, but at
    this stage it's not important.
*** DONE Modify cobaya
    :LOGBOOK:
    CLOCK: [2020-03-19 Thu 11:47]--[2020-03-21 Sat 22:47] => 59:00
    :END:

    Cobaya is mess. There's no notion of OOP design. Overridin a class
    should *not* have an =initialize= method. Instead it should have
    an =__init__= method.

    Tried to extract commented blocks out into functions. This is hard
    work. I'm not even paid to do it. Implementing a proper channel in
    =.yaml= will be untenable. It will take more time than this entire
    project to figure out how to do it without breaking anything.

    I should speak to Dr Handley about this and discuss how Cobaya's
    sampler should be implemented.

    update: I should create a fork. Implement in situ and reference in
    the project writeup. As much as I'd like to clean it up, it's not
    really even my project to worry about.

*** DONE Meet Dr Handley
    :LOGBOOK:
    CLOCK: [2020-03-20 Fri 13:00]--[2020-03-20 Fri 13:40] =>  0:40
    :END:
    Minutes. Try to have a writeup. Mentioned that I already have
    one. Asked about using the cluster to run benchmarks. Given a
    go-ahead. This should save a lot of time, given that my laptop is
    nowhere near 64-cores.
*** DONE Writeup
    SCHEDULED: <2020-03-21 Sat>
    :LOGBOOK:
    CLOCK: [2020-03-21 Sat 12:27]--[2020-03-21 Sat 14:28] =>  2:01
    CLOCK: [2020-03-21 Sat 15:30]--[2020-03-21 Sat 17:53] =>  2:23
    CLOCK: [2020-03-21 Sat 19:00]--[2020-03-21 Sat 21:28] =>  2:28
    CLOCK: [2020-03-21 Sat 22:00]--[2020-03-22 Sun 02:31] =>  4:31
    :END:
    Available on Github. Refined introduction. Added Bayes' theorem to
    writeup. Migrated to add mnras.


** Week 2
*** DONE Get Home
    :LOGBOOK:
    CLOCK: [2020-03-29 Sun 18:44]--[2020-03-25 Wed 19:00] => -95:44
    :END:
    SCHEDULED: <2020-03-25 Wed>
*** DONE Set up home Computer
    SCHEDULED: <2020-03-28 Sat>
    Fresh install of Arch.
**** DONE install software
     :LOGBOOK:
     CLOCK: [2020-03-29 Sun 16:39]--[2020-03-30 Mon 19:41] => 27:02
     :END:
***** DONE numpy
***** DONE MPI
***** DONE anesthetic
***** DONE Cobaya
***** DONE PolyChord
      gfortran in the repos not the required version. Used AUR. Created venv.
**** DONE Add home computer's ssh key to CSD3
*** DONE Migrate to Mnras
    :LOGBOOK:
    CLOCK: [2020-03-26 Thu 10:41]--[2020-03-26 Thu 11:25] =>  0:44
    :END:
    Add everything needed to conform to the [[https://academic.oup.com/mnras/pages/general_instructions][mnras style guide]].
*** DONE Write an sbatch script to run Cobaya.
    :LOGBOOK:
    CLOCK: [2020-03-24 Tue 12:49]--[2020-03-26 Thu 18:50] => 54:01
    :END:
    Completely unsure about the memory and number of clusters. Need to
    ask Lukas Hergt about the values.

    Cobaya's generated =.yaml= file is not working. Needs a few updates.
*** DONE Add figures
    :LOGBOOK:
    CLOCK: [2020-03-29 Sun 14:22]--[2020-04-18 Sat 22:26] => 488:04
    CLOCK: [2020-03-29 Sun 09:22]--[2020-03-29 Sun 11:10] =>  1:48
    :END:
    Used =tikzplotlib=. Plots in illustrations. Added illustrations of
    repartitioining functions.



*** DONE Writeup
    :LOGBOOK:
    CLOCK: [2020-03-27 Fri 09:46]--[2020-03-27 Fri 10:47] =>  1:01
    :END:
    Added clairifcation. Bayes' theorem paper. Moved defs into table.

*** DONE Meet Dr Handley
    :LOGBOOK:
    CLOCK: [2020-03-27 Fri 14:00]--[2020-03-27 Fri 14:55] =>  0:55
    :END:
    Received comments/compliments. More plots. Use the home computer
    and cluster to accelerate calculations.



*** DONE Implement offset.
	SCHEDULED: <2020-04-05 Sun>
	:LOGBOOK:
	CLOCK: [2020-04-05 Sun 17:00]--[2020-04-05 Sun 18:00] =>  1:00
	:END:
	Very easy to do. Just implement an intermediate class that
	takes a model and overrides the methods to offset the arguments of
	log-likelihood.
	[[./framework/offset_model.py]]

*** DONE Benchmark.
	:LOGBOOK:
	CLOCK: [2020-03-24 Tue 09:32]--[2020-03-24 Tue 15:33] =>  6:01
	CLOCK: [2020-04-07 Tue 11:10]--[2020-04-07 Tue 15:10] =>  4:00
	CLOCK: [2020-04-05 Sun 09:08]--[2020-04-05 Sun 17:09] =>  8:01
	:END:
	Use nLike to estimate the time it took. Investigate the use of
	Kullback-leibler.

**** DONE Figure out why MPI is causing crashes if more than one instance of PolyChord is run.
	 :LOGBOOK:
	 CLOCK: [2020-04-05 Sun 19:28]--[2020-04-06 Mon 02:30] =>  4:32
	 :END:

	 #+begin_src python
	 from mpi4py import MPI
	 #+end_src
	 This is an example of why non-functional side-effect-y code is
	 bad. importing a module should have *no* bearing on the code
	 that's being run, except for making code available to the
	 interpreter. This can cause Undefined behaviour if the module is
	 being imported more than once. This can lead to many bugs like
	 this one.

	 DITTO, remove all global code from the modules and put the tests in their own
	 functions:
	 #+begin_src python
	   if __name__ == '__main__':
		   test()
	 #+end_src

**** DONE Plot the results.

	 Benchmark PPR, SSPR mixture of Uniform and Gaussian.
	  [[./framework/benchmarks.py]].

	 If the mixture contains a Gaussian, and/or is a Gaussian itself
	 with re-partitioning done properly, i.e. the Log-likelihood is
	 coorrected, the PolyChord terminates with empty files which
	 anesthetic cannot load.

	 This is weird. Need to ask Will about this.

*** DONE Investigate usefullenss of Kullback-leibler
	:LOGBOOK:
	CLOCK: [2020-04-11 Sat 11:13]--[2020-04-11 Sat 13:13] =>  2:00
	CLOCK: [2020-04-07 Tue 17:11]--[2020-04-07 Tue 22:00] =>  4:49
	:END:
	Not a good metric. This is smaller if the posterior is wrong. It
	also doesn't correlate with performance.

	It does correlate with perofmrnace, but not as much as one would
	hope.

	[[./illustrations/kullback-leibler.tex]]

	nLike is the dominant cost. Class takes 12 seconds to evaluate it, CAMB takes 3.
*** DONE Add Slurm warnings about failures
    SCHEDULED: <2020-03-28 Sat>

*** FAILED Debug slurm failures
    SCHEDULED: <2020-03-28 Sat>
    DEADLINE: <2020-03-29 Sun>
    Weird tracebacks in output.
    MKL not recognised.
    Yaml doesnt recognise tabs.

*** DONE Migrate sbatch script to one of the examples.
    SCHEDULED: <2020-03-28 Sat>

*** DONE Obtain posterior from Cobaya
    SCHEDULED: <2020-03-20 Fri>
	Takes too long. Many repeats of

	#+BEGIN_SRC python
	[polychord] Calling PolyChord with arguments:
	[polychord]   base_dir: ./raw_polychord_output
	[polychord]   boost_posterior: 0
	[polychord]   cluster_dir: ./raw_polychord_output/clusters
	[polychord]   cluster_posteriors: True
	[polychord]   compression_factor: 0.36787944117144233
	[polychord]   do_clustering: True
	[polychord]   equals: True
	[polychord]   feedback: 1
	[polychord]   file_root: run
	[polychord]   grade_dims: [6, 21]
	[polychord]   grade_frac: [12, 840]
	[polychord]   logzero: -1e+300
	[polychord]   max_ndead: -1
	[polychord]   maximise: False
	[polychord]   nfail: -1
	[polychord]   nlive: 675
	[polychord]   nlives: {}
	[polychord]   nprior: -1
	[polychord]   num_repeats: 135
	[polychord]   posteriors: True
	[polychord]   precision_criterion: 0.001
	[polychord]   read_resume: True
	[polychord]   seed: -1
	[polychord]   write_dead: True
	[polychord]   write_live: True
	[polychord]   write_paramnames: False
	[polychord]   write_prior: True
	[polychord]   write_resume: True
	[polychord]   write_stats: True
	[polychord] Sampling!
	#+END_SRC

	May be that it doesn't terminate correctly.  edit: Lukas mentioned
	that the number of live points is too large. May be
	related.

	Will need to edit [[./cobaya/old/run.yaml]].

**** DONE Fix priors
	 The priors are too broad. Ie already written an essay on
	 why you can't just pick a prior out of thin air, so I'm very iffy
	 about this decision.

**** DONE Reduce nLive
	 Reduced the number of live points: [[./cobaya/run.yaml ]]

	 #+BEGIN_SRC python
	 nlive: 126
	 #+END_SRC

**** DONE Re-install Cobaya.
	 :LOGBOOK:
	 CLOCK: [2020-03-27 Fri 16:44]--[2020-03-27 Fri 23:05] =>  6:21
	 :END:
	 #+begin_src bash
	 pip3 install cobaya --upgrade --user
	 #+end_src
	 It doesn't respect the venv.

	 It doesn't pull in PolyChord correctly. So much for ``just works''.

	 Compile from source.
**** DONE Re-re-install Cobaya.
	 :LOGBOOK:
	 CLOCK: [2020-03-30 Mon 16:15]--[2020-03-31 Tue 02:51] => 10:36
	 :END:
	 It works only partially.
***** DONE Make sure that OpenBlas is loaded.
	  :LOGBOOK:
	  CLOCK: [2020-03-30 Mon 20:01]--[2020-03-30 Mon 23:02] =>  3:01
	  :END:
	  #+begin_src bash
	  python -c "from numpy import show_config; show_config()" \
	  | grep 'mkl\|openblas_info' -A 1
	  #+end_src
	  Produces
	  #+BEGIN_SRC bash
	  blas_mkl_info:
	  NOT AVAILABLE
	  --
	  openblas_info:
      libraries = ['openblas', 'openblas']
	  --
	  lapack_mkl_info:
	  NOT AVAILABLE
	  #+END_SRC

	  #+BEGIN_SRC bash
	  module load openblas
	  #+END_SRC
	  doesn't work. Need to re-install =numpy=.

	  #+BEGIN_SRC bash
	  pip3 install numpy
	  #+END_SRC
	  is not changing the output. Try installing from source.

	  Installing from source doesn't work.

	  EUREKA!  Cobaya defaults to =icc=, meaning that the =openblas= is
	  not the kind of linear algebra library it's even looking for.

	  SOLUTION:
	  #+BEGIN_SRC bash
	  module load openblas
	  module load intel/impi/2017.4/intel
	  module load intel/mkl/2017.4
	  module load intel/compilers/2017.4
	  module load intel/libs/idb/2017.4
	  module load intel/libs/tbb/2017.4
	  module load intel/libs/ipp/2017.4
	  module load intel/libs/daal/2017.4
	  module load intel/bundles/complib/2017.4
	  #+END_SRC
***** DONE add fix to =sbatch= script.
	  :LOGBOOK:
	  CLOCK: [2020-03-30 Mon 23:02]--[2020-03-30 Mon 23:30] =>  0:28
	  :END:
**** DONE Run Cobaya
	 :LOGBOOK:
	 CLOCK: [2020-04-01 Wed 08:55]--[2020-04-01 Wed 12:55] =>  4:00
	 CLOCK: [2020-04-21 Tue 12:50]--[2020-04-21 Tue 18:50] =>  6:00
	 CLOCK: [2020-03-30 Mon 20:03]--[2020-03-31 Tue 01:50] =>  5:47
	 :END:
	 Times out after 6 hours. relevant piece of sbatch script.

	 #+BEGIN_SRC bash
	   #SBATCH -J cobaya
	   #SBATCH --nodes=3
	   #SBATCH --ntasks=96
	   #SBATCH --time=06:00:00
	   #SBATCH --mail-type=FAIL
	   #SBATCH --mail-type=BEGIN
	   #SBATCH --mail-type=TIME_LIMIT_80

	   ##SBATCH --qos INTR
	   ## What's the deal? I keep being told by Dr Handley
	   ## to do this, and it keeps failing.

	   #SBATCH --output latest_%j.out
	 #+END_SRC
**** FAILED Re-run Cobaya
	 Timing out. Need to ask Lukas
***** DONE Ask Lukas.
	  Too many live points.

	  FINALLY!!!! IT finished.

	  I only needed to reduce nLive to 100, from 25d (\(approx 700\)),
	  this reduces the run-time sevenfold from a week down to one day!

	  Since I've had many hurdles due to many versions of =pip=, maN y
	  versions of python, and cobaya stubbornly choosing to ignore the
	  virtual environmeN t it's in, I'd say that these difficulties
	  have no academic relevance.

	  If they do, they are a lesson to Anthony Lewis, that programming
	  conventions exist for a reason. People like him, should probably
	  not ignore the conventions of their language, mainly because as
	  an academic, they cannot dedicate as much time as needed to
	  maintaining their "I did it my way" versions of the program.
*** DONE Refactor of framework
    SCHEDULED: <2020-03-29 Sun>
    DEADLINE: <2020-04-02 Thu>
    Used PyCharm. Fixed a bug. Reference
    Python hash is not random, but linear in regions. For small values
    of $\theta$ this may and does cause issues.

    fixed with

    #+BEGIN_SRC python
    h = hash(tuple(t))
    seed(h)
    r = random()
    #+END_SRC

** Week 3
*** DONE Test
    SCHEDULED: <2020-04-02 Thu>
    DEADLINE: <2020-04-04 Sat>
    All tests clear. Performance improved. Significantly. No bugs.

    Also a discovery! Under some circumstances PPR can
    break. [[./illustrations/convergence.pdf]]. In the same environment,
    Gaussian under SSPR finishes faster and gets the right
    answer. Under the same circumstances PPR inside SSPR also finishes
    faster.

    Choosing which one to include is like choosing your favourite
    child. I could make the case that SSPR makes the simulation more
    robust if wrapped inside PPR. On the other hand SSPR doesn't need
    PPR. Maybe give Hobson and Feroz some credit here. I'm already
    being overly negative about their discovery, even though my
    discovery is based on theirs. PPR it is then.

*** DONE Project presentations.
    SCHEDULED: <2020-04-06 Mon>
    According to Charles Smith we need to present our findings.

    update: Meeting is scheduled.

*** DONE Document findings.
	[[./illustrations/benchmark.tex]]
	[[./illustrations/convergence.pdf]] discussed [[*Stochastic supepositional re-partitioning.][here]].
	[[./illustrations/higson.png]]
	[[./illustrations/scaling-kld.tex]]
	documented in the write-up: [[file:paper.org::*Results%20and%20Discussion.][Results and Discussion.]]
*** DONE e-mail Dr Handley about findings.
	Says he's impressed. I thought I was behind schedule. Need to add
	[[*Add Kullback-Leibler Divergence plots][more Kullback-Leibler stuff]]. Maybe see how offsets affect the
	[[*Add Kullback-Leibler Divergence plots][Kullback Leibler divergence]].
** Week 4
*** DONE Add Kullback-Leibler Divergence plots
    :LOGBOOK:
    CLOCK: [2020-04-13 Mon 18:26]--[2020-04-18 Sat 22:47] => 124:21
    :END:
    Kullback Leibler divergence is useful but only marginally
    so. Kullback Leibler from prior to posterior indicates performance
    up to a point.

	[[./illustrations/scaling-kld.tex]]

    PolyChord may converge faster for a stronger bias: e.g. if the
    prior is sharply peaked at the origin. In that case
    \(\mathcal{D}\) is larger but run-time is smaller.

	[[./illustrations/triangle-fit.pdf]].




*** DONE Install Cosmochord.
    :LOGBOOK:
    CLOCK: [2020-04-18 Sat 13:04]--[2020-04-18 Sat 14:04] =>  1:00
    :END:
    Need [[https://cosmologist.info/cosmomc/readme_planck.html][external Planck likelihood]].

*** DONE Install on Cluster
    :LOGBOOK:
    CLOCK: [2020-04-18 Sat 14:09]--[2020-04-18 Sat 20:25] =>  6:16
    :END:
    Planck is typical academic abandonware. They have a ``python''
    script that installs the dependencies called =waf=. In their
    wizdom, they decided that they can do a better job than either
    pypi, or conda. They wrote their own package manager that
    downloads an outdated dependency if it can't find it (which it
    can't because if you think and it can't. Because if you think
    that writing your own package manager is a good idea, you're
    really too stupid to do it properly.  find it, because people who
    do pip and conda, don't abandon their software and move on.

    As this may be read by someone who's writing academic code; the
    proper procedure is telling the end-user that they need packages
    x, y and z. If you're writing a package and it needs a package
    manager, and you're not thinking of maintaining it -- Don't write
    a package manager! Mainly because I don't think you'd bother to
    check the venv, and update the download links. Also, you're
    probably thinking that either pip or conda can't do what you need
    them to do... So you don't ask.

    This is at least six hours of wasted time, that could have been
    avoided if people were not in the sweet-spot of writing difficult
    to replicate, but at the same time completely unmaintained and
    under-developed code. This is why I insist on **proper** software
    engineering techniques!

    Also! Keep Science away from Python. It's design philosophy 'we're
    all consenting adults', basically means that people do however,
    whatever and there's no responsibility. Python is a unique
    language: it has a few surface-level good ideas, a bunch of
    terrible ones, and is moderated by people that clearly have no
    idea what they're doing. That latter point is especially painful,
    as for some reason they thought that creating a breaking change to
    Python 3 was a good idea, yet they were unable to phase out
    python2, and after all of that, they thought that keeping the
    names of packages the same was not going to backfire.

    The bigger problem is that people are seduced by the mild
    syntactic sugar of python. Big projects are dragged down by the
    cesspool of moronic design. It's giving me nightmares at this
    point.

    The project report guideline asks us to think of ways that could
    make this easier. My suggestions. Don't use python. Don't! All the
    time you save by avoiding the curly braces is being paid for in
    sleep-deprived tortured people that just want to get their degree
    done with. I'm fed up with Cobaya's overengineered nonsense. I
    wanted to use CosmoChord to avoid Python at all costs. It doesn't
    work. Because Python is used there as well. It's 2am. I'm trying
    to figure out why pip swears that astropy is installed. While at
    the same time no version of python seems to detect it. If that
    wasn't enough; there's 17 version of pip on the cluster. I don't
    know which one to use to install that damn thing! I shouldn't have
    to know!

    ANd the worst part is, people assume that writing python code is
    easy, because of all the light-weight stuff. If I told someone
    that I was stuck for an entire 24 hours trying to fix a build on a
    cluster people would say "how hard can it be". Just pip install?
    Right?

    Eventual workaround
    #+BEGIN_SRC python
    module load python@3.8
    sudo /usr/local/software/master/python/3.8/bin/python -m ensurepip
    module purge
    module load python
    /usr/local/bin/python -m ensurepip
    /usr/local/bin/python -m pip install astropy cython
    #+END_SRC
    Notice that I had to use a python3 pip to install a python2
    pip. Naturally this is just stupidity.



*** DONE Have a more thorough discussion of the potential next-gen sampler.
	:LOGBOOK:
	CLOCK: [2020-04-20 Mon 11:12]--[2020-04-20 Mon 12:12] =>  1:00
	:END:
	Use dynamic linking, refactor current code to allow multiple prior
	specifications. Fix the current overloading. Re-use as much of
	PolyChord's fortran code as possible. Everyone tries to do
	unbounded priors; but you can always do that using a bounded
	uniform.


*** DONE Rename all references to SSPR to SSPM <2020-04-20>
	Posterior re-partitioning implies that it has something in common
	with Hobson-Feroz-CHen algorithm. It does *not*.

	It's /much/ more general, in that it can deal with multiple
	models! It can mix the priors completely, and choose the more
	representative model!

	This is not posterior re-partitioning, this is superpositional
	prior mixing. It just so happens that we're re-partitioning the
	posterior in every model.


	A more appropriate name for this is Stochastic Superpositional
	prior mixing. More accurately model mixing, but this is with the
	intent to mix different priors.

*** DONE describe model chaining! <2020-04-21>
	See [[./project-report.tex]].
*** DONE Produce CosmoChord output
	:LOGBOOK:
	CLOCK: [2020-04-22 Wed 09:10]--[2020-04-22 Wed 15:20] =>  6:10
	:END:
	CosmoChord works well as soon as you replace =action=5= with
	=action=1=, in =test_planck.ini=.

* Easter Term.
*** DONE prepare for the presentation.
	:LOGBOOK:
	CLOCK: [2020-04-23 Thu 15:43]--[2020-04-23 Thu 17:00] =>  1:17
	:END:
	[[./presentation.tex]]
*** DONE Give presentation.
    SCHEDULED: <2020-04-27 Mon 16:20>
*** DONE extract parameter covariances.
	:LOGBOOK:
	CLOCK: [2020-04-23 Thu 13:42]--[2020-04-23 Thu 15:33] =>  1:48
	:END:

	Coabaya's native output is not suitable for =anesthetic=. Use
	GetDist instead.

	Outputs generated and saved in plain text for later use.
**** DONE e-mail Dr Handley about using HDF.
	 This could reduce the memory usage significantly.
*** DONE Meet with Dr Handley
*** DONE Implement SSPM (SSIM) in Cobaya.
    :LOGBOOK:
    CLOCK: [2020-04-30 Thu 20:05]--[2020-05-01 Fri 11:13] => 15:08
    CLOCK: [2020-04-24 Fri 16:20]--[2020-04-24 Fri 21:20] =>  5:00
    CLOCK: [2020-04-23 Thu 13:40]--[2020-04-23 Thu 13:42] =>  0:02
    :END:
    Cobaya is a mess. There's very few places where this could be plugged in. 

    Spent time refactoring code to make it intelligible. 

    Spent time thinking about the calibration parameters. Can't just
    place the priors on half the parameters, and cobaya in its wizdom
    only produces half the parameters' posteriors. Thanks very much
    for throwing away information. It's not like you could have
    clearly marked them as rubbish and returned them in. 

    EDIT: it does. The prior is lost though. 
    EDIT: Found the priors in Planck likelihood code. 
**** DONE Use getdist to obtain mean and cov. 
**** DONE Increase the number of parameters
     Complicated. Need to add =grade_dims=, passthrough, add two
     parameters (branch index) and make sure that the stuff works. 
**** DONE Use mean and cov to implement iGPR (iPPR). 
**** DONE Make work with correlated Gaussians. 
**** DONE Implement stochastic choice
     Derived parameters trip up SSIM.  This may be a huge problem. I
     also need to implement the parameter correlation decoupler.
*** DONE Refactor the report. 
	:LOGBOOK:
	CLOCK: [2020-05-03 Sun 11:55]--[2020-05-03 Sun 17:55] =>  6:00
	CLOCK: [2020-05-02 Sat 12:54]--[2020-05-02 Sat 20:30] =>  7:36
	CLOCK: [2020-05-01 Fri 15:53]--[2020-05-02 Sat 02:00] => 10:07
	CLOCK: [2020-04-28 Tue 12:39]--[2020-04-30 Thu 01:39] => 37:00
	CLOCK: [2020-04-27 Mon 08:38]--[2020-04-27 Mon 21:38] => 13:00
	CLOCK: [2020-04-26 Sun 10:01]--[2020-04-26 Sun 16:01] =>  6:00
	CLOCK: [2020-04-25 Sat 16:28]--[2020-04-25 Sat 22:05] =>  5:37
	CLOCK: [2020-04-25 Sat 11:59]--[2020-04-25 Sat 17:10] =>  5:11
	:END:
**** DONE Communicate that we found a way to accelerate stuff. 
**** DONE Trim fat. 
**** DONE Renumber the equations. 
	 :LOGBOOK:
	 CLOCK: [2020-04-25 Sat 11:56]--[2020-04-25 Sat 15:59] =>  4:03
	 :END:
**** DONE Drop re-sizeable
**** DONE Results. 
**** DONE Check Higson always appears. 
**** DONE Clarify Figure 5 
**** DONE Use log scale in y
**** DONE State K-L divergence. 
**** DONE Section on Kullback-leibler
**** DONE Consistent D's figure 5. 
**** DONE Combine time complexity. 
**** DONE Timing Proportional to K-L D. 
**** DONE FIgure 5 analysis. 
**** DONE Generalise Additive mixtures. 
**** DONE Dock stochastic 
**** DONE Rewrite end of Additive mixtures. 
**** DONE Fiddly additive
**** DONE Add to intro that stoch> additive. 
**** DONE Remove Bayesian > frequentist.
**** DONE Remove single-sentence paragraphs.
**** DONE More stuff on Bayesian meta-analysis. 
**** DONE Combining cosmo data sets hyper-parameters. 
**** DONE Misestimating covariances. 
**** DONE INclude toy model experiments. 
**** DONE Expand on Bias. Section. 
**** FAILED Boundary effects. 
     Too much information already. Doesn't fit into the report. 
**** DONE Conservative Cosmology Umbrella sampling. 
     Turrned out to be completely not what I meant in the end of Results and Conclusions. 
**** DONE Add 3d plot of SSIM. 
**** DONE Correct all comments from second draft. 
**** TODO Expand section on cosmology after Cobaya completes
*** DONE Touch up on comments by Dr Handley
    :LOGBOOK:
    CLOCK: [2020-04-26 Sun 19:37]--[2020-04-30 Thu 19:37] => 96:00
    CLOCK: [2020-04-24 Fri 16:24]--[2020-04-24 Fri 20:33] =>  4:09
    :END:
**** DONE Abstract. 
**** DONE Theoretical Background. 
*** TODO Re-run Cobaya with SSIM on the Cluster. 
    :LOGBOOK:
    CLOCK: [2020-05-01 Fri 14:44]--[2020-05-03 Sun 15:53] => 49:09
    :END:
    Again. Cobaya's package manager saves the day. In all honesty, if
    you're thinking that your program needs a package manager, it's a
    good indicator that you should not be let near a computer. The
    python mantra of "we're all consenting adults" rings hollow in
    cases such as these. I did not consent to people being allowed to
    be stupid and jeopardize my research. I did not consent to python
    giving people like Anthony Lewis the impression that writing and
    maintaining a package manager is an easy task. Moreover I did not
    consent to Python letting him half-arse the job and me having to
    babysit his spaghetti code. If Python was relegated to toy
    programs and children, and not allowed anywhere near serious
    research, this problem would have been avoided. 

    Finally, after non-consensual brain intimacy with Cobaya's package
    manager, where I had to specifically tell it to use the modern
    polychord v1.17, it finally started.

    
*** FAILED Implement SSIM is CosmoChord
Simply not enough time. I can't afford another week. 
*** TODO Include the burn-in error paper. 
Waiting for Dr Handley to provide one. 
*** TODO Publish all produced code on Github, 
*** DONE Turn this diary into a format convenient for the examiner. 
	Apparently we only need to submit this *if* there's something
	important in it. Brilliant.
* Notes
** Michaelmas Term

   Doing some research about the subject.


*** Terminology

	Prior - \(\pi (\theta) = P(\theta | M)\)
	Likelihood - \({\cal L}(\theta) = P(M, \theta | Data)\)

	Posterior - \({\cal P}(\theta) = P(\theta | \text{Data})\)
	Evidence - \({\cal Z} = P(Model | Data)\)

	Bayes' theorem says that

	\[Likelihood \times Prior = Posterior \times Evidence\]

	So can use this to find the parameter values of a model, + the
	likelihood that the model fits the data at all.


*** How does nested sampling work

**** Skilling's paper

	 cite:skilling2006

	 Nested Sampling is a machine learning technique that allows to do Bayesian parameter estimation.

	 Fitting a line to data is an example of a parameter fitting model.

	 Set \[ \chi^{2} \triangleq \sum_{i} \left(\frac{y_{i} - f(x_{i},
	 \theta)}{\sigma_{i}} \right).  \] We need to ask a question, how
	 likely is the data observed, given that the model is true, and
	 the Model parameters have the given values. The probability is
	 usually given by a Gaussian (or normal distribution).

	 \[
	 L = \frac{1}{N} \exp{\left[ - \chi^{2}\right]}
	 \]

	 So what we need to do for Nested Sampling to work, is to
	 provide a model for estimating the fit to the hypothesis -
	 likelihood, and a prior.

	 The likelihood, or how likely is the value of data given the
	 model and the parameters, reflects how we expect the
	 fluctuations to develop. Many distributions are possible, but
	 due to the Central Limit theorem, best choice would be a
	 Normal (Gaussian) distribution.

	 The prior represents our prior knowledge of the original
	 parameters. For example, if we know nothing about the
	 possible model parameters, we can expect a uniform
	 distribution within constraints. These constraints may be
	 artificial (for example, we may only be interested in model
	 parameters that are within machine-representable floating
	 point numbers), or natural (the Hubble parameter is
	 positive).

	 If we know more about the model parameters, that information
	 can also be presented as a guideline for parameter
	 inference. For example if we have done parameter estimation
	 of the same model, using a different set of data; the
	 posterior of the aforementioned investigation can be used
	 directly as the prior for this run.

	 Nested sampling exploits that extra data to converge upon the
	 so-called typical set; which represents the data that has
	 statistically significant phase volume. The latter point can
	 be understood intuitively.

	 More accurate or tight constraints on the true data should
	 lead to better convergence time. Ideally the convergence to
	 the posterior of a distribution is the fastest, as this
	 minimises the number of errors, and given a suitable sampling
	 algorithm should lead to few wasted computations.


***** Phase volume example.

      So for example if we have a truncated spherical Gaussian in a
      box with bounds given by \(a\) and \(b\), the volume that we
      expect is given by: 

      \begin{equation*}
      \ln \mathcal{Z} = (\frac{\ln (2 \pi \sigma^{2})}{2} - \ln (b - a))\cdot d
      \end{equation*}
      where d is the dimensionality of the model, $\sigma$ - the
      standard deviation. 

      As we can see, the phase volume is dependent on $\sigma$, and
      when we're repartitioning, we're fudging what the volume should
      look like. 

      Effectively we're inflating the Gaussian in likelihood, so that
      the posterior for a shrunken one is the same as of the uniform. 




**** Notes on the nested sampling algorithm:
     Bayesian inference is essentially taking the $\int \pi
     \mathcal{L} d\theta$. But it's difficult.  Rasterising the phase
     space is ineffective, as for a model with 27 parameters, the
     space would be 27 dimensional (53 dimensions with calibration
     parameters added automatically). This leads to many quirks of
     geometry and counter-intuitive outcomes, that will be touched on
     later.
     
     We must first select a number of live points randomly from
     the phase space, usually taken to a be a hypercube with edge
     length normalised to 1.
     
     For each point one expects there to be a locus of points with
     the exact same likelihood. This locus is often connected, and
     so in analogy with isotherms it is often referred to as the
     iso-likelihood contour.
     
     Then one selects the least likely point and picks according
     to some algorithm, a point of higher likelihood. The original
     point is now referred to as dead, while the new point is
     added to the set of live points.
     
     This process is then repeated until we have reached a typical
     set. This is often determined by estimating the /evidence/
     contained outside each contour (since the points are picked
     at random, if we have \(n_\text{live}\) points, each contour will
     statistically include \(\frac{1}{n_\text{live}}\) of the total
     phase volume).


**** Piecewise power repartitioning notes.
 For the power posterior repartitioning, remember we're doing it with a
 diagonal prior covariance, so everything is separable and Z(beta)
 For the power posterior repartitioning, remember we're doing it with a
 diagonal prior covariance, so everything is separable and \(Z(\beta)\)
 should be derived as described in the posterior repartitioning paper,
 namely:

 \(\tilde{\pi} = G[\mu,\sigma](\theta)^\beta / Z(\beta)\)

 \[Z(\beta) = \int_a^b G[\mu,\sigma](\theta)^\beta d\theta\].  where \(G\) denotes a gaussian,
 and a and b are the limits of the uniform distribution. This is
 expressible using erf:

 \begin{equation}
   Z(\beta) = \frac{erf(\frac{(b-\mu)}{\sqrt{2}} \sigma) - erf(\frac{(a-\mu)}{\sqrt{2}} \sigma)}{2}
 \end{equation}



 I've spent a bit of time thinking this morning, and have realised that
 the mixture model is not quite as trivial as I had imagined.

 To be clear, working in 1d for now, our normalised modified prior is
 of the form:

 \[\tilde{\pi}(\theta) = \beta U[a,b](\theta) + (1-\beta)G[\mu,\sigma](\theta)\]

 where there will be a,b,\mu,\sigma for each dimension. To compute the prior
 transformation which maps \(x\in[0,1]\) onto \(\theta\), nominally we should do this
 via the inverse of the cdf:
  \begin{equation}
	F(\theta) = \frac{\beta (\theta-a)}{(b-a)} +
	(1-\beta) \frac{1}{2}\frac{1+erf(\theta-\mu)}{\sqrt{2}\sigma}
  \end{equation}

 Unfortunately \(x = F(\theta)\) is not invertible. There is another way
 around mapping \(x\in[0,1]\).

 In general, if you have a mixture of normalised priors: \[ \pi(\theta) = \sum_i
 A_i f_i(\theta)\]

 \[\sum_i A_i = 1 \] where each \(f_i\) has an inverse CDF of \(\theta = F^{-1}_i(x)\)

 one can define a piecewise mapping from \(x\in[0,1]\) thus:

 \(\theta = F^{-1}_{i}\left(\frac{x-\alpha_{i-1}}{A_i}\right) : \alpha_{i-1} < x < \alpha_i\)

 \[\alpha_i = \sum_j^{i} A_j\]

 Basically this uses x to first choose which component of the mixture
 is active (via the piecewise bit), and then rescales the portion of x
 within that mixture to [0,1].

 This method seems a little hacky at first, but the more I think about
 it the more reasonable it seems. I would be interested to hear your
 opinion, and we can discuss on Wednesday morning. Until then,
 practically you should focus on the diagonal PPR approach, as that is
 much more straightforward, and captures the essence of the method.


**** Data and Parameter covariance matrices.

	 To avoid having to generate data with a given distribution,
	 we can simply and directly use the Parameter covariance
	 matrix, for our toy models.

	 This basically means that instead of using the model's
	 functional form, we directly assume that the distribution is
	 of Gaussian nature. This we simply plug into the log
	 likelihood, and the rest of the algorithm proceeds as if we
	 had data and the functional form, and the \(chi^2\)
	 computation was done for free.


***** Correlated vs Uncorrelated Gaussian log likelihoods


	  If the parameter covariance matrix is completely diagonal,
	  then the parameters are each individually Gaussian
	  distributed, with a standard deviation being the diagonal
	  element.

	  An arbitrary coupling can lead to covariance on the
	  off-diagonal. These mixtures can be unmixed by using either
	  Singular Value or eigenvalue decomposition of the covariance
	  matrix. This can be simply regarded as a coordinate
	  transform, a passive one at that. Consequently, a Gaussian
	  distribution in Loglikelihood takes the following form.

	  Let \(\vec{\mu}\) be the vector of mean values of Gaussian
	  distributed parameters \(\vec{\theta}\) (we shall drop the
	  vector). The corresponding parameter covariance matrix is
	  \(G_{i,j}\).

	  Therefore the corresponding loglilkelihood is

	  \[
	  \ln {\cal L} = -N - (\theta - \mu)^{T} G^{-1} (\theta - \mu)
	  \],
	  where the normalisation constant is given by
	  \[
	  N = \frac{\det \left| 2\pi G \right|}{2}
	  \].



** Lent Term

*** Polychord <2020-01-10 Fri>

	Polychord is a nested sampling program that uses directional
	slicing, which is not (citation needed) a form of rejection
	sampling.

	To run polychord one needs to do three things:

*** =settings=

	which needs the number of dimesnions with which we're working,
	(very procedural, probably a consequence of fortran-centric
	implementation).

	The Settings have information about the verbosity of the
	system.

	#+begin_src python
	  settings.feedback = 0
	#+end_src
	seems to be a good default.

	Polychord can resume the older run, if instructed (rather by
	default), so in order to have clean bench-marking do

	#+begin_src python
	  settings.read_resume=False
	#+end_src

	To control running-time vs precision trade-off, use

	#+begin_src python
	  settings.nlive=175
	#+end_src

	Changing it to a lower value makes the program run faster.

	Another way to control termination is the

	#+begin_src python
	  settings.precision_target=0.1
	#+end_src

	But we should normally not tinker with it.

*** logLikelihood

	This is essentially a \( \chi^2\), which represents the
	probability of our data, given the model and the parameter
	values.

	We need to define it for each model. Ideally what it needs to
	return is the normalised probability, but not giving it the
	proper normalisation doesn't seem to affect the run-time, only
	the result.

*** Prior

	This is a weird function. What this is called, probably has
	nothing to do with what it actually is: it's taking a point in
	a unit hypercube and maps it onto the real \( \theta\) values.

	This function is where we can get most of our performance
	uplift.

	Ideally assuming that the /real/ prior is the posterior the
	algorithm should converge the fastest. This should however
	affect the loglikelihood calls, because we're re-scaling the
	space.

	I **think** that this simply means that the absolute value of
	the **loglikelihood** is **not a meaningful** number.

**** UPDATE: it is meaningful. Just not without the prior.  <2020-02-14 Fri>
	 AutoPR relies on the posterior to be preserved if we change the
	 value of the prior, we *need to* change the posterior. The
	 assumption is that the prior didn't really change.

*** Why PolyChord converges faster for more informative priors.

	Polychord stops as soon as the evidence remainig is a
	predetermined fraction of the original.

	It's working in a unit hypercube, so we can assume that the
	evidence is normalised.

	So the value oF
	#+begin_src python
	settings.precision_criterion=0.01
	#+end_src
	means stop when the evidence is less than one percent.

	So if we give a more informative prior the algorithm is more
	efficient at finding all the evidence that it needs.

	The prior basically says, draw from the region of higher
	likelihood more frequently. So you are more likely to draw more
	evidence in a shorter time.

	It's not guaranteed to converge the fastest if the prior is the
	posterior, hence why [[*Kullback Leibler divergence.][Kullback Leibler]] divergence is not that
	important.



*** Kullback Leibler divergence.
	KL-D is explained in the [[https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained][blog]]. It basically is the [[https://en.wikipedia.org/wiki/Entropy_(information_theory)][Shannon entropy]]
	between two distributions. It's

	\begin{equation}
	D = \log \langle \frac{P_{1}}{ P_{2}} \rangle
	\end{equation}
	so if the two distributions coincide \(D=0\), if they don't it's
	going to be slightly positive. And if the distribution is defined
	somewhere where the other one is zero it can be very negative.

	As a result of this, I can come up with examples where the
	Kullback Leibler divergence is the same, byut the speed of
	convergence is different.

	This is why it can't be used as the only performance metric.

	Another issue is that it assumes that everything goes well. What
	if re-partitioning casues many fasle positives to be drawn from
	the distribution and get rejected. This will cause a slowdown but
	will not affect \(D\).

*** Approaches to modelling systems.  <2020-01-17 Fri>
	One way to model all of our systems is by looking at the \(
	\chi^2\) and dealing with generated data. While this is close
	to what the system might actually do, this is not itself a
	good solution, it's slow and it requires extra computations in
	generating the data with the properties that we need.

	Instead we might simply treat the system as if it was already
	diagonalised in the model parameter space. So if our model has

 \begin{equation}
   \mu =
   \begin{pmatrix}
     \mu_{0}\\
     \mu_{1}\\
     \vdots\\
     \mu_{n}
   \end{pmatrix}
 \end{equation}

 and

 \begin{equation}
   G =
   \begin{pmatrix}
     \sigma_{1}^2 & \sigma_{12}^2 & \cdots & \sigma_{1n}^2\\
     \vdots & \ddots &  \vdots & \vdots \\
      \sigma_{n1}^2 & \sigma_{n2}^2 & \cdots & \sigma_{n}^2
   \end{pmatrix}
 \end{equation}

 which is itself a gaussian assumption, we get the following: as our loglikelihood

 \begin{equation}
   \ln {\cal {L}}  = - N - (\theta - \mu)^{T}G^{-1}(\theta-\mu)
 \end{equation}

 where \( N \) can be found by integrating a multivariate Gaussian. See handout for Phase Transitions:

 \begin{equation}
   N = \ln \det |2\pi G |
 \end{equation}

 this can be evaluated in one fell swoop using

 #+begin_src python
 numpy.linalg.slogdet(2*pi*G)
 #+end_src

 This allows us to do calculations in a fraction of the time.

*** Comparison of runs. <2020-01-24 Fri>

	Planck data can be downloaded from (see references), and using
	the following constraints, we can compute the misfit between
	data.


	[[../LCDM-NS/toy-models/1/Comparison of run with uniform prior and paper.pdf]]


	This shows profound agreement, usingThe following constraints
	on the parameters.

	#+begin_src python
	  planck_ranges = numpy.array(
		  [[0.019, 0.025],
		   [0.095, 0.145],
		   [1.03, 1.05],
		   [0.01, 0.4],
		   [2.5, 3.7],
		   [0.885, 1.04],
		   [0.9, 1.1],
		   [0, 200],
		   [0, 1],
		   [0, 10],
		   [0, 400],
		   [0, 400],
		   [0, 400],
		   [0, 400],
		   [0, 10],
		   [0, 50],
		   [0, 50],
		   [0, 100],
		   [0, 400],
		   [0, 10],
		   [0, 10],
		   [0, 10],
		   [0, 10],
		   [0, 10],
		   [0, 10],
		   [0, 3],
		   [0, 3]])


	  samples = anesthetic.NestedSamples(root='./data.1908.09139/lcdm/chains/planck')
	  fig, ax = samples.plot_2d(['logA', 'ns'])
	  # plt.show()


	  # params = samples.columns[:27]
	  params = samples.columns[:27]
	  Sig = samples[params].cov().values
	  mu = samples[params].mean().values
	  nDims = len(mu)

	  # Run of the original

	  args = {
		  'root_name': 'planck',
		  'm': mu,
		  's': Sig,
		  'likelihood': lambda x: gaussian_likelihood(x, mu, Sig),
		  # 'renew_plots': True,
		  'renew_plots': False,
		  'nLive': 2,
		  'prior': lambda x: uniform_prior_with_ranges(x, planck_ranges),
		  'ax': ax
	  }
	  exec_polychord(**args)

      ...
	  newSamples = anesthetic.NestedSamples(root='./chains/planck')
	  newSamples.plot_2d(ax)

	  plt.show()
	  fig = plt.figure()
	#+end_src

*** Automated Power Posterior Repartitioning. <2020-01-24 Fri>

	Looking at [[https://arxiv.org/pdf/1908.04655.pdf]] we can see
	that one can get better convergence if we use something called
	the Automated posterior repartitioning.

	We start with a Gaussian prior.

	\begin{equation}
	  \pi(\theta) = G(\mu, \sigma) (\theta)
	\end{equation}

	We then introduce an extra parameter into our system:

	\begin{equation}
	  \tilde{\theta} = \begin{pmatrix}
		\theta_{1}\\
		\downarrow\\
		\theta_{n}\\
		\beta
	  \end{pmatrix}
	\end{equation}

	We then use this parameter to rescale the prior that we
	originally had:

	\begin{equation}
	  \tilde{\pi}(\tilde{\theta}}) = \frac{{G(\mu, \sigma)(\theta)}^\beta}{Z_\pi(\beta)}
	\end{equation}

	And normalise it to one Having done that we need to keep the
	posterior the same, so we need to rescale (citation needed)
	the loglikelihood to account for this change.

*** Calculations needed for doing PPR.

	As a result, to use [[*Automated Power Posterior Repartitioning. <2020-01-24 Fri>][it]] with PolyChord we need to do the following
	three things.

	1) Estimate the quantile (called prior in PC).

	2) Estimate the correction needed to the Gaussian likelihood.

	3) Make sure that the correct dimesnionality is used.

	4) Maybe think of the prior on $\beta$.

	To estimate the quantile we need to know the CDF:
	\begin{equation}
	 \int_{-\inf}^{\theta} \pi(\theta') d\theta' = \Pi(\theta)
	\end{equation}
	Then we need to invert it.

	\begin{equation}
	 quantile(x) = \Pi^{-1}(x)
	\end{equation}

	So if it's a truncated uniform distribution:
	\begin{equation}
	\Pi(\theta) = m \theta + b
	\end{equation}

	Where
	\begin{equation}
	m = \theta^{m} - \theta_{m}
	\end{equation}
	and
	\begin{equation}
	b = \theta_{m}
	\end{equation}


	if it's a truncated Gaussian$^{\beta}$, then
	\begin{equation}
	quantile = \mu + \frac{\sqrt{2} \sigma}{\sqrt{\beta}}  erf^{-1}\left(x erf \left[\sqrt{\frac{\beta}{2}}\frac{b-\mu}{\sigma}\right] +(1-x)  erf\left[\sqrt{\frac{\beta}{2}}\frac{a-\mu}{\sigma}\right] \right)
	\end{equation}
	(Dr Handley missed the prefactor of \sqrt{2}/\beta, which caused
	headaches).

	Sanity check: as $a\rightarrow -\infty$ $b\rightarrow \infty$,
	\begin{equation}
	  quantile \rightarrow \mu + \frac{1}{2\pi \sigma}^{\beta}
	\end{equation}
	Which is what we expect from Poisson integrals.

*** When and why do repartitioning. <2020-02-19 Wed>

	After multiple experiments I arrived at the following.

	Running PolyChord with a Gaussian is **not the same** as what
	we want to accomplish.

	When we consider two different priors, a Uniform from
	\((a,b)^\otimes n\) and a Gaussian given by an \(n\times n\)
	matrix \( \sigma \).

	The histogram of the loglikelihood calls and their results
	will be different, it will differ due to the /effective
	volume/ which each of those will occupy.

	Naturally a Gaussian, even if it has the same effective
	volume cannot simultaneously give the same evidence and the
	same Posterior.

	So we can ask two kinds of questions:

	a) What is the Posterior given the prior that has a different
	volume.

	In this case the loglikelihood calls will cluster around
	different values. This is a legitimate question to ask, but
	it requires more information.

	If we take a system where we don't have the extra information
	about the location of the posterior peaks and their shape,
	and plug in a prior that does, we're biasing the system, and
	effectively /fudging/ the answer. This can in some cases be
	useful, but it's not quite what the project aims to do.

	So instead of asking what would our answer be with a
	different prior, we ask a different question. What would our
	answer be if our system was biased to picke the values that
	/suspect/ are the correct posterior values, without that
	affecting the posterior distribution and injecting extra
	information that we don't have/ can't quantify.

	This is a philosophical issue. Intuitively, if we have the
	extra information it /must/ be reflected in the prior. It
	can't otherwise. In fact by biasing the system, even if we
	repartition the combination \( {\cal L } \pi \) we can still
	end up with a biased and therefore useless result.

	In fact, my experiments clearly show this; if the Prior
	corresponding to /a/ Gaussian which is not the same as the
	posterior (has a different value of the mean), it can result
	in the algorithm terminating and generating a completely
	false posterior.

	See [[./toy-models/1/1.0 Example of parameter covariance.py/]].

	By doing repartitioning we allow our guess to be wrong,
	without that affecting the outcomes: posterior and

*** Correlated and Uncorrelated Gaussian:<2020-02-03 Mon>


	Knowing that the parameter covariance matrix, is usually
	positive defininte, one can argue that the question of
	whether or not the parameters in the model are correlated, or
	completely uncorrelated (i.e. each has a single standard
	deviation value) is moot.

	We can always perform a linear operation that diagonalises
	the parameter covariance matrix, and what the algorithm needs
	to do is only to work with the uncorrelated Gaussians.

	That of course is true, but some repartitioning schemes are
	more sensitive to this fact, and can only work after the
	coordinate transformation has been performed, which itself
	adds to the complexity.

	Other algorithms are more capable of doing this properly. SSPR,
	what we've developed doesn't require any knowledge of the
	underlying distribution. So we don't really care if it's
	correlated or uncorrelated.




*** Stochastic supepositional re-partitioning.

	We take a mixture of re-partitioning schemes. Instead of saying we
	invert the sum of them partially, like Dr Handley suggested (call
	it additive re-partitioning), we may try to do everything
	stochastically.

	That is, we point wise set $\tilde{\pi} \tilde{\cal L}$ to be
	constant. But at each point $\tilde{\pi}(\theta)$ may be
	different. This needs to be done deterministically, because
	PolyChord re-visits the same points and complains about the
	non-determinism.

	We can achieve the determininsm by usgin a [[https://docs.python.org/3/library/random.html][PRNG]] with a
	deterministic seed. I would think that this is just a Hash
	function? (edit: no it's not enough, because Python's hash
	function is linearly distributed. This *can* work, but it will
	only mix in one half of the domain. It's going to converge, but
	may cause errors if we have doubled peaks etc).

	The relevant code for this is in the
	[[./framework/general_mixture_model.py]]. This uses my implementation
	of the broken stick probabity to choose which bin to
	deterministically put a point with coordinates $\vec{\theta}$
	in. The probability of being put in each bin is *not* $\beta$, but
	is related to it. Why? Because $\beta$ vary from \(0\) to \(1\)
	independently. So if $\beta = (1, 1, \ldots, 1)$ then the
	probability is simply $\frac{1}{m}$ where $m$ is the number of
	models in the mixture.

	This is much faster: [[./toy-models/Run-time-for-comparison.pdf]]. I
	don't know why yet, but it is.

	edit: it's because the bias is either full, or non-extant. With
	PPR or an additive mixtue the bias is proportional to the value of
	$\beta$. In SSPR, for each point, the bias is either the same is
	it would have been for a gaussian (in which case values of
	$\beta$, favouring the representative prior, making it skyrocket)
	or is basically flat. This may suggest that it's also more robust.

	edit: it is more robust. [[./illustrations/convergence.pdf]]. It can
	make PPR more robust too. If it's not a panacaea, it's pretty damn
	close to being perfect based on being both faster, and more
	reliable.

	edit: =[[https://github.com/ejhigson/nestcheck][nestcheck]]= [[./illustrations/higson.png/][shows]] that it's more precise, there's a lot less
	variance in evidence, in each of the parameters. It does
	occasionally produce =inf= s and =nan= s. I need to figure out
	why.

	edit: It can also mitigate the miscalculaterd evidence.

*** Mixing non-repartitionned priors.

	What would happen if we mixed prios that didn't have the same
	posterior? What if we genuinely had a prior starting with a
	uniform, and another one that was genuinely Gaussian?

	I think that Stochastic Superpositional mixtures can deal with
	both. They can retroactively adjust the conclusions we reached
	earlier, and produce a better result in that case. It can compare
	two datasets.

	This is not an optimisation to nested sampling. This is a new
	algorithm! This is Bayesian metainferece. It can look at the
	entire pre-history of the data. We can dynamically adjust the
	priors!


*** Purpose built sampler to facilitate stochastic mixing.

**** Quantum computers.
	 Surprisingly, this algorithm maps onto quantum computers quite
	 easily (ask Prof Mike Payne, and Prof. Crispin Barnes). The thing
	 is, we need to explore both branches simultaneously, and use a
	 qubit to represent the probability of belonging to one sub-space
	 or the other. So if mixing \(m\) models, need \(m\) qubits. This
	 superposition is why this is so powerful. We can make this
	 algorithm even faster, because then there would be no extra
	 parameter, and we're exploring the entire space.

**** Modifying PolyChord.

	 May be the easiest. I've built the new API front-end in the
	 ./framework/ folder. You just need to provide an implementation
	 of the prior inversion function (edit quantile), and the
	 log-likelihood. Then set the dimensionality (parameter covariance
	 models do it automagically).

	 So as a result, you can provide building blocks: models related
	 by inheritance. (surprisingly, there are cases where inheritance
	 is better than composition, since we're using Python, we might as
	 well write it object-oriented).

	 These models can be mixed. Arbitrarily. I can mix as many as I
	 wish, and I can build them up using the provided corrections:
	 i.e. if I have a Gaussian likelihood, that's related to data in a
	 complicated way, I can just add the necessary corrections and it
	 will work: e.g.

	 #+begin_src python
	   def loglike(self, data, beta):
			   ll = self.complicated_log_like(data)
			   ll += gaussian_models.ppr_correction(ll, beta)
			   return ll
	 #+end_src

	 Meaning, that very little adjustment to existing workflows is
	 needed.


**** Better API for PolyChord.

	 Currently PolyChord has the following problems.
	 - The settings variable can be written to but not read.
	   #+begin_src python
	   settings.file_root
	   #+end_src
	   gives no such attribute error.
	 - The settings are non-trivial. Can't run PolyChord without
       settings, so need to make the settings part of either a
       PolyChord object or a model object.
	 - MPI. It caused about seven bugs in my code. Because module
       loading should not have side-effects.
	 - The code may be functional. There are many cases where Currying
       would have saved *lots* of typing.
	 - Maybe re-write in Haskell? Rust?

**** Next generation sampler.
	 Motivation: mixture re-partitioning can hugely benefit from
	 having low-level access to the sampler's internal state. For example:

	 - We may want to chain posteriors from different models as priors
       to others.
	 - This may allow us to gain the speed-ups up to and beyond what
       we've shown! We can, as a person would, do a crude estimate,
       and use that to do finer adjustments.
	 - We can do *Bayesian meta-analysis*. Imagine that you can re-run
       all of the inferences that were run up to date, and you can
       also require that the data are consistent? This *can* be done!
	 - Sampling in $\vec{\beta}$ and the branch index need not use the
       same technique. In fact, Metropolis Hastings for $\beta$, and
       slice sampling for all other parameters, may lead to better
       convergence.
	 - Want to eliminate cross-talk between re-partitioning parameters
       for things in the mixture. For example, if I'm mixing two PPR's
       I don't want for both of their powers to be the same and
       sampled the same way.
	 - The branch index is an integer and not a nuisance parameter. It
       was chosen so that the hashing doesn't cause undefined
       behaviour if Python's "smart" numerical routines and
       duck-typing decides to duck with SSPR. Ideally we want for the
       chosen branch to be global to both the prior and likelihood
       scope.
	 - The slice sampler needs to be aware which branch do points
       belong to. Ideally, it can estimate the likelihood in both
       branches by evaluating the common likelihood, and then
       comparing the corrections.
	 We can also learn from =PolyChord= 's mistakes. As of now, it only
	 supports bounded priors and even then, only on spaces that can be
	 mapped onto from a unit hypercube. It has confusing
	 nomenclature. The prior is not a prior: it's not a PDF, but a
	 specification of the prior via its quantile.

	 The bridge between Python and Fortran is unreliable. You can pass
	 a python callable, that's not being compiled to machine code into
	 Fortran. This is wrong. This is why the exception handling
	 facilities of Python and the static type checking of Fortran are
	 both made toothless.

	 The Fortran code (for performance reasons) always does unchecked
	 assignments, while =Python= inherently assumes that all
	 assignments are checked. A lot of the optimisations come in at
	 compile time. In fact, almost all of the functions are pure. Yet
	 Python doesn't support functional optimisations. Fortran does
	 support them, but only if all routines have no side-effects,
	 which current logging forbids.

	 Polychord is very obtuse about logging. Its messages cannot be
	 switched off. Control over verbosity and location of the produced
	 data needs to be made more transparent. I understand that the
	 copyright notice is the wish of the authors. That said, the
	 program reproduces these not on module load, but every time its
	 run! Making several runs pollute the interpreter =stdout=.

	 PolyChord cannot be used in-memory. A couple of my benchmarks,
	 since they deal with low-dimensional models were held back by the
	 file-system input output latency. Moving them into a =ramdisk= is
	 a solution, but not an idiomatic one. Since PolyChord has easily
	 definable inputs it should be functional.

	 As a result I suggest the following improvements:
	 - Proper error handling.
	 - An interface that *requires* a compiled likelihood and
       dynamically links against it.
	 - A memory-safe language with functional tendencies (Julia, Rust, C)



** Easter "vacation"

   Notes at this stage already in the form of write-up. If detailed
   history is needed, just refer to the git history of the
   project-report.org.


*** Cobaya settings <2020-03-18 Wed>
    Hi all, I've cc'd Aleksandr who....

    #+BEGIN_SRC yaml
    sampler:
      polychord:
       num_repeats: 2d
       blocking:
	- [1, [omega_b, omega_cdm, theta_s_1e2, tau_reio, logA, ns]]
	- [20, [A_planck, A_cib_217, xi_sz_cib, A_sz, ps_A_100_100,
	   ps_A_143_143, ps_A_143_217, ps_A_217_217, ksz_norm, gal545_A_100,
	   gal545_A_143, gal545_A_143_217, gal545_A_217, calib_100T, calib_217T,
	   galf_TE_A_100_143, galf_TE_A_143_217, galf_TE_A_100_217,
	   galf_TE_A_100, galf_TE_A_143, galf_TE_A_217]]
    #+END_SRC

 bibliographystyle:unsrt
*** Financial situation, equal opportunities and inflexibility.  :ARCHIVE:

	My experience at Cambridge wasn't good. Not your fault. You have
	to deal with people of various backgrounds, maintain traditions
	and keep everything in status quo, otherwise you'll lose the
	prestige of being one of the top universities in the world.

	That said, I must admit that my personal experience wasn't
	great. I had to work. Not as in study. I didn't have enough money
	to remain in Cambridge. Most of it was paid for by a scholarship
	from my country a year ago. What happened then? Well, inflation,
	price indexation and the fact that my money didn't grow after it
	was assigned. As a result my maintenance provisions were short by
	about Â£1200. Not a lot. Not for you. I don't earn as much as you
	do. An hour of my work is worth 200 roubles about Â£2. Far below
	minimum wage. I can't afford to spend all of my time working, in
	fact I had spent about 20 hours a week trying to make a dent in
	the shortfall.

	Of course, that wasn't enough. You see, the amount that the
	scholarship had accounted for, and the amount that the university
	specifies is considering only termly scheme, in an average
	college. My case wasn't that, mainly because I have nowhere to
	return to. I can't work at home: I have two unemployed parents
	that are deadset on making my life miserable, a wife that I can't
	live with due to our extreme poverty and the fact that my parents
	are deadset on making my life miserable and a sister that needs
	about Â£500 a year in tuition fees.

	I lived for about a year give or take by living on Â£2 a day. Yeah,
	four coffees from the vending machine. It takes cooking, which
	takes time. I have work. Supervision work. I have to prepare and
	revise for exams. All in the same day as needing to wake up early
	and conduct lessons in the morning.

	Of course there's a solution. Maybe I don't need to starve, maybe
	I could borrow money from well-endowed friends. Of course, I need
	to have a job, in order to return that money within a lifetime. So
	what do I do? I look for one. This is also a bit unfair. Why?
	Because a job in software engineering requires you to jump through
	hoops and bounds. I don't just go to an interview. I have to
	prepare for one. And it's usually good, if the interview is just
	the thing on the day. And not, say, do the work we do in a month,
	within a week, submit it following our extremely demanding format
	requirements and within the time frame. And the worst part of it,
	is that all of that work is not compensated.

	I don't get a job. Why? Not because I'm stupid. Because your home
	office has decided that people like me are untermenschen. I have
	to register with the police. I have to update them about every
	single move I make, including, as I found out the hard way, any
	travel to London. Apparently, a job interview after lectures is a
	violation of the codex. I was fined: Â£600. And I didn't get the
	job. I can't get it anywhere: it has to be a corporation that is
	listed on www.gov.uk, it needs to go through tons of extra
	paperwork, and it has to pay. Why hire me, when they can hire
	someone almost the same level as me from Europe or the UK? They
	usually don't.

	Get a tutoring job? It's either under-paid, or inaccessible. DBS
	checks and no experience from the UK.

	So you see, I'm stuck between a rock and a hard place. It's not
	just that, this is something that I have had to deal over my
	entire lifetime, and Cambridge's claim that all students are put
	into equal opportunity situations is, well, for lack of a better
	word Bovine Fecal matter.

	I've had these issues ever since I came to Cambridge. Not your
	fault. It's been the way it's been. It's just that everyone
	immediately assumes that a degree from Cambridge is a ticket into
	a better world. For you it is! For me, well, it doesn't even count
	for my home country's academy of sciences.

	But what's that you say? Allowances? Special circumstances?
	No. These things don't work. I can't get a PhD based on
	allowances. People stop listening at the "but...". Ask Dr
	Castelnovo. He didn't care if I had extenuating circumstances, he
	didn't care if my life was misearble, he didn't care that I've
	spent weeks without sleep. He couldn't be bothered to write me a
	reference, depsite me getting the highest score on his research
	review. And he's right. This is a meritocracy, and you should not
	help the weak. Except I'd be a jackass if I told my parents and my
	wife, that they're weak and that I was going to do the rational
	thing.

	Your allowances should stop existing! If someone deserves a higher
	mark, and was affected by illness, get them another chance to take
	the exam. I would have aced mine, if you had. But now I can't
	reference anything. Your allowances are a patch for a dysfunction;
	it's a not a solution, but it takes the place of one.

	I know that you're going to pedantically look through this diary
	to find a way to remove marks as much as possible. Not your
	fault. This is what you *have to do*. I'm sure that knowing what
	hell I went through, you're going to take extra pride in noting
	that my diary wasn't detailed enough. I'm sure that all of the
	lecture courses were done from scratch, perfectly proof-read, and
	never contain any mistakes or typos.

	Tutors and DoS don't help. They are obviously busy and care
	about their bottom line more than about the people. They got
	me an allowance, yes. A waste of both your and my resources. I
	went away, I had to work for the entire year. I had to fake a
	document from a psychiatrist to get back. Why? Because instead
	of recovering, I was fighting an uphill battle: trying to pay
	for my sister's tutors, help her get into uni, earning enough
	to pay my family's living. Earning enough to rent an
	apartment, because in spite of being totally useless, my
	parents have a right to veto my spouse, and all of that
	culminating in Lung cancer. Figures, Dad smoked his entire
	life. Nobody cares. Nobody's going to take that into
	account. Nor should they. What's the point?
** Easter term
*** Renaming. 

SSPR is a misnomer. It's much more general than mixing priors and
posterior repartitioning schemes. I'll rename every occurrence in the
text to SSIM (isometric mixtures). 

iGPR sounds wrong. Proposal sounds better. iGPR == iPPR. 
*** BOundary effects

This is something that I hadn't touched upon in the report that I
think is quite important. PPR fails to properly account for boundary
effects. The exaggerated tilt for truncated gaussians rears its ugly
head: more precisely, the nature of how a truncated gaussian is
represented changes what $\pi^{\beta}$ is at the boundary. 

The effect is that the $\pi$ at the boundary (or infinitesimal
$\epsilon$ region around it) is highly suppressed. So a distribution
that is cut at the boundary will have a suppressed ${\cal L}$. 

I'd love to investigate this further but I neither have the time, nor
the strength to do it. Besides, it's a little pointless
anyway. Nothing I do is quite good enough. Why bother...
