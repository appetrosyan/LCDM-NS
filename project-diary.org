#+TITLE: Project Diary for "Cosmological parameter estimation using Bayesian accelerated Machine learning"
#+AUTHOR: Aleksandr Petrosyan
#+BIBLIOGRAPHY: bibliography
* Michaelmas Term

  Doing some research about the subject. 

  
** Terminology

   Prior - \(P(\theta | M)\)
   Likelihood - \(P(M, \theta | Data)\)
   
   Posterior - \(P(\theta | \text{Data})\)
   Evidence - \(P(Model | Data)\)

   Bayes' theorem says that 

   \[Likelihood \times Prior = Posterior \times Evidence\]

   So can use this to find the parameter values of a model, + the likelihood that the model fits the data at all. 

   
** How does nested sampling work

*** Skilling's paper

	\cite{skilling2006}
	
	Nested Sampling is a machine learning technique that allows to do Bayesian parameter estimation. 

	Fitting a line to data is an example of a parameter fitting model. 

	Set \( 
	\chi^{2} \triangleq \sum_{i} \left(\frac{y_{i} - f(x_{i}, \theta)}{\sigma_{i}} \right)
	\). We need to ask a question, how likely is the data observed, given that the model is true, and the Model parameters have the given values. The probability is usually given by a Gaussian (or normal distribution). 
	
	\[ 
	L = \frac{1}{N} \exp{\left[ - \chi^{2}\right]}
	\]
	
	So what we need to do for Nested Sampling to work, is to provide a model for estimating the fit to the hypothesis - likelihood, and a prior. 

	The likelihood, or how likely is the value of data given the model and the parameters, reflects how we expect the fluctuations to develop. Many distributions are possible, but due to the Central Limit theorem, best choice would be a Normal (Gaussian) distribution. 

	The prior represents our prior knowledge of the original parameters. For example, if we know nothing about the possible model parameters, we can expect a uniform distribution within constraints. These constraints may be artificial (for example, we may only be interested in model parameters that are within machine-representable floating point numbers), or natural (the Hubble parameter is positive). 
	
	If we know more about the model parameters, that information can also be presented as a guideline for parameter inference. For example if we have done parameter estimation of the same model, using a different set of data; the posterior of the aforementioned investigation can be used directly as the prior for this run. 

	Nested sampling exploits that extra data to converge upon the so-called typical set; which represents the data that has statistically significant phase volume. The latter point can be understood intuitively. 

	More accurate or tight constraints on the true data should lead to better convergence time. Ideally the convergence to the posterior of a distribution is the fastest, as this minimises the number of errors, and given a suitable sampling algorithm should lead to few wasted computations. 


**** TODO Phase volume example. 

    
	
	

*** Notes on the Algorithm itself:

	Rasterising the phase space is too computationally ineffective, as for a model with 27 parameters, the space would be 27 dimensional. This leads to many quirks of geometry and counter-intuitive outcomes, that will be touched on later. 

	We must first select a number of live points randomly from the phase space, usually taken to a be a hypercube with edge length normalised to 1. 

	For each point one expects there to be a locus of points with the exact same likelihood. This locus is often connected, and so in analogy with isotherms it is often referred to as the iso-likelihood contour. 

	Then one selects the least likely point and picks according to some algorithm, a point of higher likelihood. The original point is now referred to as dead, while the new point is added to the set of live points. 

	This process is then repeated until we have reached a typical set. This is often determined by estimating the /evidence/ contained outside each contour (since the points are picked at random, if we have n_\text{live} points, each contour will statistically include \frac{1}{n_\text{live}} of the total phase volume). 

	
*** Piecewise power repartitioning notes.
Are these issues you're encountering for the mixture model, or the
temperature-dependent gaussian? (in the posterior repartitioning
paper, the pi^\beta prior is terms 'power posterior repartitioning', so
we should refer to it as that).

For the power posterior repartitioning, remember we're doing it with a
diagonal prior covariance, so everything is separable and Z(beta)
should be derived as described in the posterior repartitioning paper,
namely:

\(\tilde{\pi} = G[\mu,\sigma](\theta)^\beta / Z(\beta)\)

\[Z(\beta) = \int_a^b G[\mu,\sigma](\theta)^\beta d\theta\].  where \(G\) denotes a gaussian,
and a and b are the limits of the uniform distribution. This is
expressible using erf:

\begin{equation}
  Z(\beta) = \frac{erf(\frac{(b-\mu)}{\sqrt{2}} \sigma) - erf(\frac{(a-\mu)}{\sqrt{2}} \sigma)}{2}
\end{equation}



I've spent a bit of time thinking this morning, and have realised that
the mixture model is not quite as trivial as I had imagined.

To be clear, working in 1d for now, our normalised modified prior is
of the form:

\[\tilde{\pi}(\theta) = \beta U[a,b](\theta) + (1-\beta)G[mu,sigma](\theta)\]

where there will be a,b,\mu,\sigma for each dimension. To compute the prior
transformation which maps x\in[0,1] onto \theta, nominally we should do this
via the inverse of the cdf:
 \begin{equation}
   F(\theta) = \frac{\beta (\theta-a)}{(b-a)} +
   (1-\beta) \frac{1}{2}\frac{1+erf(\theta-\mu)}{\sqrt{2}\sigma}
 \end{equation}

Unfortunately \(x = F(\theta)\) is not invertible. There is another way
around mapping \(x\in[0,1]\).

In general, if you have a mixture of normalised priors: \[ \pi(\theta) = \sum_i
A_i f_i(\theta)\]

\[\sum_i A_i = 1 \] where each \(f_i\) has an inverse CDF of \(\theta = F^{-1}_i(x)\)

one can define a piecewise mapping from \(x\in[0,1]\) thus:

\(\theta = F^{-1}_{i}\left(\frac{x-\alpha_{i-1}}{A_i}\right) : \alpha_{i-1} < x < \alpha_i\)

\[\alpha_i = \sum_j^{i} A_j\]

Basically this uses x to first choose which component of the mixture
is active (via the piecewise bit), and then rescales the portion of x
within that mixture to [0,1].

This method seems a little hacky at first, but the more I think about
it the more reasonable it seems. I would be interested to hear your
opinion, and we can discuss on Wednesday morning. Until then,
practically you should focus on the diagonal PPR approach, as that is
much more straightforward, and captures the essence of the method.


*** Data and Parameter covariance matrices. 

	To avoid having to generate data with a given distribution, we can simply and directly use the Parameter covariance matrix, for our toy models. 

	This basically means that instead of using the model's functional form, we directly assume that the distribution is of Gaussian nature. This we simply plug into the log likelihood, and the rest of the algorithm proceeds as if we had data and the functional form, and the \(chi^2\) computation was done for free. 

	
**** Correlated vs Uncorrelated Gaussian log likelihoods

	 
	 If the parameter covariance matrix is completely diagonal, then the parameters are each individually Gaussian distributed, with a standard deviation being the diagonal element. 
	 
	 An arbitrary coupling can lead to covariance on the off-diagonal. These mixtures can be unmixed by using either Singular Value or eigenvalue  decomposition of the covariance matrix. This can be simply regarded as a coordinate transform, a passive one at that. Consequently, a Gaussian distribution in Loglikelihood takes the following form. 

	 Let \(\vec{\mu}\) be the vector of mean values of Gaussian distributed parameters \(\vec{\theta}\) (we shall drop the vector). The corresponding parameter covariance matrix is \(G_{i,j}\). 
	 
	 Therefore the corresponding loglilkelihood is 

	 \[ 
	 \ln {\cal L} = -N - (\theta - \mu)^{T} G^{-1} (\theta - \mu)
	 \],
	 where the normalisation constant is given by 
	 \[
	 N = \frac{\det \left| 2\pi G \right|}{2}
	 \]. 
	 
	 
	 
